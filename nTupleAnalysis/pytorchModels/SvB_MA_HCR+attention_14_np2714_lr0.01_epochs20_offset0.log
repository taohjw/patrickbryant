0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6611 (26, 23, 12, 29) | 0.808 | 0.980 | 88.46 | 88.87 |------------------------------------------------|
0             Validation | 0.6607 (26, 23, 12, 30) | 0.808 | 0.976 | 88.11 | 88.73 |###############################################| ^ (0.4%, 12.55, 2.2, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1096 batches)
0 >>  2/20 <<   Training | 0.6455 (28, 24, 10, 29) | 0.797 | 0.965 | 88.68 | 89.41 |------------------------------------------------------|
0             Validation | 0.6466 (28, 24, 10, 29) | 0.818 | 0.985 | 88.25 | 89.19 |###################################################| ^ (0.6%, 13.82, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6341 (20, 28,  9, 34) | 0.893 | 1.071 | 89.68 | 89.91 |-----------------------------------------------------------|
0             Validation | 0.6375 (20, 28,  9, 34) | 0.905 | 1.082 | 89.20 | 89.71 |#########################################################| ^ (0.5%, 12.21, 1.7, 2%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (548 batches)
0 >>  4/20 <<   Training | 0.6143 (25, 26, 12, 27) | 0.960 | 1.149 | 90.22 | 90.24 |--------------------------------------------------------------|
0             Validation | 0.6183 (25, 26, 12, 27) | 0.958 | 1.146 | 89.71 | 89.99 |###########################################################| ^ (0.6%, 14.68, 2.2, 0%) 
0 >>  5/20 <<   Training | 0.6150 (25, 26, 12, 27) | 0.935 | 1.121 | 90.17 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.6193 (25, 26, 12, 27) | 0.935 | 1.112 | 89.77 | 89.85 |##########################################################| ^ (0.8%, 15.39, 1.6, 3%) 
0 >>  6/20 <<   Training | 0.6131 (23, 27, 11, 29) | 0.961 | 1.149 | 90.20 | 90.24 |--------------------------------------------------------------|
0             Validation | 0.6175 (23, 27, 11, 29) | 0.968 | 1.155 | 89.70 | 89.98 |###########################################################| ^ (0.6%, 15.68, 1.6, 3%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (274 batches)
0 >>  7/20 <<   Training | 0.6065 (23, 27, 11, 30) | 0.970 | 1.161 | 90.58 | 90.41 |----------------------------------------------------------------|
0             Validation | 0.6129 (23, 26, 11, 30) | 0.979 | 1.165 | 90.05 | 90.08 |############################################################| ^ (0.8%, 15.57, 2.3, 0%) 
0 >>  8/20 <<   Training | 0.6093 (23, 25,  9, 32) | 0.981 | 1.174 | 90.55 | 90.43 |----------------------------------------------------------------|
0             Validation | 0.6154 (24, 25,  9, 32) | 0.989 | 1.184 | 89.89 | 90.13 |#############################################################| ^ (0.7%, 14.47, 1.8, 1%) 
0 >>  9/20 <<   Training | 0.6103 (22, 26, 13, 30) | 0.962 | 1.151 | 90.48 | 90.36 |---------------------------------------------------------------|
0             Validation | 0.6167 (22, 26, 12, 29) | 0.968 | 1.154 | 89.86 | 90.05 |############################################################| ^ (0.8%, 16.70, 2.0, 0%) 
0 >> 10/20 <<   Training | 0.6359 (20, 22,  8, 40) | 0.887 | 1.075 | 90.71 | 89.83 |----------------------------------------------------------|
0             Validation | 0.6418 (21, 22,  8, 40) | 0.900 | 1.086 | 90.20 | 89.53 |#######################################################| ^ (0.8%, 16.53, 1.9, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (137 batches)
0 >> 11/20 <<   Training | 0.6076 (20, 26, 11, 33) | 0.961 | 1.150 | 90.81 | 90.46 |----------------------------------------------------------------|
0             Validation | 0.6145 (21, 26, 11, 33) | 0.950 | 1.136 | 90.12 | 90.13 |#############################################################| ^ (0.8%, 16.31, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.6051 (22, 29, 10, 30) | 0.971 | 1.163 | 90.79 | 90.46 |----------------------------------------------------------------|
0             Validation | 0.6125 (22, 29, 10, 30) | 0.986 | 1.175 | 90.08 | 90.10 |############################################################| ^ (0.9%, 16.05, 2.8, 0%) 
0 >> 13/20 <<   Training | 0.6051 (23, 29, 12, 27) | 0.967 | 1.155 | 90.62 | 90.50 |-----------------------------------------------------------------|
0             Validation | 0.6125 (24, 28, 12, 27) | 0.974 | 1.161 | 89.93 | 90.15 |#############################################################| ^ (0.9%, 15.60, 1.9, 1%) 
0 >> 14/20 <<   Training | 0.6042 (21, 27, 11, 31) | 0.963 | 1.151 | 90.83 | 90.49 |----------------------------------------------------------------|
0             Validation | 0.6119 (22, 27, 11, 30) | 0.987 | 1.179 | 90.06 | 90.12 |#############################################################| ^ (0.9%, 14.31, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.6043 (22, 27, 11, 29) | 0.986 | 1.179 | 90.64 | 90.49 |----------------------------------------------------------------|
0             Validation | 0.6109 (23, 27, 11, 29) | 0.994 | 1.185 | 89.96 | 90.17 |#############################################################| ^ (0.8%, 15.73, 1.7, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5995 (24, 27, 11, 28) | 0.995 | 1.188 | 90.83 | 90.60 |------------------------------------------------------------------|
0             Validation | 0.6076 (25, 26, 11, 28) | 1.015 | 1.213 | 90.11 | 90.24 |##############################################################| ^ (0.9%, 16.23, 1.9, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5989 (24, 27, 11, 29) | 0.993 | 1.186 | 90.86 | 90.62 |------------------------------------------------------------------|
0             Validation | 0.6072 (25, 27, 11, 29) | 1.006 | 1.201 | 90.12 | 90.25 |##############################################################| ^ (0.9%, 16.02, 2.1, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5986 (24, 27, 11, 29) | 0.995 | 1.186 | 90.89 | 90.62 |------------------------------------------------------------------|
0             Validation | 0.6070 (24, 26, 11, 29) | 1.010 | 1.205 | 90.15 | 90.25 |##############################################################| ^ (0.9%, 16.06, 2.6, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5985 (23, 27, 11, 30) | 0.995 | 1.187 | 90.90 | 90.62 |------------------------------------------------------------------|
0             Validation | 0.6070 (24, 26, 11, 29) | 1.012 | 1.208 | 90.16 | 90.25 |##############################################################| ^ (0.9%, 16.06, 2.3, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5985 (23, 27, 11, 30) | 0.994 | 1.186 | 90.90 | 90.62 |------------------------------------------------------------------|
0             Validation | 0.6070 (24, 26, 11, 29) | 1.012 | 1.207 | 90.16 | 90.25 |##############################################################| ^ (0.9%, 16.06, 2.4, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6574 (28, 33, 10, 30) | 0.721 | 0.840 | 89.03 | 86.72 |---------------------------|
0             Validation | 0.6624 (29, 33,  9, 30) | 0.720 | 0.837 | 88.69 | 86.49 |########################| ^ (0.6%, 13.92, 1.6, 2%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (996 batches)
0 >>  2/20 <<   Training | 0.6198 (27, 34, 10, 30) | 0.828 | 0.958 | 89.93 | 88.19 |-----------------------------------------|
0             Validation | 0.6270 (28, 34, 10, 29) | 0.813 | 0.940 | 89.36 | 87.98 |#######################################| ^ (0.6%, 14.51, 2.2, 0%) 
0 >>  3/20 <<   Training | 0.6164 (29, 31,  9, 33) | 0.804 | 0.930 | 89.96 | 88.28 |------------------------------------------|
0             Validation | 0.6255 (30, 30,  9, 32) | 0.785 | 0.910 | 89.35 | 88.00 |#######################################| ^ (0.7%, 12.01, 1.5, 6%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (498 batches)
0 >>  4/20 <<   Training | 0.6094 (24, 28,  9, 40) | 0.848 | 0.980 | 90.92 | 88.56 |---------------------------------------------|
0             Validation | 0.6162 (25, 27,  9, 40) | 0.845 | 0.978 | 90.37 | 88.33 |###########################################| ^ (0.6%, 10.99, 2.2, 0%) 
0 >>  5/20 <<   Training | 0.6022 (28, 31, 10, 33) | 0.867 | 1.001 | 90.49 | 88.70 |----------------------------------------------|
0             Validation | 0.6109 (29, 30, 10, 32) | 0.870 | 1.007 | 89.88 | 88.48 |############################################| ^ (0.6%, 13.03, 2.9, 0%) 
0 >>  6/20 <<   Training | 0.6014 (29, 30, 10, 32) | 0.874 | 1.007 | 90.67 | 88.69 |----------------------------------------------|
0             Validation | 0.6102 (30, 30, 10, 31) | 0.849 | 0.980 | 90.03 | 88.47 |############################################| ^ (0.6%, 14.85, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (249 batches)
0 >>  7/20 <<   Training | 0.5991 (28, 32, 10, 31) | 0.891 | 1.030 | 90.78 | 88.74 |-----------------------------------------------|
0             Validation | 0.6100 (29, 31, 10, 30) | 0.875 | 1.012 | 90.15 | 88.41 |############################################| ^ (0.9%, 12.14, 2.5, 0%) 
0 >>  8/20 <<   Training | 0.5947 (26, 32, 10, 33) | 0.874 | 1.009 | 91.07 | 88.91 |-------------------------------------------------|
0             Validation | 0.6041 (27, 32, 10, 32) | 0.860 | 0.996 | 90.39 | 88.64 |##############################################| ^ (0.7%, 13.81, 3.1, 0%) 
0 >>  9/20 <<   Training | 0.5919 (27, 31, 10, 34) | 0.909 | 1.049 | 91.07 | 89.05 |--------------------------------------------------|
0             Validation | 0.6022 (28, 30, 10, 33) | 0.898 | 1.038 | 90.38 | 88.75 |###############################################| ^ (0.8%, 12.73, 2.3, 0%) 
0 >> 10/20 <<   Training | 0.5948 (24, 33, 10, 34) | 0.891 | 1.027 | 91.05 | 88.98 |-------------------------------------------------|
0             Validation | 0.6054 (26, 32, 10, 33) | 0.877 | 1.016 | 90.25 | 88.67 |##############################################| ^ (0.8%, 13.06, 2.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (124 batches)
0 >> 11/20 <<   Training | 0.5951 (24, 28,  9, 40) | 0.903 | 1.043 | 91.34 | 89.10 |--------------------------------------------------|
0             Validation | 0.6052 (25, 28,  9, 39) | 0.905 | 1.046 | 90.57 | 88.76 |###############################################| ^ (0.9%, 13.19, 2.3, 0%) 
0 >> 12/20 <<   Training | 0.5970 (27, 34,  8, 32) | 0.904 | 1.044 | 91.14 | 88.94 |-------------------------------------------------|
0             Validation | 0.6095 (28, 33,  8, 31) | 0.880 | 1.018 | 90.24 | 88.57 |#############################################| ^ (0.9%, 13.68, 2.2, 0%) 
0 >> 13/20 <<   Training | 0.5918 (26, 31, 11, 34) | 0.903 | 1.041 | 91.22 | 89.03 |--------------------------------------------------|
0             Validation | 0.6046 (28, 30, 10, 33) | 0.885 | 1.025 | 90.31 | 88.65 |##############################################| ^ (1.0%, 13.55, 2.2, 0%) 
0 >> 14/20 <<   Training | 0.5904 (28, 31, 10, 32) | 0.910 | 1.051 | 91.13 | 89.08 |--------------------------------------------------|
0             Validation | 0.6032 (30, 31, 10, 31) | 0.889 | 1.027 | 90.30 | 88.72 |###############################################| ^ (0.9%, 13.02, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.5914 (28, 31,  9, 33) | 0.890 | 1.027 | 91.11 | 89.03 |--------------------------------------------------|
0             Validation | 0.6048 (30, 30,  9, 32) | 0.882 | 1.020 | 90.28 | 88.68 |##############################################| ^ (0.9%, 14.16, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5862 (27, 31, 10, 33) | 0.922 | 1.064 | 91.35 | 89.22 |----------------------------------------------------|
0             Validation | 0.6001 (29, 30, 10, 32) | 0.910 | 1.051 | 90.43 | 88.83 |################################################| ^ (1.0%, 13.32, 2.8, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5853 (27, 31, 10, 34) | 0.925 | 1.066 | 91.41 | 89.24 |----------------------------------------------------|
0             Validation | 0.5994 (29, 30, 10, 33) | 0.906 | 1.047 | 90.49 | 88.84 |################################################| ^ (1.0%, 13.55, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5852 (26, 31, 10, 34) | 0.921 | 1.062 | 91.43 | 89.24 |----------------------------------------------------|
0             Validation | 0.5993 (28, 30, 10, 33) | 0.907 | 1.046 | 90.50 | 88.84 |################################################| ^ (1.0%, 13.51, 2.0, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5852 (26, 31, 10, 34) | 0.923 | 1.064 | 91.42 | 89.24 |----------------------------------------------------|
0             Validation | 0.5993 (28, 30, 10, 33) | 0.905 | 1.045 | 90.49 | 88.84 |################################################| ^ (1.0%, 13.54, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5852 (26, 31, 10, 34) | 0.923 | 1.064 | 91.42 | 89.24 |----------------------------------------------------|
0             Validation | 0.5993 (28, 30, 10, 33) | 0.905 | 1.045 | 90.49 | 88.84 |################################################| ^ (1.0%, 13.55, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
