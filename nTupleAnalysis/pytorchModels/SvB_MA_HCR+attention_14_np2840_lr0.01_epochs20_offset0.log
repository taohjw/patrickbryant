0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7782 (25, 30,  8, 37) | 0.827 |   0.0 | 89.23 | 86.08 |--------------------|
0             Validation | 0.7847 (26, 30,  8, 37) | 0.780 |   0.0 | 88.94 | 85.79 |#################| ^ (0.8%, 50.74, 1.8, 1%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (857 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7435 (31, 34,  7, 28) | 0.817 |   0.0 | 89.64 | 86.97 |-----------------------------|
0             Validation | 0.7549 (32, 33,  7, 28) | 0.773 |   0.0 | 89.05 | 86.61 |##########################| ^ (1.0%, 41.56, 2.2, 0%) 
0 >>  3/20 <<   Training | 0.7726 (36, 33,  7, 25) | 0.774 |   0.0 | 88.29 | 86.34 |-----------------------|
0             Validation | 0.7851 (37, 32,  7, 24) | 0.743 |   0.0 | 87.75 | 86.03 |####################| ^ (0.9%, 45.10, 2.3, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (428 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7642 (39, 27,  7, 27) | 0.773 |   0.0 | 88.90 | 86.68 |--------------------------|
0             Validation | 0.7764 (40, 27,  7, 27) | 0.747 |   0.0 | 88.43 | 86.35 |#######################| ^ (0.9%, 45.76, 2.7, 0%) 
0 >>  5/20 <<   Training | 0.7537 (37, 30,  7, 26) | 0.811 |   0.0 | 89.42 | 86.71 |---------------------------|
0             Validation | 0.7675 (38, 29,  7, 26) | 0.783 |   0.0 | 88.85 | 86.36 |#######################| ^ (1.0%, 47.53, 2.5, 0%) 
0 >>  6/20 <<   Training | 0.7455 (36, 29,  7, 28) | 0.837 |   0.0 | 89.66 | 86.97 |-----------------------------|
0             Validation | 0.7590 (37, 28,  7, 27) | 0.813 |   0.0 | 89.15 | 86.61 |##########################| ^ (1.0%, 56.00, 2.6, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (214 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.7007 (30, 31,  6, 33) | 0.957 |   0.0 | 90.93 | 88.09 |----------------------------------------|
0             Validation | 0.7157 (32, 30,  6, 32) | 0.915 |   0.0 | 90.25 | 87.65 |####################################| ^ (1.2%, 41.62, 1.9, 1%) 
0 >>  8/20 <<   Training | 0.6996 (30, 32,  6, 32) | 0.961 |   0.0 | 90.87 | 88.15 |-----------------------------------------|
0             Validation | 0.7155 (32, 31,  6, 31) | 0.924 |   0.0 | 90.17 | 87.69 |####################################| ^ (1.2%, 44.98, 2.0, 0%) 
0 >>  9/20 <<   Training | 0.6988 (30, 31,  6, 33) | 0.958 |   0.0 | 90.92 | 88.16 |-----------------------------------------|
0             Validation | 0.7147 (31, 30,  6, 32) | 0.925 |   0.0 | 90.21 | 87.70 |#####################################| ^ (1.2%, 42.19, 2.0, 0%) 
0 >> 10/20 <<   Training | 0.6982 (30, 31,  6, 33) | 0.955 |   0.0 | 90.95 | 88.16 |-----------------------------------------|
0             Validation | 0.7143 (31, 30,  6, 32) | 0.926 |   0.0 | 90.22 | 87.69 |####################################| ^ (1.2%, 43.67, 1.6, 3%) 
Increase training batch size: 8192 -> 16384 (107 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6973 (30, 32,  6, 32) | 0.959 |   0.0 | 90.99 | 88.19 |-----------------------------------------|
0             Validation | 0.7135 (31, 31,  6, 31) | 0.927 |   0.0 | 90.24 | 87.73 |#####################################| ^ (1.2%, 45.11, 1.6, 3%) 
0 >> 12/20 <<   Training | 0.6971 (29, 32,  6, 33) | 0.962 |   0.0 | 91.01 | 88.19 |-----------------------------------------|
0             Validation | 0.7131 (31, 31,  6, 32) | 0.926 |   0.0 | 90.27 | 87.73 |#####################################| ^ (1.2%, 44.17, 1.6, 2%) 
0 >> 13/20 <<   Training | 0.6970 (30, 32,  6, 32) | 0.968 |   0.0 | 90.97 | 88.19 |-----------------------------------------|
0             Validation | 0.7134 (32, 31,  6, 31) | 0.930 |   0.0 | 90.23 | 87.73 |#####################################| ^ (1.2%, 44.13, 2.1, 0%) 
0 >> 14/20 <<   Training | 0.6968 (29, 32,  6, 33) | 0.964 |   0.0 | 91.03 | 88.19 |-----------------------------------------|
0             Validation | 0.7130 (31, 31,  6, 32) | 0.925 |   0.0 | 90.28 | 87.73 |#####################################| ^ (1.2%, 43.79, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.6964 (30, 32,  6, 32) | 0.966 |   0.0 | 91.01 | 88.21 |------------------------------------------|
0             Validation | 0.7131 (31, 31,  6, 32) | 0.934 |   0.0 | 90.25 | 87.74 |#####################################| ^ (1.2%, 43.75, 1.6, 2%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.6963 (30, 31,  6, 33) | 0.963 |   0.0 | 91.03 | 88.21 |------------------------------------------|
0             Validation | 0.7129 (31, 31,  6, 32) | 0.929 |   0.0 | 90.27 | 87.75 |#####################################| ^ (1.2%, 44.66, 1.6, 3%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.6962 (30, 31,  6, 33) | 0.967 |   0.0 | 91.03 | 88.21 |------------------------------------------|
0             Validation | 0.7129 (31, 31,  6, 32) | 0.930 |   0.0 | 90.26 | 87.75 |#####################################| ^ (1.2%, 44.46, 1.5, 5%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.6961 (30, 32,  6, 33) | 0.967 |   0.0 | 91.03 | 88.21 |------------------------------------------|
0             Validation | 0.7129 (31, 31,  6, 32) | 0.930 |   0.0 | 90.26 | 87.75 |#####################################| ^ (1.2%, 44.49, 1.5, 4%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.6961 (30, 32,  6, 32) | 0.967 |   0.0 | 91.02 | 88.21 |------------------------------------------|
0             Validation | 0.7129 (31, 31,  6, 32) | 0.930 |   0.0 | 90.26 | 87.75 |#####################################| ^ (1.2%, 44.47, 1.6, 4%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.6961 (30, 32,  6, 32) | 0.967 |   0.0 | 91.02 | 88.21 |------------------------------------------|
0             Validation | 0.7129 (31, 31,  6, 32) | 0.930 |   0.0 | 90.26 | 87.75 |#####################################| ^ (1.2%, 44.50, 1.6, 4%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 |   0.4 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 |   0.4 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 |   0.4 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.414 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.407 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.414 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7184 (34, 38,  9, 18) | 0.606 | 0.390 | 82.37 | 85.54 |---------------|
0             Validation | 0.7221 (35, 38,  9, 18) | 0.613 | 0.396 | 81.61 | 85.39 |#############| ^ (0.6%, 18.07, 1.4, 10%) 
0 >>  3/20 <<   Training | 0.7696 (36, 40,  8, 16) | 0.564 | 0.370 | 78.22 | 83.90 ||
0             Validation | 0.7733 (36, 40,  8, 16) | 0.554 | 0.365 | 77.65 | 83.74 || ^ (0.6%, 17.41, 1.7, 1%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7808 (37, 40,  7, 16) | 0.573 | 0.379 | 78.35 | 84.81 |--------|
0             Validation | 0.7850 (37, 40,  7, 16) | 0.587 | 0.372 | 77.94 | 84.64 |######| ^ (0.6%, 17.19, 1.2, 23%) 
0 >>  5/20 <<   Training | 0.8381 (38, 41,  6, 16) | 0.515 | 0.349 | 75.11 | 82.85 ||
0             Validation | 0.8421 (38, 40,  6, 16) | 0.516 | 0.343 | 74.96 | 82.69 || ^ (0.7%, 16.87, 1.6, 2%) 
0 >>  6/20 <<   Training | 0.8285 (38, 40,  7, 15) | 0.477 | 0.312 | 73.35 | 82.93 ||
0             Validation | 0.8323 (38, 40,  7, 15) | 0.479 | 0.309 | 73.23 | 82.76 || ^ (0.7%, 16.20, 1.0, 35%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5618 (26, 29, 13, 32) | 0.988 | 0.645 | 90.10 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5674 (27, 29, 13, 31) | 0.994 | 0.648 | 89.74 | 89.85 |##########################################################| ^ (0.6%, 16.50, 0.8, 69%) 
0 >>  8/20 <<   Training | 0.5614 (27, 29, 13, 31) | 0.984 | 0.640 | 90.09 | 90.10 |------------------------------------------------------------|
0             Validation | 0.5673 (27, 29, 13, 31) | 0.992 | 0.644 | 89.70 | 89.86 |##########################################################| ^ (0.7%, 16.80, 1.2, 23%) 
0 >>  9/20 <<   Training | 0.5623 (24, 30, 13, 33) | 0.964 | 0.629 | 90.20 | 90.06 |------------------------------------------------------------|
0             Validation | 0.5682 (25, 30, 13, 33) | 0.969 | 0.626 | 89.76 | 89.83 |##########################################################| ^ (0.6%, 16.07, 1.7, 1%) 
0 >> 10/20 <<   Training | 0.5606 (26, 29, 13, 32) | 0.985 | 0.646 | 90.13 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5668 (27, 28, 13, 32) | 0.990 | 0.646 | 89.70 | 89.89 |##########################################################| ^ (0.6%, 16.38, 1.4, 10%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5600 (26, 30, 13, 32) | 0.987 | 0.648 | 90.19 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5662 (26, 29, 12, 32) | 0.996 | 0.647 | 89.76 | 89.90 |##########################################################| ^ (0.7%, 16.21, 1.4, 9%) 
0 >> 12/20 <<   Training | 0.5600 (26, 30, 13, 31) | 0.988 | 0.646 | 90.15 | 90.15 |-------------------------------------------------------------|
0             Validation | 0.5664 (27, 30, 12, 31) | 0.987 | 0.649 | 89.71 | 89.90 |##########################################################| ^ (0.7%, 16.52, 1.2, 22%) 
0 >> 13/20 <<   Training | 0.5598 (25, 30, 12, 32) | 0.986 | 0.647 | 90.22 | 90.15 |-------------------------------------------------------------|
0             Validation | 0.5662 (26, 30, 12, 32) | 0.996 | 0.649 | 89.78 | 89.90 |##########################################################| ^ (0.7%, 16.06, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.5598 (25, 30, 13, 33) | 0.979 | 0.641 | 90.23 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5661 (26, 29, 12, 32) | 0.982 | 0.647 | 89.79 | 89.90 |##########################################################| ^ (0.7%, 16.30, 1.3, 15%) 
0 >> 15/20 <<   Training | 0.5595 (26, 29, 13, 32) | 0.989 | 0.649 | 90.21 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5660 (27, 29, 13, 31) | 1.000 | 0.648 | 89.77 | 89.90 |###########################################################| ^ (0.7%, 16.20, 1.1, 25%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5595 (26, 29, 13, 32) | 0.986 | 0.648 | 90.19 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5660 (27, 29, 12, 31) | 0.988 | 0.650 | 89.75 | 89.90 |###########################################################| ^ (0.7%, 16.35, 1.6, 4%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5594 (26, 30, 13, 32) | 0.989 | 0.648 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.996 | 0.647 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.20, 1.4, 7%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.989 | 0.647 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.997 | 0.648 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.21, 1.4, 8%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.988 | 0.648 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.996 | 0.650 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.20, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.988 | 0.648 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.995 | 0.649 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.22, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7184 (34, 38,  9, 18) | 0.606 | 0.750 | 82.37 | 85.54 |---------------|
0             Validation | 0.7221 (35, 38,  9, 18) | 0.613 | 0.748 | 81.61 | 85.39 |#############| ^ (0.6%, 18.07, 1.4, 10%) 
0 >>  3/20 <<   Training | 0.7696 (36, 40,  8, 16) | 0.564 | 0.693 | 78.22 | 83.90 ||
0             Validation | 0.7733 (36, 40,  8, 16) | 0.554 | 0.679 | 77.65 | 83.74 || ^ (0.6%, 17.41, 1.7, 1%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7808 (37, 40,  7, 16) | 0.573 | 0.719 | 78.35 | 84.81 |--------|
0             Validation | 0.7850 (37, 40,  7, 16) | 0.587 | 0.722 | 77.94 | 84.64 |######| ^ (0.6%, 17.19, 1.2, 23%) 
0 >>  5/20 <<   Training | 0.8381 (38, 41,  6, 16) | 0.515 | 0.654 | 75.11 | 82.85 ||
0             Validation | 0.8421 (38, 40,  6, 16) | 0.516 | 0.654 | 74.96 | 82.69 || ^ (0.7%, 16.87, 1.6, 2%) 
0 >>  6/20 <<   Training | 0.8285 (38, 40,  7, 15) | 0.477 | 0.624 | 73.35 | 82.93 ||
0             Validation | 0.8323 (38, 40,  7, 15) | 0.479 | 0.620 | 73.23 | 82.76 || ^ (0.7%, 16.20, 1.0, 35%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5618 (26, 29, 13, 32) | 0.988 | 1.178 | 90.10 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5674 (27, 29, 13, 31) | 0.994 | 1.187 | 89.74 | 89.85 |##########################################################| ^ (0.6%, 16.50, 0.8, 69%) 
0 >>  8/20 <<   Training | 0.5614 (27, 29, 13, 31) | 0.984 | 1.173 | 90.09 | 90.10 |------------------------------------------------------------|
0             Validation | 0.5673 (27, 29, 13, 31) | 0.992 | 1.185 | 89.70 | 89.86 |##########################################################| ^ (0.7%, 16.80, 1.2, 23%) 
0 >>  9/20 <<   Training | 0.5623 (24, 30, 13, 33) | 0.964 | 1.149 | 90.20 | 90.06 |------------------------------------------------------------|
0             Validation | 0.5682 (25, 30, 13, 33) | 0.969 | 1.159 | 89.76 | 89.83 |##########################################################| ^ (0.6%, 16.07, 1.7, 1%) 
0 >> 10/20 <<   Training | 0.5606 (26, 29, 13, 32) | 0.985 | 1.176 | 90.13 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5668 (27, 28, 13, 32) | 0.990 | 1.181 | 89.70 | 89.89 |##########################################################| ^ (0.6%, 16.38, 1.4, 10%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5600 (26, 30, 13, 32) | 0.987 | 1.176 | 90.19 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5662 (26, 29, 12, 32) | 0.996 | 1.188 | 89.76 | 89.90 |##########################################################| ^ (0.7%, 16.21, 1.4, 9%) 
0 >> 12/20 <<   Training | 0.5600 (26, 30, 13, 31) | 0.988 | 1.177 | 90.15 | 90.15 |-------------------------------------------------------------|
0             Validation | 0.5664 (27, 30, 12, 31) | 0.987 | 1.177 | 89.71 | 89.90 |##########################################################| ^ (0.7%, 16.52, 1.2, 22%) 
0 >> 13/20 <<   Training | 0.5598 (25, 30, 12, 32) | 0.986 | 1.176 | 90.22 | 90.15 |-------------------------------------------------------------|
0             Validation | 0.5662 (26, 30, 12, 32) | 0.996 | 1.189 | 89.78 | 89.90 |##########################################################| ^ (0.7%, 16.06, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.5598 (25, 30, 13, 33) | 0.979 | 1.167 | 90.23 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5661 (26, 29, 12, 32) | 0.982 | 1.173 | 89.79 | 89.90 |##########################################################| ^ (0.7%, 16.30, 1.3, 15%) 
0 >> 15/20 <<   Training | 0.5595 (26, 29, 13, 32) | 0.989 | 1.179 | 90.21 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5660 (27, 29, 13, 31) | 1.000 | 1.194 | 89.77 | 89.90 |###########################################################| ^ (0.7%, 16.20, 1.1, 25%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5595 (26, 29, 13, 32) | 0.986 | 1.177 | 90.19 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5660 (27, 29, 12, 31) | 0.988 | 1.181 | 89.75 | 89.90 |###########################################################| ^ (0.7%, 16.35, 1.6, 4%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5594 (26, 30, 13, 32) | 0.989 | 1.179 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.996 | 1.188 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.20, 1.4, 7%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.989 | 1.178 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.997 | 1.189 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.21, 1.4, 8%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.988 | 1.178 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.996 | 1.188 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.20, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5593 (26, 29, 13, 32) | 0.988 | 1.179 | 90.22 | 90.16 |-------------------------------------------------------------|
0             Validation | 0.5659 (27, 29, 12, 32) | 0.995 | 1.187 | 89.77 | 89.91 |###########################################################| ^ (0.7%, 16.22, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(0)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.5770 (25, 30, 12, 33) | 0.920 | 1.100 | 89.36 | 89.61 |--------------------------------------------------------|
0             Validation | 0.5808 (25, 30, 12, 33) | 0.943 | 1.127 | 89.00 | 89.46 |######################################################| ^ (0.4%, 16.80, 1.2, 21%) 
0 >>  3/20 <<   Training | 0.5787 (29, 33, 12, 26) | 0.926 | 1.105 | 89.15 | 89.71 |---------------------------------------------------------|
0             Validation | 0.5837 (30, 33, 12, 26) | 0.923 | 1.100 | 88.80 | 89.49 |######################################################| ^ (0.6%, 17.99, 0.8, 71%) 
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.5665 (27, 30, 13, 30) | 0.969 | 1.157 | 89.77 | 89.95 |-----------------------------------------------------------|
0             Validation | 0.5715 (28, 29, 13, 30) | 0.985 | 1.176 | 89.43 | 89.74 |#########################################################| ^ (0.6%, 17.09, 0.9, 56%) 
0 >>  5/20 <<   Training | 0.5663 (27, 32, 12, 30) | 0.970 | 1.159 | 89.89 | 90.00 |-----------------------------------------------------------|
0             Validation | 0.5716 (27, 32, 12, 29) | 0.986 | 1.176 | 89.55 | 89.78 |#########################################################| ^ (0.6%, 15.75, 1.1, 29%) 
0 >>  6/20 <<   Training | 0.5635 (26, 29, 12, 33) | 0.976 | 1.166 | 90.02 | 90.04 |------------------------------------------------------------|
0             Validation | 0.5686 (27, 28, 12, 32) | 0.983 | 1.173 | 89.68 | 89.81 |##########################################################| ^ (0.6%, 14.92, 1.2, 17%) 
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5619 (26, 29, 13, 33) | 0.983 | 1.173 | 90.07 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5675 (26, 29, 12, 32) | 0.991 | 1.182 | 89.72 | 89.85 |##########################################################| ^ (0.6%, 15.56, 1.4, 9%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(-1)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.5759 (26, 29, 12, 32) | 0.943 | 1.126 | 89.41 | 89.62 |--------------------------------------------------------|
0             Validation | 0.5800 (27, 29, 12, 32) | 0.956 | 1.145 | 89.05 | 89.47 |######################################################| ^ (0.5%, 16.81, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.5765 (30, 31, 12, 27) | 0.935 | 1.115 | 89.13 | 89.77 |---------------------------------------------------------|
0             Validation | 0.5819 (30, 31, 12, 27) | 0.936 | 1.116 | 88.80 | 89.55 |#######################################################| ^ (0.6%, 16.78, 0.9, 56%) 
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.5656 (27, 30, 13, 31) | 0.975 | 1.165 | 89.78 | 89.99 |-----------------------------------------------------------|
0             Validation | 0.5710 (28, 30, 13, 30) | 0.985 | 1.178 | 89.45 | 89.77 |#########################################################| ^ (0.6%, 16.09, 1.5, 6%) 
0 >>  5/20 <<   Training | 0.5663 (27, 31, 12, 30) | 0.969 | 1.157 | 89.90 | 90.00 |-----------------------------------------------------------|
0             Validation | 0.5722 (28, 31, 11, 30) | 0.972 | 1.161 | 89.58 | 89.76 |#########################################################| ^ (0.6%, 15.08, 1.0, 36%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7359 (35, 40,  9, 16) | 0.612 | 0.753 | 82.52 | 85.48 |--------------|
0             Validation | 0.7399 (35, 40,  9, 16) | 0.619 | 0.754 | 81.71 | 85.31 |#############| ^ (0.7%, 18.46, 0.6, 92%) 
0 >>  3/20 <<   Training | 0.8123 (36, 43,  7, 14) | 0.526 | 0.653 | 77.38 | 83.44 ||
0             Validation | 0.8158 (36, 42,  7, 14) | 0.516 | 0.636 | 76.87 | 83.27 || ^ (0.6%, 17.59, 1.8, 1%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8324 (37, 44,  7, 13) | 0.563 | 0.701 | 78.28 | 84.44 |----|
0             Validation | 0.8365 (37, 43,  7, 13) | 0.574 | 0.704 | 77.87 | 84.26 |##| ^ (0.6%, 17.55, 1.6, 3%) 
0 >>  5/20 <<   Training | 0.8729 (37, 44,  7, 12) | 0.504 | 0.636 | 76.03 | 82.57 ||
0             Validation | 0.8772 (38, 43,  7, 12) | 0.497 | 0.634 | 75.74 | 82.39 || ^ (0.7%, 17.30, 1.2, 19%) 
0 >>  6/20 <<   Training | 0.8681 (38, 43,  7, 12) | 0.479 | 0.618 | 74.20 | 82.65 ||
0             Validation | 0.8718 (38, 43,  7, 12) | 0.486 | 0.617 | 73.99 | 82.49 || ^ (0.7%, 17.02, 1.7, 2%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5641 (26, 29, 13, 32) | 0.976 | 1.164 | 89.97 | 89.99 |-----------------------------------------------------------|
0             Validation | 0.5696 (27, 29, 12, 32) | 0.980 | 1.172 | 89.64 | 89.78 |#########################################################| ^ (0.6%, 17.51, 1.9, 0%) 
0 >>  8/20 <<   Training | 0.5629 (26, 29, 13, 32) | 0.982 | 1.170 | 90.00 | 90.04 |------------------------------------------------------------|
0             Validation | 0.5685 (27, 29, 12, 32) | 0.994 | 1.187 | 89.67 | 89.81 |##########################################################| ^ (0.6%, 17.37, 1.6, 4%) 
0 >>  9/20 <<   Training | 0.5638 (24, 30, 12, 33) | 0.959 | 1.143 | 90.11 | 90.01 |------------------------------------------------------------|
0             Validation | 0.5695 (25, 30, 12, 33) | 0.969 | 1.161 | 89.72 | 89.77 |#########################################################| ^ (0.6%, 16.62, 1.5, 5%) 
0 >> 10/20 <<   Training | 0.5617 (26, 28, 13, 33) | 0.986 | 1.176 | 90.10 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5676 (27, 28, 13, 33) | 0.993 | 1.185 | 89.75 | 89.85 |##########################################################| ^ (0.6%, 16.95, 0.9, 54%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5611 (25, 30, 13, 32) | 0.985 | 1.176 | 90.12 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5669 (26, 30, 13, 32) | 0.997 | 1.193 | 89.74 | 89.86 |##########################################################| ^ (0.6%, 16.82, 1.6, 3%) 
0 >> 12/20 <<   Training | 0.5608 (26, 30, 13, 31) | 0.986 | 1.174 | 90.09 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5669 (27, 30, 12, 31) | 0.994 | 1.187 | 89.72 | 89.87 |##########################################################| ^ (0.6%, 16.93, 1.4, 9%) 
0 >> 13/20 <<   Training | 0.5609 (26, 30, 13, 31) | 0.986 | 1.176 | 90.14 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5669 (26, 30, 13, 31) | 1.000 | 1.194 | 89.75 | 89.87 |##########################################################| ^ (0.6%, 16.71, 1.5, 6%) 
0 >> 14/20 <<   Training | 0.5608 (25, 29, 13, 33) | 0.978 | 1.166 | 90.17 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5667 (26, 29, 12, 33) | 0.985 | 1.178 | 89.79 | 89.87 |##########################################################| ^ (0.6%, 16.72, 1.0, 37%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6471 (26, 31, 18, 24) | 0.657 | 0.805 | 87.59 | 87.34 |---------------------------------|
0             Validation | 0.6510 (27, 31, 18, 24) | 0.664 | 0.808 | 87.28 | 87.20 |###############################| ^ (0.4%, 14.71, 1.1, 24%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6221 (29, 31, 17, 23) | 0.744 | 0.906 | 87.91 | 88.36 |-------------------------------------------|
0             Validation | 0.6262 (29, 31, 17, 23) | 0.747 | 0.904 | 87.43 | 88.23 |##########################################| ^ (0.4%, 16.86, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6248 (31, 30, 15, 24) | 0.759 | 0.916 | 87.41 | 88.06 |----------------------------------------|
0             Validation | 0.6293 (31, 30, 15, 24) | 0.753 | 0.904 | 86.84 | 87.93 |#######################################| ^ (0.4%, 19.08, 2.2, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6137 (31, 32, 14, 23) | 0.818 | 0.993 | 87.91 | 88.49 |--------------------------------------------|
0             Validation | 0.6191 (32, 32, 14, 23) | 0.819 | 0.992 | 87.25 | 88.33 |###########################################| ^ (0.4%, 18.58, 1.4, 7%) 
0 >>  5/20 <<   Training | 0.6188 (32, 32, 13, 23) | 0.808 | 0.988 | 87.92 | 88.07 |----------------------------------------|
0             Validation | 0.6241 (32, 32, 13, 23) | 0.797 | 0.969 | 87.41 | 87.90 |######################################| ^ (0.5%, 17.97, 1.1, 24%) 
0 >>  6/20 <<   Training | 0.6253 (32, 30, 11, 26) | 0.790 | 0.971 | 86.88 | 87.82 |--------------------------------------|
0             Validation | 0.6306 (33, 30, 11, 26) | 0.780 | 0.950 | 86.26 | 87.65 |####################################| ^ (0.5%, 17.08, 1.4, 9%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5643 (27, 29, 13, 32) | 0.959 | 1.147 | 90.02 | 90.01 |------------------------------------------------------------|
0             Validation | 0.5698 (28, 28, 13, 31) | 0.963 | 1.150 | 89.63 | 89.78 |#########################################################| ^ (0.6%, 15.82, 0.9, 51%) 
0 >>  8/20 <<   Training | 0.5633 (27, 30, 13, 31) | 0.971 | 1.159 | 90.08 | 90.04 |------------------------------------------------------------|
0             Validation | 0.5692 (28, 29, 13, 30) | 0.973 | 1.162 | 89.68 | 89.80 |#########################################################| ^ (0.6%, 16.20, 0.6, 87%) 
0 >>  9/20 <<   Training | 0.5645 (24, 31, 13, 32) | 0.944 | 1.129 | 90.23 | 89.98 |-----------------------------------------------------------|
0             Validation | 0.5704 (25, 30, 13, 32) | 0.939 | 1.124 | 89.77 | 89.73 |#########################################################| ^ (0.6%, 15.49, 1.2, 23%) 
0 >> 10/20 <<   Training | 0.5621 (26, 29, 13, 32) | 0.974 | 1.164 | 90.15 | 90.07 |------------------------------------------------------------|
0             Validation | 0.5681 (27, 28, 13, 32) | 0.980 | 1.171 | 89.72 | 89.83 |##########################################################| ^ (0.6%, 15.89, 0.8, 72%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5616 (26, 30, 13, 32) | 0.972 | 1.160 | 90.19 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5678 (26, 30, 13, 31) | 0.983 | 1.174 | 89.71 | 89.84 |##########################################################| ^ (0.6%, 15.78, 1.0, 38%) 
0 >> 12/20 <<   Training | 0.5614 (26, 30, 12, 32) | 0.978 | 1.168 | 90.17 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5677 (27, 29, 12, 31) | 0.981 | 1.172 | 89.69 | 89.84 |##########################################################| ^ (0.6%, 15.90, 0.9, 50%) 
0 >> 13/20 <<   Training | 0.5614 (25, 30, 13, 32) | 0.974 | 1.163 | 90.24 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5677 (26, 30, 13, 31) | 0.974 | 1.164 | 89.75 | 89.84 |##########################################################| ^ (0.7%, 15.62, 0.8, 67%) 
0 >> 14/20 <<   Training | 0.5615 (25, 29, 13, 33) | 0.965 | 1.151 | 90.25 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5677 (26, 29, 13, 32) | 0.963 | 1.151 | 89.76 | 89.84 |##########################################################| ^ (0.6%, 15.86, 0.9, 50%) 
0 >> 15/20 <<   Training | 0.5612 (26, 29, 13, 32) | 0.978 | 1.168 | 90.21 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5676 (27, 29, 12, 32) | 0.980 | 1.172 | 89.72 | 89.84 |##########################################################| ^ (0.6%, 15.58, 1.0, 37%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5610 (26, 29, 13, 32) | 0.980 | 1.170 | 90.21 | 90.10 |------------------------------------------------------------|
0             Validation | 0.5674 (27, 29, 13, 32) | 0.977 | 1.168 | 89.71 | 89.85 |##########################################################| ^ (0.6%, 15.87, 1.0, 41%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5609 (26, 29, 13, 32) | 0.980 | 1.171 | 90.23 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5674 (27, 29, 12, 32) | 0.980 | 1.172 | 89.72 | 89.85 |##########################################################| ^ (0.6%, 15.81, 0.7, 73%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5609 (26, 29, 13, 32) | 0.979 | 1.170 | 90.23 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5673 (27, 29, 13, 32) | 0.980 | 1.172 | 89.72 | 89.85 |##########################################################| ^ (0.6%, 15.80, 0.9, 47%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5609 (26, 29, 13, 32) | 0.980 | 1.171 | 90.24 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5673 (27, 29, 12, 32) | 0.980 | 1.172 | 89.73 | 89.85 |##########################################################| ^ (0.6%, 15.80, 0.9, 48%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5609 (26, 29, 13, 32) | 0.979 | 1.170 | 90.23 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5673 (27, 29, 13, 32) | 0.980 | 1.173 | 89.73 | 89.85 |##########################################################| ^ (0.6%, 15.82, 0.9, 57%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7420 (32, 40, 12, 15) | 0.470 | 0.631 | 80.42 | 84.26 |--|
0             Validation | 0.7460 (32, 40, 12, 15) | 0.471 | 0.636 | 79.81 | 84.09 || ^ (0.7%, 16.07, 1.7, 2%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7740 (35, 37,  7, 21) | 0.514 | 0.651 | 79.46 | 84.05 ||
0             Validation | 0.7769 (35, 37,  7, 21) | 0.527 | 0.660 | 79.06 | 83.91 || ^ (0.7%, 17.51, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.7988 (37, 36,  7, 21) | 0.464 | 0.593 | 73.62 | 82.51 ||
0             Validation | 0.8016 (37, 36,  7, 21) | 0.468 | 0.583 | 73.25 | 82.38 || ^ (0.8%, 15.92, 0.8, 62%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8700 (36, 36,  4, 23) | 0.438 | 0.582 | 72.05 | 83.24 ||
0             Validation | 0.8735 (36, 36,  4, 23) | 0.444 | 0.580 | 71.34 | 83.10 || ^ (0.7%, 17.02, 1.2, 23%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 1.7364 (35,  6, 12, 48) | 0.114 | 0.245 | 49.18 | 68.38 ||
0             Validation | 1.7318 (34,  6, 12, 48) | 0.113 | 0.246 | 49.45 | 68.29 || ^ (0.6%, 32.38, 1.3, 15%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 1.5442 (17,  5,  3, 75) | 0.228 | 0.404 | 68.25 | 75.25 ||
0             Validation | 1.5437 (17,  5,  3, 75) | 0.227 | 0.406 | 67.88 | 75.41 || ^ (0.7%, 16.25, 2.2, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 2.0248 ( 9,  3,  3, 84) | 0.174 | 0.334 | 80.05 | 77.83 ||
0             Validation | 2.0299 (10,  3,  3, 84) | 0.171 | 0.326 | 79.61 | 77.55 || ^ (1.0%, 13.46, 1.3, 13%) 
0 >>  3/20 <<   Training | 2.2571 (17, 12,  3, 67) | 0.384 | 0.552 | 70.15 | 79.09 ||
0             Validation | 2.2594 (18, 12,  3, 67) | 0.386 | 0.557 | 69.93 | 78.85 || ^ (0.9%, 108.03, 1.3, 12%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8125 (26, 16,  6, 52) | 0.477 | 0.616 | 80.60 | 85.62 |----------------|
0             Validation | 0.8140 (26, 16,  6, 52) | 0.477 | 0.624 | 80.08 | 85.51 |###############| ^ (0.5%, 47.92, 0.8, 65%) 
0 >>  5/20 <<   Training | 0.9203 (22, 11,  5, 61) | 0.512 | 0.666 | 81.24 | 84.31 |---|
0             Validation | 0.9231 (22, 11,  5, 61) | 0.508 | 0.662 | 80.79 | 84.21 |##| ^ (0.5%, 24.94, 2.0, 0%) 
0 >>  6/20 <<   Training | 5.1555 (14, 10,  1, 75) | 0.311 | 0.514 | 71.50 | 74.13 ||
0             Validation | 5.1333 (14, 10,  1, 76) | 0.316 | 0.524 | 71.32 | 74.03 || ^ (0.7%, 335.75, 1.4, 7%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5835 (27, 29, 13, 31) | 0.905 | 1.083 | 88.98 | 89.47 |------------------------------------------------------|
0             Validation | 0.5890 (27, 29, 13, 31) | 0.909 | 1.088 | 88.67 | 89.23 |####################################################| ^ (0.6%, 17.47, 1.1, 26%) 
0 >>  8/20 <<   Training | 0.5795 (27, 29, 13, 32) | 0.919 | 1.101 | 89.13 | 89.58 |-------------------------------------------------------|
0             Validation | 0.5850 (27, 29, 13, 31) | 0.920 | 1.100 | 88.86 | 89.33 |#####################################################| ^ (0.7%, 17.63, 2.1, 0%) 
0 >>  9/20 <<   Training | 0.5778 (25, 30, 13, 33) | 0.927 | 1.110 | 89.42 | 89.61 |--------------------------------------------------------|
0             Validation | 0.5831 (25, 29, 13, 33) | 0.928 | 1.108 | 89.10 | 89.37 |#####################################################| ^ (0.7%, 17.99, 1.4, 9%) 
0 >> 10/20 <<   Training | 0.5756 (26, 29, 13, 31) | 0.940 | 1.126 | 89.40 | 89.68 |--------------------------------------------------------|
0             Validation | 0.5812 (27, 29, 13, 31) | 0.928 | 1.109 | 89.12 | 89.43 |######################################################| ^ (0.7%, 17.10, 1.5, 6%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5747 (26, 30, 13, 32) | 0.942 | 1.128 | 89.47 | 89.70 |--------------------------------------------------------|
0             Validation | 0.5803 (26, 30, 13, 31) | 0.942 | 1.125 | 89.14 | 89.45 |######################################################| ^ (0.7%, 17.08, 1.7, 2%) 
0 >> 12/20 <<   Training | 0.5745 (26, 30, 13, 31) | 0.941 | 1.128 | 89.45 | 89.71 |---------------------------------------------------------|
0             Validation | 0.5802 (27, 30, 13, 31) | 0.940 | 1.122 | 89.11 | 89.45 |######################################################| ^ (0.7%, 17.25, 1.7, 2%) 
0 >> 13/20 <<   Training | 0.5743 (26, 30, 13, 31) | 0.942 | 1.129 | 89.52 | 89.71 |---------------------------------------------------------|
0             Validation | 0.5800 (27, 30, 13, 31) | 0.942 | 1.126 | 89.19 | 89.45 |######################################################| ^ (0.7%, 17.14, 2.1, 0%) 
0 >> 14/20 <<   Training | 0.5741 (26, 29, 13, 32) | 0.938 | 1.124 | 89.53 | 89.72 |---------------------------------------------------------|
0             Validation | 0.5797 (27, 29, 13, 32) | 0.932 | 1.116 | 89.20 | 89.46 |######################################################| ^ (0.7%, 17.39, 1.3, 14%) 
0 >> 15/20 <<   Training | 0.5737 (26, 29, 13, 32) | 0.943 | 1.130 | 89.53 | 89.73 |---------------------------------------------------------|
0             Validation | 0.5795 (27, 29, 12, 32) | 0.942 | 1.125 | 89.21 | 89.46 |######################################################| ^ (0.7%, 16.97, 1.8, 1%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5733 (26, 29, 13, 32) | 0.941 | 1.127 | 89.55 | 89.74 |---------------------------------------------------------|
0             Validation | 0.5792 (27, 29, 13, 32) | 0.938 | 1.121 | 89.18 | 89.47 |######################################################| ^ (0.7%, 17.32, 1.6, 2%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5731 (26, 29, 13, 32) | 0.943 | 1.129 | 89.57 | 89.74 |---------------------------------------------------------|
0             Validation | 0.5790 (26, 29, 13, 32) | 0.940 | 1.124 | 89.21 | 89.48 |######################################################| ^ (0.7%, 17.11, 1.1, 26%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5731 (26, 29, 13, 32) | 0.948 | 1.135 | 89.57 | 89.74 |---------------------------------------------------------|
0             Validation | 0.5789 (27, 29, 13, 31) | 0.942 | 1.126 | 89.21 | 89.48 |######################################################| ^ (0.7%, 17.12, 1.1, 33%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5730 (26, 29, 13, 32) | 0.947 | 1.133 | 89.57 | 89.75 |---------------------------------------------------------|
0             Validation | 0.5789 (27, 29, 13, 32) | 0.943 | 1.127 | 89.21 | 89.48 |######################################################| ^ (0.7%, 17.09, 1.1, 33%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5730 (26, 29, 13, 32) | 0.947 | 1.134 | 89.57 | 89.75 |---------------------------------------------------------|
0             Validation | 0.5789 (27, 29, 13, 31) | 0.943 | 1.127 | 89.21 | 89.48 |######################################################| ^ (0.7%, 17.12, 1.0, 34%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7359 (35, 40,  9, 16) | 0.612 | 0.753 | 82.52 | 85.48 |--------------|
0             Validation | 0.7399 (35, 40,  9, 16) | 0.619 | 0.754 | 81.71 | 85.31 |#############| ^ (0.7%, 18.46, 0.6, 92%) 
0 >>  3/20 <<   Training | 0.8123 (36, 43,  7, 14) | 0.526 | 0.653 | 77.38 | 83.44 ||
0             Validation | 0.8158 (36, 42,  7, 14) | 0.516 | 0.636 | 76.87 | 83.27 || ^ (0.6%, 17.59, 1.8, 1%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8324 (37, 44,  7, 13) | 0.563 | 0.701 | 78.28 | 84.44 |----|
0             Validation | 0.8365 (37, 43,  7, 13) | 0.574 | 0.704 | 77.87 | 84.26 |##| ^ (0.6%, 17.55, 1.6, 3%) 
0 >>  5/20 <<   Training | 0.8729 (37, 44,  7, 12) | 0.504 | 0.636 | 76.03 | 82.57 ||
0             Validation | 0.8772 (38, 43,  7, 12) | 0.497 | 0.634 | 75.74 | 82.39 || ^ (0.7%, 17.30, 1.2, 19%) 
0 >>  6/20 <<   Training | 0.8681 (38, 43,  7, 12) | 0.479 | 0.618 | 74.20 | 82.65 ||
0             Validation | 0.8718 (38, 43,  7, 12) | 0.486 | 0.617 | 73.99 | 82.49 || ^ (0.7%, 17.02, 1.7, 2%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5641 (26, 29, 13, 32) | 0.976 | 1.164 | 89.97 | 89.99 |-----------------------------------------------------------|
0             Validation | 0.5696 (27, 29, 12, 32) | 0.980 | 1.172 | 89.64 | 89.78 |#########################################################| ^ (0.6%, 17.51, 1.9, 0%) 
0 >>  8/20 <<   Training | 0.5629 (26, 29, 13, 32) | 0.982 | 1.170 | 90.00 | 90.04 |------------------------------------------------------------|
0             Validation | 0.5685 (27, 29, 12, 32) | 0.994 | 1.187 | 89.67 | 89.81 |##########################################################| ^ (0.6%, 17.37, 1.6, 4%) 
0 >>  9/20 <<   Training | 0.5638 (24, 30, 12, 33) | 0.959 | 1.143 | 90.11 | 90.01 |------------------------------------------------------------|
0             Validation | 0.5695 (25, 30, 12, 33) | 0.969 | 1.161 | 89.72 | 89.77 |#########################################################| ^ (0.6%, 16.62, 1.5, 5%) 
0 >> 10/20 <<   Training | 0.5617 (26, 28, 13, 33) | 0.986 | 1.176 | 90.10 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5676 (27, 28, 13, 33) | 0.993 | 1.185 | 89.75 | 89.85 |##########################################################| ^ (0.6%, 16.95, 0.9, 54%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5611 (25, 30, 13, 32) | 0.985 | 1.176 | 90.12 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5669 (26, 30, 13, 32) | 0.997 | 1.193 | 89.74 | 89.86 |##########################################################| ^ (0.6%, 16.82, 1.6, 3%) 
0 >> 12/20 <<   Training | 0.5608 (26, 30, 13, 31) | 0.986 | 1.174 | 90.09 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5669 (27, 30, 12, 31) | 0.994 | 1.187 | 89.72 | 89.87 |##########################################################| ^ (0.6%, 16.93, 1.4, 9%) 
0 >> 13/20 <<   Training | 0.5609 (26, 30, 13, 31) | 0.986 | 1.176 | 90.14 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5669 (26, 30, 13, 31) | 1.000 | 1.194 | 89.75 | 89.87 |##########################################################| ^ (0.6%, 16.71, 1.5, 6%) 
0 >> 14/20 <<   Training | 0.5608 (25, 29, 13, 33) | 0.978 | 1.166 | 90.17 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5667 (26, 29, 12, 33) | 0.985 | 1.178 | 89.79 | 89.87 |##########################################################| ^ (0.6%, 16.72, 1.0, 37%) 
0 >> 15/20 <<   Training | 0.5605 (26, 29, 13, 32) | 0.991 | 1.181 | 90.15 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5666 (27, 29, 12, 32) | 1.007 | 1.203 | 89.76 | 89.87 |##########################################################| ^ (0.7%, 16.52, 1.6, 3%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5604 (26, 29, 13, 32) | 0.990 | 1.179 | 90.13 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5665 (27, 29, 13, 32) | 0.994 | 1.185 | 89.75 | 89.88 |##########################################################| ^ (0.7%, 16.79, 1.1, 29%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5602 (26, 30, 13, 32) | 0.985 | 1.173 | 90.15 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5664 (27, 29, 13, 31) | 0.995 | 1.189 | 89.76 | 89.88 |##########################################################| ^ (0.7%, 16.68, 1.6, 3%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5602 (26, 29, 13, 32) | 0.986 | 1.176 | 90.15 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5663 (27, 29, 12, 32) | 1.001 | 1.196 | 89.76 | 89.88 |##########################################################| ^ (0.7%, 16.64, 1.2, 17%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5602 (26, 29, 13, 33) | 0.986 | 1.176 | 90.16 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5663 (27, 29, 12, 32) | 1.000 | 1.195 | 89.77 | 89.88 |##########################################################| ^ (0.7%, 16.62, 1.3, 15%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5602 (26, 29, 13, 32) | 0.986 | 1.174 | 90.15 | 90.14 |-------------------------------------------------------------|
0             Validation | 0.5663 (27, 29, 13, 32) | 1.002 | 1.197 | 89.76 | 89.88 |##########################################################| ^ (0.7%, 16.66, 1.3, 16%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7372 (35, 37,  9, 19) | 0.618 | 0.761 | 80.71 | 84.04 ||
0             Validation | 0.7414 (35, 37,  9, 19) | 0.626 | 0.759 | 80.05 | 83.82 || ^ (0.7%, 14.30, 0.8, 61%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7359 (35, 40,  9, 16) | 0.612 | 0.753 | 82.52 | 85.48 |--------------|
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6708 (33, 27, 10, 31) | 0.560 | 0.704 | 82.28 | 86.39 |-----------------------|
0             Validation | 0.6713 (32, 27, 10, 31) | 0.545 | 0.686 | 82.32 | 86.23 |######################| ^ (0.5%, 16.49, 2.0, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6923 (30, 27,  8, 35) | 0.449 | 0.616 | 82.67 | 85.07 |----------|
0             Validation | 0.6933 (30, 27,  8, 36) | 0.444 | 0.606 | 82.83 | 84.87 |########| ^ (0.6%, 17.18, 1.3, 14%) 
0 >>  3/20 <<   Training | 0.6619 (34, 28,  9, 29) | 0.615 | 0.765 | 83.17 | 87.08 |------------------------------|
0             Validation | 0.6631 (33, 29,  9, 29) | 0.595 | 0.737 | 83.26 | 86.87 |############################| ^ (0.6%, 18.58, 1.5, 6%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6838 (35, 28,  8, 29) | 0.569 | 0.732 | 80.97 | 86.61 |--------------------------|
0             Validation | 0.6849 (35, 28,  8, 29) | 0.569 | 0.719 | 81.13 | 86.40 |#######################| ^ (0.7%, 19.25, 1.0, 38%) 
0 >>  5/20 <<   Training | 0.6863 (34, 27,  7, 31) | 0.538 | 0.698 | 82.05 | 86.14 |---------------------|
0             Validation | 0.6875 (34, 27,  7, 31) | 0.529 | 0.681 | 82.11 | 85.95 |###################| ^ (0.6%, 19.79, 0.9, 48%) 
0 >>  6/20 <<   Training | 0.6790 (35, 28,  8, 30) | 0.571 | 0.726 | 81.84 | 86.57 |-------------------------|
0             Validation | 0.6802 (34, 28,  8, 30) | 0.569 | 0.725 | 81.82 | 86.40 |#######################| ^ (0.6%, 19.23, 1.3, 15%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5626 (26, 29, 13, 32) | 0.963 | 1.147 | 89.85 | 90.07 |------------------------------------------------------------|
0             Validation | 0.5677 (27, 29, 13, 31) | 0.960 | 1.145 | 89.76 | 89.70 |#########################################################| ^ (0.9%, 18.19, 1.4, 7%) 
0 >>  8/20 <<   Training | 0.5620 (26, 29, 13, 33) | 0.974 | 1.160 | 89.88 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5670 (26, 28, 12, 33) | 0.970 | 1.160 | 89.79 | 89.72 |#########################################################| ^ (0.9%, 17.55, 1.5, 4%) 
0 >>  9/20 <<   Training | 0.5614 (25, 30, 13, 33) | 0.973 | 1.158 | 89.98 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5665 (25, 29, 12, 33) | 0.987 | 1.179 | 89.92 | 89.75 |#########################################################| ^ (0.9%, 18.10, 1.4, 10%) 
0 >> 10/20 <<   Training | 0.5610 (25, 29, 12, 33) | 0.968 | 1.156 | 90.02 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5661 (26, 29, 12, 33) | 0.970 | 1.158 | 89.96 | 89.75 |#########################################################| ^ (0.9%, 17.86, 1.7, 2%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6035 (28, 28, 12, 33) | 0.812 | 0.976 | 87.92 | 88.82 |------------------------------------------------|
0             Validation | 0.6034 (28, 28, 12, 33) | 0.816 | 0.983 | 88.29 | 88.61 |##############################################| ^ (0.6%, 15.08, 1.1, 26%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.5746 (25, 30, 12, 33) | 0.917 | 1.097 | 89.57 | 89.70 |--------------------------------------------------------|
0             Validation | 0.5774 (25, 30, 12, 33) | 0.914 | 1.092 | 89.68 | 89.42 |######################################################| ^ (0.7%, 17.06, 0.9, 59%) 
0 >>  3/20 <<   Training | 0.5720 (27, 30, 12, 31) | 0.940 | 1.127 | 89.29 | 89.73 |---------------------------------------------------------|
0             Validation | 0.5746 (27, 30, 12, 31) | 0.954 | 1.143 | 89.42 | 89.47 |######################################################| ^ (0.7%, 15.12, 1.6, 4%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.5652 (27, 30, 13, 31) | 0.946 | 1.131 | 89.74 | 89.99 |-----------------------------------------------------------|
0             Validation | 0.5687 (27, 30, 13, 30) | 0.959 | 1.144 | 89.82 | 89.68 |########################################################| ^ (0.8%, 15.83, 1.4, 8%) 
0 >>  5/20 <<   Training | 0.5651 (25, 31, 12, 32) | 0.943 | 1.128 | 89.85 | 90.02 |------------------------------------------------------------|
0             Validation | 0.5689 (25, 31, 12, 32) | 0.954 | 1.142 | 89.89 | 89.72 |#########################################################| ^ (0.8%, 16.84, 1.1, 25%) 
0 >>  6/20 <<   Training | 0.5636 (26, 30, 12, 31) | 0.950 | 1.135 | 89.92 | 90.02 |------------------------------------------------------------|
0             Validation | 0.5676 (26, 30, 12, 31) | 0.955 | 1.138 | 89.96 | 89.71 |#########################################################| ^ (0.8%, 16.05, 1.5, 6%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5627 (26, 29, 12, 33) | 0.942 | 1.127 | 89.96 | 90.06 |------------------------------------------------------------|
0             Validation | 0.5667 (26, 29, 12, 33) | 0.941 | 1.124 | 89.98 | 89.75 |#########################################################| ^ (0.8%, 15.76, 1.0, 35%) 
0 >>  8/20 <<   Training | 0.5619 (27, 29, 12, 31) | 0.964 | 1.152 | 89.90 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5660 (27, 29, 12, 31) | 0.965 | 1.154 | 89.93 | 89.77 |#########################################################| ^ (0.8%, 15.47, 1.1, 25%) 
0 >>  9/20 <<   Training | 0.5616 (25, 29, 12, 34) | 0.959 | 1.147 | 90.04 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5659 (25, 29, 12, 34) | 0.971 | 1.160 | 90.05 | 89.79 |#########################################################| ^ (0.8%, 15.65, 1.6, 3%) 
0 >> 10/20 <<   Training | 0.5615 (26, 29, 13, 33) | 0.950 | 1.134 | 90.07 | 90.10 |-------------------------------------------------------------|
0             Validation | 0.5658 (26, 29, 13, 33) | 0.958 | 1.144 | 90.10 | 89.78 |#########################################################| ^ (0.8%, 15.67, 1.4, 10%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6078 (23, 27, 12, 38) | 0.796 | 0.966 | 88.32 | 88.58 |---------------------------------------------|
0             Validation | 0.6087 (24, 26, 12, 38) | 0.766 | 0.923 | 88.30 | 88.64 |##############################################| ^ (0.4%, 16.48, 4.0, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.5776 (26, 33, 13, 28) | 0.962 | 1.152 | 89.58 | 89.55 |-------------------------------------------------------|
0             Validation | 0.5810 (27, 32, 13, 27) | 0.924 | 1.104 | 89.37 | 89.55 |#######################################################| ^ (0.3%, 16.82, 4.6, 0%) 
0 >>  3/20 <<   Training | 0.5744 (25, 30, 12, 32) | 0.944 | 1.133 | 89.54 | 89.48 |------------------------------------------------------|
0             Validation | 0.5787 (26, 29, 12, 32) | 0.909 | 1.085 | 89.34 | 89.41 |######################################################| ^ (0.2%, 16.24, 4.0, 0%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.5637 (25, 29, 13, 33) | 0.976 | 1.169 | 89.99 | 89.95 |-----------------------------------------------------------|
0             Validation | 0.5676 (26, 29, 12, 33) | 0.927 | 1.100 | 89.76 | 89.92 |###########################################################| ^ (0.2%, 17.20, 3.3, 0%) 
0 >>  5/20 <<   Training | 0.5630 (25, 29, 12, 33) | 0.980 | 1.172 | 89.97 | 89.93 |-----------------------------------------------------------|
0             Validation | 0.5677 (26, 29, 12, 33) | 0.936 | 1.115 | 89.61 | 89.90 |##########################################################| ^ (0.3%, 16.65, 3.7, 0%) 
0 >>  6/20 <<   Training | 0.5633 (26, 28, 12, 34) | 0.965 | 1.155 | 89.98 | 89.94 |-----------------------------------------------------------|
0             Validation | 0.5684 (27, 28, 11, 34) | 0.924 | 1.103 | 89.62 | 89.89 |##########################################################| ^ (0.3%, 16.81, 3.2, 0%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5592 (26, 30, 12, 32) | 0.996 | 1.191 | 90.16 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5645 (27, 29, 12, 31) | 0.946 | 1.129 | 89.80 | 90.02 |############################################################| ^ (0.3%, 16.55, 3.0, 0%) 
0 >>  8/20 <<   Training | 0.5592 (25, 30, 13, 33) | 0.997 | 1.194 | 90.19 | 90.08 |------------------------------------------------------------|
0             Validation | 0.5640 (26, 29, 13, 32) | 0.944 | 1.122 | 89.83 | 90.03 |############################################################| ^ (0.3%, 16.96, 3.2, 0%) 
0 >>  9/20 <<   Training | 0.5586 (26, 29, 13, 32) | 0.998 | 1.193 | 90.20 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5641 (27, 29, 12, 32) | 0.948 | 1.130 | 89.81 | 90.03 |############################################################| ^ (0.3%, 16.48, 3.3, 0%) 
0 >> 10/20 <<   Training | 0.5584 (26, 29, 12, 32) | 0.991 | 1.187 | 90.19 | 90.09 |------------------------------------------------------------|
0             Validation | 0.5644 (27, 29, 12, 32) | 0.940 | 1.119 | 89.76 | 90.02 |############################################################| ^ (0.3%, 16.44, 2.9, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5586 (25, 29, 12, 34) | 1.006 | 1.205 | 90.24 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5638 (26, 29, 12, 33) | 0.946 | 1.127 | 89.86 | 90.04 |############################################################| ^ (0.3%, 16.26, 3.4, 0%) 
0 >> 12/20 <<   Training | 0.5581 (26, 30, 12, 32) | 1.003 | 1.200 | 90.22 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5641 (27, 30, 12, 31) | 0.951 | 1.134 | 89.81 | 90.03 |############################################################| ^ (0.3%, 16.60, 3.5, 0%) 
0 >> 13/20 <<   Training | 0.5579 (26, 30, 13, 32) | 1.003 | 1.200 | 90.24 | 90.11 |-------------------------------------------------------------|
0             Validation | 0.5638 (27, 30, 13, 31) | 0.951 | 1.131 | 89.82 | 90.04 |############################################################| ^ (0.3%, 16.70, 3.6, 0%) 
0 >> 14/20 <<   Training | 0.5578 (26, 30, 12, 32) | 0.998 | 1.196 | 90.26 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5637 (27, 30, 12, 31) | 0.944 | 1.125 | 89.84 | 90.04 |############################################################| ^ (0.3%, 16.62, 3.4, 0%) 
0 >> 15/20 <<   Training | 0.5575 (26, 30, 13, 32) | 0.995 | 1.193 | 90.24 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5636 (27, 29, 12, 31) | 0.946 | 1.124 | 89.79 | 90.05 |############################################################| ^ (0.3%, 16.72, 3.4, 0%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5574 (26, 29, 13, 32) | 0.998 | 1.197 | 90.23 | 90.12 |-------------------------------------------------------------|
0             Validation | 0.5636 (27, 29, 12, 31) | 0.946 | 1.127 | 89.79 | 90.06 |############################################################| ^ (0.3%, 16.66, 3.4, 0%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5573 (26, 30, 12, 32) | 0.998 | 1.197 | 90.25 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5634 (27, 29, 12, 32) | 0.948 | 1.129 | 89.82 | 90.06 |############################################################| ^ (0.3%, 16.56, 3.3, 0%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5573 (26, 29, 12, 32) | 1.000 | 1.197 | 90.25 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5634 (27, 29, 12, 32) | 0.949 | 1.130 | 89.81 | 90.05 |############################################################| ^ (0.3%, 16.59, 3.2, 0%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5573 (26, 29, 12, 32) | 1.000 | 1.199 | 90.26 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5633 (27, 29, 12, 32) | 0.947 | 1.128 | 89.82 | 90.06 |############################################################| ^ (0.3%, 16.56, 3.1, 0%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5573 (26, 29, 12, 32) | 1.000 | 1.199 | 90.26 | 90.13 |-------------------------------------------------------------|
0             Validation | 0.5633 (27, 29, 12, 32) | 0.947 | 1.128 | 89.82 | 90.06 |############################################################| ^ (0.3%, 16.57, 3.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
