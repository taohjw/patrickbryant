1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9232 (36, 37,  6, 17) | 0.955 | 12.945 | 83.01 | 66.55 |---------------------------------------------|
1             Validation | 0.9235 (36, 37,  6, 17) | 0.961 | 7.651 | 83.00 | 66.59 |#############################################| ^ (0.9%, 5.85, 2.1, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
1 >>  2/20 <<   Training | 0.9312 (36, 36,  6, 18) | 1.014 | 10.679 | 83.91 | 63.33 |-------------|
1             Validation | 0.9333 (37, 35,  6, 18) | 1.022 | 4.950 | 83.81 | 63.04 |##########| ^ (2.3%, 6.92, 2.2, 0%) 
1 >>  3/20 <<   Training | 0.9220 (37, 36,  6, 17) | 1.016 | 1.633 | 83.73 | 65.85 |--------------------------------------|
1             Validation | 0.9243 (37, 36,  6, 17) | 1.013 | 1.351 | 83.73 | 65.49 |##################################| ^ (2.4%, 4.82, 1.0, 43%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
1 >>  4/20 <<   Training | 0.9238 (37, 35,  6, 18) | 1.023 | 1.940 | 83.80 | 65.54 |-----------------------------------|
1             Validation | 0.9260 (37, 35,  6, 18) | 1.024 | 1.492 | 83.69 | 65.24 |################################| ^ (2.0%, 5.92, 1.4, 8%) 
1 >>  5/20 <<   Training | 0.9150 (36, 38,  6, 16) | 0.985 | 1.252 | 84.62 | 65.87 |--------------------------------------|
1             Validation | 0.9165 (37, 37,  6, 16) | 0.989 | 1.417 | 84.63 | 65.62 |####################################| ^ (1.6%, 4.95, 1.5, 5%) 
1 >>  6/20 <<   Training | 0.9213 (38, 36,  6, 16) | 1.031 | 2.442 | 84.14 | 65.44 |----------------------------------|
1             Validation | 0.9246 (38, 36,  6, 16) | 1.032 | 1.697 | 84.07 | 64.88 |############################| ^ (3.7%, 7.48, 2.1, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
1 >>  7/20 <<   Training | 0.9137 (37, 36,  6, 17) | 1.031 | 2.813 | 84.79 | 66.18 |-----------------------------------------|
1             Validation | 0.9171 (37, 36,  6, 17) | 1.029 | 1.677 | 84.66 | 65.66 |####################################| ^ (3.2%, 9.67, 2.3, 0%) 
1 >>  8/20 <<   Training | 0.9141 (37, 36,  6, 18) | 1.012 | 1.515 | 84.68 | 66.12 |-----------------------------------------|
1             Validation | 0.9169 (37, 36,  6, 18) | 1.018 | 1.232 | 84.64 | 65.70 |#####################################| ^ (2.6%, 4.57, 1.8, 1%) 
1 >>  9/20 <<   Training | 0.9182 (37, 36,  6, 17) | 1.039 | 5.485 | 84.41 | 65.83 |--------------------------------------|
1             Validation | 0.9222 (37, 36,  6, 17) | 1.043 | 3.309 | 84.26 | 65.22 |################################| ^ (3.9%, 5.57, 1.2, 22%) 
1 >> 10/20 <<   Training | 0.9141 (37, 35,  6, 18) | 1.022 | 2.584 | 84.61 | 66.54 |---------------------------------------------|
1             Validation | 0.9167 (37, 35,  6, 18) | 1.036 | 1.803 | 84.53 | 66.17 |#########################################| ^ (2.2%, 3.89, 3.6, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
1 >> 11/20 <<   Training | 0.9076 (37, 36,  6, 17) | 1.026 | 1.952 | 84.99 | 67.07 |--------------------------------------------------|
1             Validation | 0.9105 (37, 36,  6, 17) | 1.025 | 1.801 | 84.94 | 66.69 |##############################################| ^ (2.3%, 5.14, 1.3, 12%) 
1 >> 12/20 <<   Training | 0.9125 (37, 36,  6, 17) | 1.028 | 1.779 | 84.76 | 66.38 |-------------------------------------------|
1             Validation | 0.9162 (37, 36,  6, 17) | 1.031 | 1.620 | 84.65 | 65.84 |######################################| ^ (3.3%, 5.13, 2.1, 0%) 
1 >> 13/20 <<   Training | 0.9069 (35, 38,  6, 17) | 0.934 | 7.789 | 85.15 | 66.72 |-----------------------------------------------|
1             Validation | 0.9091 (35, 38,  6, 17) | 0.946 | 3.870 | 85.16 | 66.33 |###########################################| ^ (2.3%, 4.51, 3.3, 0%) 
1 >> 14/20 <<   Training | 0.9047 (36, 37,  6, 17) | 0.993 | 2.106 | 85.00 | 67.56 |-------------------------------------------------------|
1             Validation | 0.9090 (36, 37,  6, 17) | 0.988 | 2.608 | 84.88 | 66.95 |#################################################| ^ (3.4%, 4.55, 1.1, 28%) 
1 >> 15/20 <<   Training | 0.9049 (38, 35,  6, 17) | 1.037 | 3.625 | 84.99 | 67.67 |--------------------------------------------------------|
1             Validation | 0.9082 (38, 35,  6, 17) | 1.041 | 3.417 | 84.97 | 67.29 |####################################################| ^ (2.2%, 5.14, 1.6, 3%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.9073 (37, 37,  6, 17) | 1.002 | 1.318 | 84.96 | 67.06 |--------------------------------------------------|
1             Validation | 0.9106 (37, 37,  6, 17) | 1.014 | 1.425 | 84.89 | 66.63 |##############################################| ^ (2.6%, 5.01, 2.7, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.9075 (36, 37,  6, 17) | 0.996 | 1.056 | 84.95 | 66.99 |-------------------------------------------------|
1             Validation | 0.9122 (37, 36,  6, 17) | 1.001 | 1.566 | 84.87 | 66.33 |###########################################| ^ (3.9%, 4.66, 1.4, 8%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.9070 (37, 36,  6, 17) | 1.002 | 0.762 | 84.99 | 67.08 |--------------------------------------------------|
1             Validation | 0.9108 (37, 36,  6, 17) | 1.009 | 1.314 | 84.93 | 66.50 |#############################################| ^ (3.4%, 5.92, 1.9, 1%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.9065 (36, 36,  6, 17) | 1.002 | 0.903 | 85.03 | 67.17 |---------------------------------------------------|
1             Validation | 0.9105 (37, 36,  6, 17) | 1.013 | 1.383 | 84.88 | 66.57 |#############################################| ^ (3.4%, 6.39, 2.4, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.9070 (36, 37,  6, 17) | 0.999 | 1.375 | 84.98 | 67.08 |--------------------------------------------------|
1             Validation | 0.9099 (37, 36,  6, 17) | 1.016 | 1.215 | 84.89 | 66.69 |##############################################| ^ (2.3%, 4.99, 4.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.9065 (37, 37,  6, 17) | 1.001 | 1.184 | 84.96 | 67.14 |---------------------------------------------------|
1             Validation | 0.9112 (37, 36,  6, 17) | 1.004 | 1.442 | 84.89 | 66.48 |############################################| ^ (3.8%, 4.76, 1.0, 39%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9484 (21, 30,  7, 18) | 0.860 | 57.226 | 83.39 | 68.75 |-------------------------------------|
1             Validation | 0.9511 (21, 30,  7, 18) | 0.869 | 29.716 | 83.33 | 68.53 |###################################| ^ (1.3%, 3.80, 1.7, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
1 >>  2/20 <<   Training | 0.9385 (22, 26,  7, 21) | 1.003 | 6.196 | 84.12 | 68.47 |----------------------------------|
1             Validation | 0.9422 (22, 26,  7, 20) | 1.012 | 4.351 | 84.04 | 68.09 |##############################| ^ (2.4%, 3.28, 2.8, 0%) 
1 >>  3/20 <<   Training | 0.9386 (24, 27,  7, 18) | 1.186 | 33.679 | 84.55 | 68.68 |------------------------------------|
1             Validation | 0.9426 (24, 27,  7, 18) | 1.197 | 16.898 | 84.47 | 67.97 |#############################| ^ (3.9%, 2.96, 2.8, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
1 >>  4/20 <<   Training | 0.9293 (21, 29,  7, 18) | 0.960 | 2.385 | 84.72 | 69.87 |------------------------------------------------|
1             Validation | 0.9335 (21, 29,  7, 18) | 0.969 | 1.838 | 84.64 | 69.31 |###########################################| ^ (2.9%, 3.63, 2.8, 0%) 
1 >>  5/20 <<   Training | 0.9277 (22, 29,  7, 18) | 0.990 | 2.705 | 84.73 | 69.60 |---------------------------------------------|
1             Validation | 0.9326 (22, 29,  7, 18) | 0.999 | 2.569 | 84.65 | 68.93 |#######################################| ^ (3.4%, 4.26, 2.0, 0%) 
1 >>  6/20 <<   Training | 0.9298 (22, 27,  7, 20) | 1.097 | 8.016 | 84.69 | 69.87 |------------------------------------------------|
1             Validation | 0.9344 (23, 26,  7, 20) | 1.108 | 4.342 | 84.61 | 69.23 |##########################################| ^ (3.2%, 3.57, 3.0, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
1 >>  7/20 <<   Training | 0.9248 (22, 28,  7, 18) | 1.045 | 3.021 | 84.91 | 70.44 |------------------------------------------------------|
1             Validation | 0.9301 (23, 28,  7, 18) | 1.054 | 3.003 | 84.83 | 69.69 |##############################################| ^ (3.7%, 3.81, 1.9, 1%) 
1 >>  8/20 <<   Training | 0.9242 (22, 28,  7, 19) | 1.009 | 1.191 | 84.86 | 70.61 |--------------------------------------------------------|
1             Validation | 0.9298 (22, 28,  7, 18) | 1.019 | 1.293 | 84.77 | 69.78 |###############################################| ^ (4.1%, 3.57, 2.4, 0%) 
1 >>  9/20 <<   Training | 0.9259 (23, 28,  7, 18) | 1.065 | 3.995 | 84.95 | 70.15 |---------------------------------------------------|
1             Validation | 0.9309 (23, 28,  7, 18) | 1.075 | 3.027 | 84.88 | 69.38 |###########################################| ^ (3.9%, 3.74, 2.2, 0%) 
1 >> 10/20 <<   Training | 0.9243 (21, 28,  7, 19) | 1.004 | 1.857 | 84.95 | 70.41 |------------------------------------------------------|
1             Validation | 0.9300 (22, 28,  7, 19) | 1.013 | 1.004 | 84.85 | 69.52 |#############################################| ^ (4.3%, 3.99, 2.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
1 >> 11/20 <<   Training | 0.9247 (23, 29,  7, 17) | 1.064 | 4.142 | 84.96 | 70.57 |-------------------------------------------------------|
1             Validation | 0.9311 (23, 29,  7, 17) | 1.074 | 4.029 | 84.84 | 69.67 |##############################################| ^ (4.4%, 3.90, 2.0, 0%) 
1 >> 12/20 <<   Training | 0.9227 (22, 28,  7, 19) | 1.034 | 1.930 | 85.03 | 70.49 |------------------------------------------------------|
1             Validation | 0.9285 (22, 28,  7, 19) | 1.043 | 1.339 | 84.93 | 69.58 |#############################################| ^ (4.4%, 3.71, 2.3, 0%) 
1 >> 13/20 <<   Training | 0.9222 (22, 28,  7, 19) | 1.013 | 1.305 | 85.05 | 70.64 |--------------------------------------------------------|
1             Validation | 0.9281 (22, 28,  7, 18) | 1.022 | 1.512 | 84.95 | 69.75 |###############################################| ^ (4.3%, 3.85, 2.1, 0%) 
1 >> 14/20 <<   Training | 0.9248 (20, 28,  7, 20) | 0.938 | 6.100 | 85.04 | 70.93 |-----------------------------------------------------------|
1             Validation | 0.9305 (21, 28,  7, 20) | 0.947 | 2.598 | 84.93 | 69.99 |#################################################| ^ (4.5%, 3.95, 1.9, 1%) 
1 >> 15/20 <<   Training | 0.9236 (23, 27,  7, 18) | 1.129 | 10.707 | 85.03 | 70.28 |----------------------------------------------------|
1             Validation | 0.9300 (24, 27,  7, 18) | 1.139 | 6.167 | 84.92 | 69.29 |##########################################| ^ (4.9%, 3.92, 2.2, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.9207 (21, 29,  7, 18) | 0.984 | 1.075 | 85.15 | 71.02 |------------------------------------------------------------|
1             Validation | 0.9274 (22, 29,  7, 18) | 0.992 | 1.564 | 85.04 | 69.99 |#################################################| ^ (4.9%, 4.29, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.9204 (22, 29,  7, 18) | 1.010 | 1.137 | 85.14 | 70.98 |-----------------------------------------------------------|
1             Validation | 0.9272 (22, 28,  7, 18) | 1.018 | 1.499 | 85.03 | 69.93 |#################################################| ^ (5.0%, 4.33, 2.2, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.9203 (22, 28,  7, 19) | 1.001 | 1.025 | 85.13 | 70.97 |-----------------------------------------------------------|
1             Validation | 0.9271 (22, 28,  7, 18) | 1.010 | 1.514 | 85.02 | 69.93 |#################################################| ^ (5.0%, 4.31, 1.9, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.9203 (22, 29,  7, 18) | 0.999 | 1.047 | 85.13 | 70.98 |-----------------------------------------------------------|
1             Validation | 0.9271 (22, 28,  7, 18) | 1.008 | 1.475 | 85.02 | 69.93 |#################################################| ^ (5.0%, 4.33, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.9203 (22, 29,  7, 18) | 1.000 | 0.981 | 85.13 | 70.98 |-----------------------------------------------------------|
1             Validation | 0.9271 (22, 28,  7, 18) | 1.009 | 1.430 | 85.02 | 69.93 |#################################################| ^ (5.0%, 4.33, 1.9, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.9203 (22, 29,  7, 18) | 0.995 | 1.074 | 85.13 | 70.97 |-----------------------------------------------------------|
1             Validation | 0.9271 (22, 28,  7, 18) | 1.003 | 1.579 | 85.02 | 69.93 |#################################################| ^ (5.0%, 4.34, 1.9, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
