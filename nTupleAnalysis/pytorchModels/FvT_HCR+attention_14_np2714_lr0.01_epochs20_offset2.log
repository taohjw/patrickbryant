2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.9041 (44, 34,  6, 15) | 1.267 | 0.010 | 84.08 | 64.77 |---------------------------|
2             Validation | 0.9038 (43, 34,  6, 15) | 1.258 | 0.009 | 84.08 | 64.78 |###########################| ^ (0.5%, 5.97, 1.1, 33%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
2 >>  2/20 <<   Training | 0.8871 (38, 36,  7, 17) | 1.045 | 0.001 | 85.51 | 65.43 |----------------------------------|
2             Validation | 0.8880 (38, 36,  7, 17) | 1.038 | 0.001 | 85.42 | 65.37 |#################################| ^ (0.9%, 3.79, 2.0, 0%) 
2 >>  3/20 <<   Training | 0.8888 (42, 36,  5, 13) | 1.197 | 0.008 | 85.61 | 66.04 |----------------------------------------|
2             Validation | 0.8896 (42, 37,  5, 13) | 1.190 | 0.008 | 85.57 | 65.89 |######################################| ^ (1.1%, 4.17, 2.2, 0%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
2 >>  4/20 <<   Training | 0.8829 (34, 39,  7, 18) | 0.865 | 0.000 | 85.93 | 66.52 |---------------------------------------------|
2             Validation | 0.8848 (34, 39,  7, 18) | 0.860 | 0.000 | 85.84 | 66.32 |###########################################| ^ (1.2%, 3.43, 1.0, 36%) 
2 >>  5/20 <<   Training | 0.8757 (39, 38,  6, 14) | 1.033 | 0.000 | 86.11 | 66.30 |------------------------------------------|
2             Validation | 0.8775 (39, 38,  6, 15) | 1.027 | 0.000 | 86.03 | 66.07 |########################################| ^ (1.4%, 3.59, 2.4, 0%) 
2 >>  6/20 <<   Training | 0.8759 (38, 37,  6, 16) | 0.999 | 0.001 | 86.18 | 66.28 |------------------------------------------|
2             Validation | 0.8778 (38, 37,  6, 16) | 0.993 | 0.001 | 86.10 | 66.01 |########################################| ^ (1.7%, 3.56, 1.1, 24%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
2 >>  7/20 <<   Training | 0.8746 (40, 38,  6, 14) | 1.067 | 0.001 | 86.35 | 66.51 |---------------------------------------------|
2             Validation | 0.8770 (40, 38,  6, 14) | 1.060 | 0.001 | 86.24 | 66.19 |#########################################| ^ (2.0%, 3.70, 1.1, 26%) 
2 >>  8/20 <<   Training | 0.8728 (39, 38,  6, 15) | 1.011 | 0.000 | 86.25 | 66.67 |----------------------------------------------|
2             Validation | 0.8754 (39, 38,  6, 15) | 1.005 | 0.000 | 86.15 | 66.29 |##########################################| ^ (2.3%, 4.11, 2.0, 0%) 
2 >>  9/20 <<   Training | 0.8726 (38, 39,  6, 15) | 0.992 | 0.001 | 86.38 | 66.50 |---------------------------------------------|
2             Validation | 0.8757 (38, 39,  6, 15) | 0.986 | 0.001 | 86.27 | 66.07 |########################################| ^ (2.6%, 4.82, 1.1, 28%) 
2 >> 10/20 <<   Training | 0.8720 (39, 37,  6, 15) | 1.039 | 0.000 | 86.39 | 66.70 |----------------------------------------------|
2             Validation | 0.8750 (39, 37,  6, 15) | 1.032 | 0.000 | 86.28 | 66.22 |##########################################| ^ (2.8%, 3.72, 1.4, 8%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
2 >> 11/20 <<   Training | 0.8720 (36, 39,  6, 16) | 0.946 | 0.000 | 86.39 | 66.82 |------------------------------------------------|
2             Validation | 0.8755 (36, 39,  6, 16) | 0.940 | 0.000 | 86.27 | 66.31 |###########################################| ^ (3.0%, 3.86, 1.5, 4%) 
2 >> 12/20 <<   Training | 0.8712 (38, 38,  6, 16) | 0.987 | 0.000 | 86.41 | 66.84 |------------------------------------------------|
2             Validation | 0.8745 (38, 38,  6, 16) | 0.981 | 0.000 | 86.30 | 66.34 |###########################################| ^ (3.0%, 4.03, 1.3, 14%) 
2 >> 13/20 <<   Training | 0.8703 (39, 38,  6, 15) | 1.010 | 0.001 | 86.45 | 66.80 |------------------------------------------------|
2             Validation | 0.8739 (39, 38,  6, 15) | 1.004 | 0.001 | 86.32 | 66.32 |###########################################| ^ (2.9%, 4.52, 1.3, 12%) 
2 >> 14/20 <<   Training | 0.8699 (37, 38,  6, 16) | 0.987 | 0.000 | 86.50 | 66.92 |-------------------------------------------------|
2             Validation | 0.8735 (37, 38,  6, 16) | 0.981 | 0.000 | 86.36 | 66.41 |############################################| ^ (3.0%, 4.93, 1.6, 3%) 
2 >> 15/20 <<   Training | 0.8742 (42, 36,  6, 14) | 1.126 | 0.002 | 86.45 | 66.70 |-----------------------------------------------|
2             Validation | 0.8781 (42, 36,  6, 14) | 1.120 | 0.002 | 86.31 | 66.15 |#########################################| ^ (3.3%, 4.34, 1.4, 10%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8695 (38, 38,  6, 16) | 1.008 | 0.001 | 86.54 | 67.02 |--------------------------------------------------|
2             Validation | 0.8736 (38, 37,  6, 16) | 1.001 | 0.001 | 86.39 | 66.42 |############################################| ^ (3.5%, 4.45, 1.2, 23%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8685 (38, 38,  6, 15) | 1.012 | 0.001 | 86.56 | 66.97 |-------------------------------------------------|
2             Validation | 0.8728 (38, 38,  6, 15) | 1.006 | 0.001 | 86.41 | 66.38 |###########################################| ^ (3.5%, 4.65, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8684 (38, 38,  6, 15) | 1.000 | 0.001 | 86.57 | 66.97 |-------------------------------------------------|
2             Validation | 0.8727 (38, 38,  6, 15) | 0.994 | 0.001 | 86.42 | 66.37 |###########################################| ^ (3.6%, 4.68, 1.5, 5%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 0.001 | 86.57 | 66.99 |-------------------------------------------------|
2             Validation | 0.8727 (38, 38,  6, 15) | 0.993 | 0.001 | 86.42 | 66.39 |###########################################| ^ (3.5%, 4.67, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 0.001 | 86.57 | 66.99 |-------------------------------------------------|
2             Validation | 0.8727 (38, 38,  6, 15) | 0.993 | 0.001 | 86.42 | 66.39 |###########################################| ^ (3.5%, 4.67, 1.7, 2%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
2 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 1.032 | 86.60 | 66.99 |-------------------------------------------------|
2             Validation | 0.8718 (38, 38,  6, 15) | 0.993 | 1.248 | 86.51 | 66.39 |###########################################| ^ (3.6%, 4.67, 1.6, 3%) 
Run Finetuning
2 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 0.962 | 86.60 | 66.99 |-------------------------------------------------|
2             Validation | 0.8718 (38, 38,  6, 15) | 0.993 | 1.169 | 86.51 | 66.39 |###########################################| ^ (3.6%, 4.69, 1.6, 4%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20_finetune01.pkl
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
2 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 1.032 | 86.60 | 66.99 |-------------------------------------------------|
2             Validation | 0.8718 (38, 38,  6, 15) | 0.993 | 1.248 | 86.51 | 66.39 |###########################################| ^ (3.6%, 4.67, 1.6, 3%) 
Run Finetuning
2 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.999 | 0.968 | 86.60 | 66.99 |-------------------------------------------------|
2             Validation | 0.8718 (38, 38,  6, 15) | 0.993 | 1.202 | 86.51 | 66.39 |###########################################| ^ (3.6%, 4.70, 1.5, 4%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20_finetune01.pkl
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.9055 (40, 35,  6, 15) | 1.121 | 12.552 | 83.59 | 64.24 |----------------------|
2             Validation | 0.9046 (40, 35,  6, 15) | 1.119 | 6.971 | 83.66 | 64.39 |#######################| ^ (1.4%, 3.64, 0.9, 49%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
2 >>  2/20 <<   Training | 0.8948 (41, 35,  6, 15) | 1.144 | 13.219 | 84.42 | 65.99 |---------------------------------------|
2             Validation | 0.8940 (41, 35,  6, 15) | 1.144 | 7.310 | 84.39 | 66.26 |##########################################| ^ (2.0%, 3.62, 1.7, 2%) 
2 >>  3/20 <<   Training | 0.8862 (37, 38,  6, 15) | 0.971 | 2.465 | 85.01 | 66.14 |-----------------------------------------|
2             Validation | 0.8865 (37, 38,  6, 15) | 0.970 | 1.893 | 84.97 | 66.12 |#########################################| ^ (1.4%, 4.59, 0.7, 77%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
2 >>  4/20 <<   Training | 0.8835 (37, 39,  5, 15) | 0.933 | 7.574 | 85.42 | 66.25 |------------------------------------------|
2             Validation | 0.8842 (37, 39,  5, 15) | 0.932 | 4.325 | 85.38 | 66.23 |##########################################| ^ (1.4%, 4.93, 0.9, 46%) 
2 >>  5/20 <<   Training | 0.8832 (38, 37,  5, 15) | 1.008 | 2.314 | 85.46 | 66.40 |-------------------------------------------|
2             Validation | 0.8843 (38, 37,  5, 15) | 1.007 | 1.881 | 85.39 | 66.30 |##########################################| ^ (1.2%, 5.98, 1.8, 1%) 
2 >>  6/20 <<   Training | 0.8864 (40, 36,  6, 14) | 1.088 | 10.104 | 85.40 | 66.11 |-----------------------------------------|
2             Validation | 0.8879 (40, 36,  6, 14) | 1.087 | 4.925 | 85.34 | 65.90 |#######################################| ^ (1.7%, 3.18, 1.2, 19%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
2 >>  7/20 <<   Training | 0.8795 (37, 38,  5, 15) | 0.990 | 2.840 | 85.78 | 66.53 |---------------------------------------------|
2             Validation | 0.8813 (37, 38,  5, 15) | 0.989 | 1.971 | 85.71 | 66.27 |##########################################| ^ (1.9%, 6.17, 1.6, 4%) 
2 >>  8/20 <<   Training | 0.8789 (38, 38,  6, 15) | 1.032 | 4.608 | 85.78 | 66.63 |----------------------------------------------|
2             Validation | 0.8816 (38, 38,  6, 15) | 1.030 | 2.147 | 85.65 | 66.22 |##########################################| ^ (2.5%, 3.76, 1.7, 2%) 
2 >>  9/20 <<   Training | 0.8784 (37, 39,  6, 14) | 0.948 | 3.995 | 85.68 | 66.68 |----------------------------------------------|
2             Validation | 0.8808 (37, 39,  6, 14) | 0.948 | 2.838 | 85.59 | 66.38 |###########################################| ^ (2.0%, 3.91, 1.3, 14%) 
2 >> 10/20 <<   Training | 0.8806 (36, 41,  5, 14) | 0.904 | 10.767 | 85.85 | 66.51 |---------------------------------------------|
2             Validation | 0.8836 (36, 41,  5, 14) | 0.903 | 6.932 | 85.74 | 66.19 |#########################################| ^ (2.3%, 6.06, 1.7, 2%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
2 >> 11/20 <<   Training | 0.8772 (39, 38,  5, 14) | 1.036 | 2.970 | 85.89 | 66.88 |------------------------------------------------|
2             Validation | 0.8802 (39, 38,  5, 14) | 1.034 | 1.750 | 85.79 | 66.47 |############################################| ^ (2.5%, 3.38, 1.6, 3%) 
2 >> 12/20 <<   Training | 0.8783 (40, 37,  5, 14) | 1.081 | 4.827 | 85.89 | 66.90 |------------------------------------------------|
2             Validation | 0.8825 (40, 37,  5, 14) | 1.080 | 3.013 | 85.76 | 66.38 |###########################################| ^ (3.1%, 4.77, 1.1, 28%) 
2 >> 13/20 <<   Training | 0.8776 (35, 41,  5, 15) | 0.895 | 10.409 | 85.95 | 66.80 |-----------------------------------------------|
2             Validation | 0.8814 (35, 40,  5, 15) | 0.894 | 6.639 | 85.84 | 66.30 |##########################################| ^ (3.0%, 4.98, 1.1, 28%) 
2 >> 14/20 <<   Training | 0.8766 (36, 39,  5, 15) | 0.954 | 4.478 | 85.91 | 66.89 |------------------------------------------------|
2             Validation | 0.8806 (36, 39,  5, 15) | 0.952 | 3.142 | 85.77 | 66.33 |###########################################| ^ (3.3%, 4.45, 0.8, 72%) 
2 >> 15/20 <<   Training | 0.8771 (35, 40,  6, 15) | 0.904 | 9.012 | 85.97 | 67.00 |-------------------------------------------------|
2             Validation | 0.8816 (35, 40,  6, 15) | 0.903 | 5.628 | 85.83 | 66.41 |############################################| ^ (3.4%, 5.42, 0.8, 59%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.8738 (37, 38,  6, 15) | 0.982 | 1.397 | 86.03 | 67.16 |---------------------------------------------------|
2             Validation | 0.8789 (38, 38,  6, 15) | 0.981 | 1.884 | 85.89 | 66.51 |#############################################| ^ (3.8%, 4.53, 0.8, 68%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.8733 (38, 38,  5, 15) | 1.001 | 0.868 | 86.06 | 67.25 |----------------------------------------------------|
2             Validation | 0.8787 (38, 38,  5, 15) | 1.000 | 1.407 | 85.90 | 66.55 |#############################################| ^ (4.1%, 4.37, 0.9, 55%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.8732 (38, 38,  5, 15) | 0.997 | 1.086 | 86.06 | 67.21 |----------------------------------------------------|
2             Validation | 0.8786 (38, 38,  5, 15) | 0.996 | 1.516 | 85.91 | 66.52 |#############################################| ^ (4.1%, 4.44, 0.7, 78%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.8732 (38, 38,  5, 15) | 1.000 | 0.965 | 86.06 | 67.22 |----------------------------------------------------|
2             Validation | 0.8786 (38, 38,  5, 15) | 0.999 | 1.532 | 85.91 | 66.52 |#############################################| ^ (4.1%, 4.43, 0.6, 87%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.8732 (38, 38,  5, 15) | 1.000 | 0.955 | 86.06 | 67.22 |----------------------------------------------------|
2             Validation | 0.8786 (38, 38,  5, 15) | 0.999 | 1.454 | 85.91 | 66.52 |#############################################| ^ (4.1%, 4.43, 0.7, 79%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.8732 (38, 38,  5, 15) | 1.000 | 0.851 | 86.06 | 67.22 |----------------------------------------------------|
2             Validation | 0.8786 (38, 38,  5, 15) | 0.999 | 1.494 | 85.91 | 66.52 |#############################################| ^ (4.1%, 4.45, 0.6, 88%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.9464 (23, 28,  7, 18) | 1.073 | 5.040 | 83.44 | 68.68 |------------------------------------|
2             Validation | 0.9444 (23, 28,  7, 18) | 1.072 | 3.200 | 83.53 | 69.03 |########################################| ^ (1.9%, 4.35, 1.5, 6%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
2 >>  2/20 <<   Training | 0.9390 (21, 29,  7, 17) | 0.909 | 31.501 | 84.27 | 69.65 |----------------------------------------------|
2             Validation | 0.9381 (21, 30,  7, 17) | 0.909 | 15.933 | 84.30 | 69.77 |###############################################| ^ (0.7%, 4.63, 1.3, 15%) 
2 >>  3/20 <<   Training | 0.9369 (23, 26,  7, 19) | 1.097 | 6.273 | 84.17 | 68.16 |-------------------------------|
2             Validation | 0.9369 (23, 26,  7, 19) | 1.096 | 3.632 | 84.19 | 68.21 |################################| ^ (1.1%, 4.38, 1.3, 12%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
2 >>  4/20 <<   Training | 0.9280 (22, 28,  7, 18) | 0.994 | 1.162 | 84.71 | 69.62 |----------------------------------------------|
2             Validation | 0.9280 (22, 28,  7, 18) | 0.993 | 1.431 | 84.73 | 69.63 |##############################################| ^ (0.9%, 6.56, 1.2, 23%) 
2 >>  5/20 <<   Training | 0.9287 (20, 29,  7, 19) | 0.908 | 10.969 | 84.68 | 69.72 |-----------------------------------------------|
2             Validation | 0.9293 (20, 29,  7, 19) | 0.907 | 6.456 | 84.69 | 69.61 |##############################################| ^ (1.0%, 6.90, 1.5, 4%) 
2 >>  6/20 <<   Training | 0.9310 (22, 26,  7, 20) | 1.079 | 5.254 | 84.79 | 69.91 |-------------------------------------------------|
2             Validation | 0.9316 (23, 26,  7, 20) | 1.077 | 3.031 | 84.79 | 69.74 |###############################################| ^ (1.1%, 4.29, 1.0, 35%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
2 >>  7/20 <<   Training | 0.9258 (21, 27,  7, 20) | 0.994 | 1.035 | 85.01 | 69.81 |------------------------------------------------|
2             Validation | 0.9273 (21, 27,  7, 20) | 0.992 | 1.239 | 84.98 | 69.55 |#############################################| ^ (1.3%, 6.37, 1.7, 2%) 
2 >>  8/20 <<   Training | 0.9241 (22, 28,  7, 18) | 1.000 | 6.542 | 85.05 | 70.46 |------------------------------------------------------|
2             Validation | 0.9259 (22, 28,  7, 18) | 0.999 | 4.717 | 85.02 | 70.15 |###################################################| ^ (1.6%, 6.05, 0.9, 54%) 
2 >>  9/20 <<   Training | 0.9245 (22, 28,  7, 18) | 1.000 | 6.387 | 85.04 | 70.37 |-----------------------------------------------------|
2             Validation | 0.9268 (22, 28,  7, 18) | 0.999 | 4.715 | 85.00 | 70.08 |##################################################| ^ (1.4%, 7.42, 1.9, 1%) 
2 >> 10/20 <<   Training | 0.9246 (22, 28,  7, 18) | 1.049 | 2.791 | 84.99 | 70.34 |-----------------------------------------------------|
2             Validation | 0.9266 (23, 28,  7, 18) | 1.047 | 2.105 | 84.96 | 70.04 |##################################################| ^ (1.5%, 6.53, 0.8, 71%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
2 >> 11/20 <<   Training | 0.9218 (21, 29,  7, 18) | 0.974 | 1.740 | 85.08 | 70.51 |-------------------------------------------------------|
2             Validation | 0.9242 (21, 29,  7, 18) | 0.973 | 1.485 | 85.02 | 70.11 |###################################################| ^ (1.9%, 6.17, 1.6, 3%) 
2 >> 12/20 <<   Training | 0.9316 (21, 26,  7, 22) | 0.985 | 2.158 | 85.04 | 70.17 |---------------------------------------------------|
2             Validation | 0.9336 (21, 26,  7, 22) | 0.984 | 1.361 | 85.00 | 69.73 |###############################################| ^ (2.2%, 5.77, 1.0, 34%) 
2 >> 13/20 <<   Training | 0.9227 (21, 28,  7, 19) | 1.007 | 2.059 | 85.15 | 70.90 |----------------------------------------------------------|
2             Validation | 0.9253 (22, 28,  7, 19) | 1.006 | 0.965 | 85.10 | 70.41 |######################################################| ^ (2.3%, 5.71, 1.6, 4%) 
2 >> 14/20 <<   Training | 0.9213 (22, 29,  7, 18) | 1.001 | 1.474 | 85.06 | 70.32 |-----------------------------------------------------|
2             Validation | 0.9239 (22, 29,  7, 18) | 0.999 | 1.822 | 85.02 | 69.86 |################################################| ^ (2.2%, 5.14, 1.1, 29%) 
2 >> 15/20 <<   Training | 0.9217 (21, 28,  7, 19) | 0.981 | 2.376 | 85.14 | 70.84 |----------------------------------------------------------|
2             Validation | 0.9242 (21, 28,  7, 19) | 0.980 | 1.581 | 85.10 | 70.41 |######################################################| ^ (2.0%, 5.87, 1.7, 2%) 
Decay learning rate: 0.010000 -> 0.002500
2 >> 16/20 <<   Training | 0.9196 (21, 29,  7, 18) | 0.971 | 1.906 | 85.21 | 70.88 |----------------------------------------------------------|
2             Validation | 0.9228 (21, 29,  7, 18) | 0.970 | 1.804 | 85.16 | 70.33 |#####################################################| ^ (2.6%, 6.37, 1.1, 33%) 
Decay learning rate: 0.002500 -> 0.000625
2 >> 17/20 <<   Training | 0.9192 (21, 29,  7, 18) | 0.983 | 1.222 | 85.22 | 71.00 |------------------------------------------------------------|
2             Validation | 0.9227 (22, 29,  7, 18) | 0.981 | 1.406 | 85.16 | 70.42 |######################################################| ^ (2.7%, 6.74, 1.2, 21%) 
Decay learning rate: 0.000625 -> 0.000156
2 >> 18/20 <<   Training | 0.9190 (22, 29,  7, 18) | 1.002 | 1.016 | 85.22 | 70.98 |-----------------------------------------------------------|
2             Validation | 0.9227 (22, 29,  7, 18) | 1.001 | 1.721 | 85.16 | 70.39 |#####################################################| ^ (2.8%, 6.76, 1.4, 10%) 
Decay learning rate: 0.000156 -> 0.000039
2 >> 19/20 <<   Training | 0.9190 (22, 29,  7, 18) | 1.002 | 0.986 | 85.22 | 70.97 |-----------------------------------------------------------|
2             Validation | 0.9227 (22, 28,  7, 18) | 1.000 | 1.825 | 85.16 | 70.38 |#####################################################| ^ (2.8%, 6.78, 1.5, 5%) 
Decay learning rate: 0.000039 -> 0.000010
2 >> 20/20 <<   Training | 0.9190 (22, 29,  7, 18) | 1.002 | 0.930 | 85.22 | 70.98 |-----------------------------------------------------------|
2             Validation | 0.9227 (22, 29,  7, 18) | 1.000 | 1.910 | 85.16 | 70.39 |#####################################################| ^ (2.8%, 6.78, 1.5, 5%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20_before_finetuning.pkl
Run Finetuning
2 >> 20/20 <<   Training | 0.9190 (22, 29,  7, 18) | 1.000 | 1.088 | 85.22 | 70.98 |-----------------------------------------------------------|
2             Validation | 0.9227 (22, 29,  7, 18) | 0.998 | 2.039 | 85.16 | 70.39 |#####################################################| ^ (2.8%, 6.83, 1.6, 4%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
