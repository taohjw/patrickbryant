0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8964 (40, 37,  6, 14) | 1.098 | 0.004 | 84.23 | 64.96 |-----------------------------|
0             Validation | 0.8987 (40, 37,  6, 14) | 1.105 | 0.004 | 84.09 | 64.71 |###########################| ^ (1.7%, 4.57, 1.1, 29%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
0 >>  2/20 <<   Training | 0.8895 (37, 40,  6, 15) | 0.938 | 0.001 | 84.68 | 64.95 |-----------------------------|
0             Validation | 0.8919 (37, 40,  6, 15) | 0.943 | 0.001 | 84.55 | 64.59 |#########################| ^ (2.5%, 3.68, 2.0, 0%) 
0 >>  3/20 <<   Training | 0.8875 (40, 34,  7, 17) | 1.110 | 0.001 | 85.38 | 65.96 |---------------------------------------|
0             Validation | 0.8901 (40, 34,  7, 17) | 1.116 | 0.001 | 85.24 | 65.63 |####################################| ^ (2.1%, 4.72, 1.6, 3%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
0 >>  4/20 <<   Training | 0.8777 (38, 37,  7, 16) | 0.991 | 0.001 | 85.85 | 66.40 |-------------------------------------------|
0             Validation | 0.8808 (38, 37,  7, 16) | 0.996 | 0.001 | 85.71 | 65.99 |#######################################| ^ (2.5%, 4.28, 2.0, 0%) 
0 >>  5/20 <<   Training | 0.8748 (39, 38,  6, 15) | 1.028 | 0.001 | 86.15 | 66.38 |-------------------------------------------|
0             Validation | 0.8780 (39, 38,  6, 15) | 1.035 | 0.001 | 86.01 | 65.97 |#######################################| ^ (2.5%, 3.94, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.8769 (36, 39,  6, 16) | 0.948 | 0.002 | 86.14 | 66.42 |--------------------------------------------|
0             Validation | 0.8802 (36, 39,  6, 16) | 0.954 | 0.002 | 85.97 | 65.97 |#######################################| ^ (2.7%, 4.42, 1.4, 7%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
0 >>  7/20 <<   Training | 0.8742 (35, 39,  6, 17) | 0.917 | 0.001 | 86.33 | 66.66 |----------------------------------------------|
0             Validation | 0.8775 (36, 39,  7, 17) | 0.922 | 0.001 | 86.17 | 66.20 |#########################################| ^ (2.8%, 3.78, 2.0, 0%) 
0 >>  8/20 <<   Training | 0.8733 (40, 37,  6, 15) | 1.074 | 0.001 | 86.29 | 66.74 |-----------------------------------------------|
0             Validation | 0.8773 (41, 37,  6, 15) | 1.080 | 0.001 | 86.13 | 66.25 |##########################################| ^ (2.9%, 3.83, 1.7, 2%) 
0 >>  9/20 <<   Training | 0.8718 (37, 40,  6, 15) | 0.927 | 0.001 | 86.39 | 66.60 |---------------------------------------------|
0             Validation | 0.8759 (37, 40,  6, 15) | 0.932 | 0.001 | 86.22 | 66.10 |#########################################| ^ (3.0%, 4.47, 1.8, 1%) 
0 >> 10/20 <<   Training | 0.8715 (39, 37,  6, 15) | 1.042 | 0.001 | 86.38 | 66.54 |---------------------------------------------|
0             Validation | 0.8760 (39, 37,  6, 15) | 1.048 | 0.001 | 86.18 | 66.01 |########################################| ^ (3.2%, 3.72, 2.2, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
0 >> 11/20 <<   Training | 0.8717 (38, 38,  6, 16) | 1.013 | 0.001 | 86.41 | 66.84 |------------------------------------------------|
0             Validation | 0.8754 (38, 38,  6, 16) | 1.019 | 0.001 | 86.24 | 66.31 |###########################################| ^ (3.1%, 4.21, 1.6, 4%) 
0 >> 12/20 <<   Training | 0.8699 (39, 38,  6, 15) | 1.023 | 0.001 | 86.51 | 66.72 |-----------------------------------------------|
0             Validation | 0.8744 (39, 38,  6, 14) | 1.029 | 0.001 | 86.32 | 66.17 |#########################################| ^ (3.3%, 3.51, 2.2, 0%) 
0 >> 13/20 <<   Training | 0.8711 (35, 40,  6, 16) | 0.914 | 0.001 | 86.54 | 66.83 |------------------------------------------------|
0             Validation | 0.8751 (36, 40,  6, 16) | 0.920 | 0.001 | 86.36 | 66.31 |###########################################| ^ (3.1%, 3.88, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.8714 (35, 40,  6, 16) | 0.902 | 0.001 | 86.48 | 66.72 |-----------------------------------------------|
0             Validation | 0.8756 (36, 39,  6, 16) | 0.907 | 0.001 | 86.31 | 66.18 |#########################################| ^ (3.2%, 3.91, 1.7, 2%) 
0 >> 15/20 <<   Training | 0.8716 (40, 37,  6, 14) | 1.072 | 0.001 | 86.54 | 66.46 |--------------------------------------------|
0             Validation | 0.8768 (41, 37,  6, 14) | 1.079 | 0.001 | 86.35 | 65.88 |######################################| ^ (3.6%, 3.96, 1.8, 1%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8678 (38, 38,  6, 15) | 1.014 | 0.001 | 86.62 | 66.95 |-------------------------------------------------|
0             Validation | 0.8729 (39, 38,  6, 15) | 1.020 | 0.001 | 86.43 | 66.37 |###########################################| ^ (3.5%, 4.33, 1.9, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8678 (38, 38,  6, 16) | 1.000 | 0.001 | 86.64 | 67.07 |--------------------------------------------------|
0             Validation | 0.8726 (38, 38,  6, 16) | 1.006 | 0.001 | 86.45 | 66.48 |############################################| ^ (3.4%, 4.03, 2.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.004 | 0.001 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (39, 38,  6, 15) | 1.010 | 0.001 | 86.45 | 66.42 |############################################| ^ (3.5%, 4.06, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.001 | 0.001 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 0.001 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.04, 2.3, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.001 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 0.001 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.04, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.799 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.050 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.04, 2.3, 0%) 0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.799 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.050 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.04, 2.3, 0%) Run Finetuning
Increase training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.764 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 0.988 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.2, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.809 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.177 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.796 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.199 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.828 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.171 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.824 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.174 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.824 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.174 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) Increase training batch size: 16384 -> 16384 (151 batches)
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.799 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.050 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.04, 2.3, 0%) Run Finetuning
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.764 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 0.988 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.999 | 0.809 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.177 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 0.998 | 0.820 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.004 | 1.172 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.3, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.810 | 86.64 | 67.02 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.005 | 1.244 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.784 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.237 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.804 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.208 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.833 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.209 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.831 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.205 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.826 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.204 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8675 (38, 38,  6, 15) | 1.000 | 0.828 | 86.64 | 67.03 |--------------------------------------------------|
0             Validation | 0.8725 (38, 38,  6, 15) | 1.006 | 1.203 | 86.45 | 66.43 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.000 | 0.800 | 86.62 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.069 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.04, 1.9, 1%) 
Run Finetuning
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.000 | 0.836 | 86.62 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.075 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.9, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.000 | 0.868 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.091 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.8, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.000 | 0.854 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.007 | 1.132 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.8, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.001 | 0.848 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.007 | 1.111 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.8, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.001 | 0.892 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.007 | 1.117 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.7, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.001 | 0.895 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.007 | 1.130 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.8, 1%) 
0 >> 20/20 <<   Training | 0.8677 (38, 38,  6, 15) | 1.001 | 0.877 | 86.62 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.007 | 1.137 | 86.48 | 66.43 |############################################| ^ (3.5%, 4.06, 1.8, 1%) 
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.811 | 86.67 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.083 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.04, 2.2, 0%) 
Run Finetuning
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.002 | 0.687 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (39, 38,  6, 15) | 1.008 | 1.313 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.06, 2.0, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.787 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.193 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.07, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.769 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.193 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.07, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.758 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.221 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.792 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.278 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.800 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.298 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.801 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.335 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.804 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.317 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.809 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.315 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.807 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.328 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.1, 0%) 
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.811 | 86.67 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.083 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.04, 2.2, 0%) 
Run Finetuning
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.002 | 0.687 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (39, 38,  6, 15) | 1.008 | 1.313 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.06, 2.0, 0%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_finetune01.pkl
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.811 | 86.67 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.083 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.04, 2.2, 0%) 
Run Finetuning
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.002 | 0.693 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (39, 38,  6, 15) | 1.008 | 1.338 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_finetune01.pkl
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.811 | 86.67 | 67.02 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.083 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.04, 2.2, 0%) 
Run Finetuning
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.002 | 0.693 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (39, 38,  6, 15) | 1.008 | 1.338 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.06, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.775 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.006 | 1.192 | 86.51 | 66.44 |############################################| ^ (3.5%, 4.07, 2.1, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.749 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.205 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.757 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.235 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.782 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.237 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 0.999 | 0.793 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.271 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.814 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.324 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.817 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.327 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.795 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.324 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> 20/20 <<   Training | 0.8674 (38, 38,  6, 15) | 1.000 | 0.805 | 86.67 | 67.03 |--------------------------------------------------|
0             Validation | 0.8721 (38, 38,  6, 15) | 1.005 | 1.336 | 86.51 | 66.43 |############################################| ^ (3.5%, 4.08, 2.2, 0%) 
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9150 (38, 37,  6, 10) | 1.033 | 23.141 | 83.98 | 64.47 |------------------------|
0             Validation | 0.9138 (38, 37,  6, 10) | 1.035 | 12.781 | 83.97 | 64.78 |###########################| ^ (2.2%, 6.54, 1.8, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1246 batches)
0 >>  2/20 <<   Training | 0.9027 (38, 35,  7, 11) | 1.090 | 22.680 | 85.52 | 65.79 |-------------------------------------|
0             Validation | 0.9024 (38, 35,  7, 11) | 1.093 | 13.125 | 85.37 | 66.01 |########################################| ^ (1.5%, 3.95, 1.3, 13%) 
0 >>  3/20 <<   Training | 0.8977 (37, 37,  7, 11) | 0.999 | 3.128 | 85.92 | 65.63 |------------------------------------|
0             Validation | 0.8978 (37, 37,  7, 11) | 1.001 | 2.742 | 85.84 | 65.76 |#####################################| ^ (1.2%, 2.86, 1.0, 38%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (623 batches)
0 >>  4/20 <<   Training | 0.8922 (37, 38,  6, 10) | 0.982 | 3.811 | 86.21 | 66.34 |-------------------------------------------|
0             Validation | 0.8927 (37, 38,  6, 10) | 0.983 | 2.352 | 86.12 | 66.33 |###########################################| ^ (1.0%, 4.39, 1.0, 42%) 
0 >>  5/20 <<   Training | 0.8954 (39, 36,  6, 10) | 1.038 | 9.928 | 86.32 | 66.19 |-----------------------------------------|
0             Validation | 0.8958 (39, 36,  6, 10) | 1.040 | 5.291 | 86.21 | 66.25 |##########################################| ^ (1.2%, 2.98, 1.3, 11%) 
0 >>  6/20 <<   Training | 0.8922 (39, 36,  6, 10) | 1.091 | 10.947 | 86.31 | 66.45 |--------------------------------------------|
0             Validation | 0.8928 (39, 36,  6, 11) | 1.093 | 6.273 | 86.21 | 66.47 |############################################| ^ (1.3%, 3.23, 0.9, 55%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (311 batches)
0 >>  7/20 <<   Training | 0.8902 (38, 36,  7, 11) | 1.069 | 6.651 | 86.61 | 66.54 |---------------------------------------------|
0             Validation | 0.8911 (39, 35,  7, 11) | 1.071 | 3.553 | 86.48 | 66.51 |#############################################| ^ (1.2%, 3.76, 0.7, 74%) 
0 >>  8/20 <<   Training | 0.8881 (38, 37,  6, 10) | 1.044 | 3.688 | 86.69 | 66.58 |---------------------------------------------|
0             Validation | 0.8893 (38, 37,  6, 11) | 1.046 | 2.348 | 86.56 | 66.51 |#############################################| ^ (1.4%, 5.01, 1.4, 7%) 
0 >>  9/20 <<   Training | 0.8911 (40, 36,  6, 10) | 1.159 | 26.658 | 86.73 | 66.59 |---------------------------------------------|
0             Validation | 0.8926 (41, 36,  6, 10) | 1.161 | 14.168 | 86.59 | 66.45 |############################################| ^ (1.4%, 4.42, 1.2, 17%) 
0 >> 10/20 <<   Training | 0.8876 (37, 38,  7, 10) | 0.988 | 2.972 | 86.68 | 66.64 |----------------------------------------------|
0             Validation | 0.8891 (37, 38,  7, 10) | 0.990 | 1.565 | 86.54 | 66.52 |#############################################| ^ (1.7%, 4.04, 0.7, 83%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (155 batches)
0 >> 11/20 <<   Training | 0.8890 (34, 40,  7, 11) | 0.882 | 22.457 | 86.76 | 66.75 |-----------------------------------------------|
0             Validation | 0.8906 (34, 40,  7, 11) | 0.883 | 11.422 | 86.60 | 66.62 |##############################################| ^ (1.6%, 4.65, 0.6, 91%) 
0 >> 12/20 <<   Training | 0.8864 (38, 36,  7, 11) | 1.065 | 7.664 | 86.87 | 66.76 |-----------------------------------------------|
0             Validation | 0.8881 (38, 36,  7, 11) | 1.066 | 3.978 | 86.73 | 66.62 |##############################################| ^ (1.8%, 4.55, 1.5, 6%) 
0 >> 13/20 <<   Training | 0.8858 (37, 38,  7, 10) | 0.999 | 2.696 | 86.80 | 66.81 |------------------------------------------------|
0             Validation | 0.8880 (38, 38,  7, 10) | 1.000 | 1.227 | 86.63 | 66.59 |#############################################| ^ (2.0%, 4.48, 0.7, 74%) 
0 >> 14/20 <<   Training | 0.8859 (37, 37,  7, 11) | 1.016 | 1.503 | 86.80 | 66.87 |------------------------------------------------|
0             Validation | 0.8879 (37, 37,  7, 11) | 1.017 | 0.858 | 86.65 | 66.68 |##############################################| ^ (1.7%, 3.95, 1.3, 11%) 
0 >> 15/20 <<   Training | 0.8865 (38, 36,  7, 11) | 1.052 | 4.420 | 86.80 | 66.85 |------------------------------------------------|
0             Validation | 0.8884 (38, 36,  7, 11) | 1.054 | 2.691 | 86.64 | 66.68 |##############################################| ^ (1.6%, 4.86, 1.4, 8%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8838 (37, 37,  7, 11) | 1.007 | 1.612 | 86.98 | 66.96 |-------------------------------------------------|
0             Validation | 0.8864 (37, 37,  7, 11) | 1.008 | 1.011 | 86.81 | 66.70 |##############################################| ^ (2.1%, 4.43, 0.9, 49%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8835 (37, 37,  7, 10) | 1.017 | 1.688 | 86.99 | 66.99 |-------------------------------------------------|
0             Validation | 0.8862 (38, 37,  6, 11) | 1.018 | 1.125 | 86.82 | 66.70 |###############################################| ^ (2.3%, 4.54, 1.1, 25%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8834 (37, 38,  7, 11) | 0.997 | 1.087 | 87.00 | 66.99 |-------------------------------------------------|
0             Validation | 0.8862 (37, 38,  6, 11) | 0.999 | 1.014 | 86.82 | 66.71 |###############################################| ^ (2.3%, 4.55, 0.7, 84%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8834 (37, 38,  7, 10) | 1.003 | 1.113 | 87.00 | 66.99 |-------------------------------------------------|
0             Validation | 0.8862 (37, 37,  6, 11) | 1.004 | 1.062 | 86.82 | 66.70 |###############################################| ^ (2.3%, 4.56, 0.7, 75%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8834 (37, 38,  7, 10) | 1.002 | 1.191 | 87.00 | 66.99 |-------------------------------------------------|
0             Validation | 0.8862 (37, 37,  6, 11) | 1.003 | 0.999 | 86.82 | 66.70 |###############################################| ^ (2.3%, 4.56, 0.8, 71%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
Change training batch size: 16384 -> 1024 (2492 batches)
0 >> 20/20 <<   Training | 0.8834 (37, 37,  6, 11) | 1.009 | 1.202 | 87.00 | 66.99 |-------------------------------------------------|
0             Validation | 0.8862 (37, 37,  6, 11) | 1.011 | 0.959 | 86.82 | 66.71 |###############################################| ^ (2.3%, 4.57, 0.8, 62%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9220 (37, 36,  6,  8) | 1.013 | 3.456 | 83.67 | 65.08 |------------------------------|
0             Validation | 0.9257 (37, 36,  6,  8) | 1.005 | 2.860 | 83.58 | 64.75 |###########################| ^ (2.2%, 4.01, 1.9, 1%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (939 batches)
0 >>  2/20 <<   Training | 0.9159 (34, 39,  6,  8) | 0.856 | 28.237 | 85.03 | 65.87 |--------------------------------------|
0             Validation | 0.9206 (33, 39,  6,  8) | 0.849 | 16.854 | 84.89 | 65.48 |##################################| ^ (2.5%, 3.51, 1.7, 2%) 
0 >>  3/20 <<   Training | 0.9117 (37, 36,  6,  8) | 1.020 | 2.111 | 85.02 | 65.68 |------------------------------------|
0             Validation | 0.9159 (37, 36,  6,  8) | 1.011 | 1.824 | 84.86 | 65.27 |################################| ^ (2.8%, 2.38, 2.0, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (469 batches)
0 >>  4/20 <<   Training | 0.9060 (38, 35,  5,  8) | 1.093 | 9.172 | 85.70 | 66.44 |--------------------------------------------|
0             Validation | 0.9104 (38, 35,  5,  8) | 1.083 | 3.800 | 85.54 | 66.00 |########################################| ^ (2.7%, 2.52, 2.3, 0%) 
0 >>  5/20 <<   Training | 0.9103 (38, 36,  5,  7) | 1.014 | 15.571 | 85.56 | 66.26 |------------------------------------------|
0             Validation | 0.9158 (38, 36,  5,  7) | 1.005 | 9.697 | 85.37 | 65.73 |#####################################| ^ (3.2%, 2.79, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.9094 (38, 34,  6,  9) | 1.111 | 10.888 | 85.87 | 66.16 |-----------------------------------------|
0             Validation | 0.9141 (38, 34,  6,  9) | 1.101 | 4.487 | 85.72 | 65.68 |####################################| ^ (3.0%, 2.30, 2.9, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.9019 (36, 38,  5,  8) | 0.965 | 1.764 | 85.91 | 66.75 |-----------------------------------------------|
0             Validation | 0.9081 (36, 38,  5,  8) | 0.956 | 2.463 | 85.77 | 66.12 |#########################################| ^ (3.8%, 2.83, 2.2, 0%) 
0 >>  8/20 <<   Training | 0.9063 (35, 39,  5,  7) | 0.900 | 21.197 | 85.81 | 66.57 |---------------------------------------------|
0             Validation | 0.9131 (35, 39,  5,  7) | 0.892 | 14.186 | 85.66 | 65.90 |######################################| ^ (4.1%, 2.43, 1.9, 0%) 
0 >>  9/20 <<   Training | 0.9011 (35, 37,  6,  8) | 0.958 | 3.059 | 85.94 | 66.93 |-------------------------------------------------|
0             Validation | 0.9082 (35, 37,  6,  8) | 0.949 | 2.550 | 85.74 | 66.18 |#########################################| ^ (4.5%, 2.84, 1.9, 0%) 
0 >> 10/20 <<   Training | 0.9042 (35, 38,  6,  8) | 0.933 | 6.424 | 85.97 | 66.57 |---------------------------------------------|
0             Validation | 0.9113 (35, 38,  6,  8) | 0.925 | 5.266 | 85.80 | 65.88 |######################################| ^ (4.2%, 2.64, 2.0, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.8992 (36, 36,  6,  8) | 0.999 | 1.144 | 86.02 | 67.10 |---------------------------------------------------|
0             Validation | 0.9069 (36, 36,  6,  8) | 0.989 | 1.624 | 85.86 | 66.32 |###########################################| ^ (4.6%, 2.69, 1.9, 0%) 
0 >> 12/20 <<   Training | 0.8991 (35, 38,  6,  8) | 0.946 | 3.869 | 86.13 | 67.15 |---------------------------------------------------|
0             Validation | 0.9071 (35, 38,  6,  8) | 0.937 | 2.776 | 85.94 | 66.25 |##########################################| ^ (5.3%, 2.67, 2.2, 0%) 
0 >> 13/20 <<   Training | 0.8997 (37, 37,  5,  7) | 1.001 | 3.994 | 86.00 | 67.10 |---------------------------------------------------|
0             Validation | 0.9079 (37, 36,  5,  7) | 0.991 | 4.107 | 85.84 | 66.25 |##########################################| ^ (5.0%, 2.81, 1.7, 2%) 
0 >> 14/20 <<   Training | 0.9002 (34, 39,  6,  8) | 0.895 | 10.558 | 86.04 | 67.15 |---------------------------------------------------|
0             Validation | 0.9087 (34, 39,  6,  8) | 0.886 | 7.192 | 85.86 | 66.19 |#########################################| ^ (5.6%, 2.71, 2.2, 0%) 
0 >> 15/20 <<   Training | 0.8983 (36, 37,  6,  8) | 0.992 | 1.392 | 86.08 | 67.11 |---------------------------------------------------|
0             Validation | 0.9070 (36, 37,  5,  8) | 0.983 | 1.544 | 85.89 | 66.14 |#########################################| ^ (5.6%, 2.78, 1.5, 4%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8967 (36, 37,  6,  8) | 1.003 | 1.864 | 86.21 | 67.33 |-----------------------------------------------------|
0             Validation | 0.9059 (36, 37,  6,  8) | 0.993 | 1.535 | 86.01 | 66.28 |##########################################| ^ (6.1%, 2.87, 2.7, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8963 (36, 37,  6,  8) | 0.992 | 1.199 | 86.24 | 67.37 |-----------------------------------------------------|
0             Validation | 0.9058 (36, 37,  5,  8) | 0.982 | 1.992 | 86.04 | 66.32 |###########################################| ^ (6.1%, 2.89, 2.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8961 (37, 37,  6,  8) | 1.005 | 1.065 | 86.24 | 67.38 |-----------------------------------------------------|
0             Validation | 0.9056 (37, 36,  6,  8) | 0.995 | 1.643 | 86.04 | 66.33 |###########################################| ^ (6.1%, 2.87, 2.4, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8961 (36, 37,  6,  8) | 1.002 | 1.103 | 86.24 | 67.38 |-----------------------------------------------------|
0             Validation | 0.9056 (37, 36,  6,  8) | 0.992 | 1.699 | 86.04 | 66.33 |###########################################| ^ (6.1%, 2.87, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8961 (36, 37,  6,  8) | 1.002 | 1.059 | 86.24 | 67.38 |-----------------------------------------------------|
0             Validation | 0.9056 (37, 36,  6,  8) | 0.992 | 1.697 | 86.04 | 66.33 |###########################################| ^ (6.1%, 2.88, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8961 (37, 37,  6,  8) | 1.003 | 1.039 | 86.24 | 67.38 |-----------------------------------------------------|
0             Validation | 0.9056 (37, 36,  6,  8) | 0.993 | 1.698 | 86.04 | 66.33 |###########################################| ^ (6.1%, 2.88, 2.6, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9232 (38, 36,  6,  8) | 1.027 | 5.714 | 83.66 | 65.10 |-------------------------------|
0             Validation | 0.9277 (38, 36,  6,  8) | 1.019 | 4.252 | 83.47 | 64.72 |###########################| ^ (2.5%, 3.16, 2.2, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (939 batches)
0 >>  2/20 <<   Training | 0.9168 (33, 39,  6,  8) | 0.827 | 43.140 | 84.87 | 66.05 |----------------------------------------|
0             Validation | 0.9213 (33, 39,  6,  8) | 0.820 | 25.071 | 84.78 | 65.71 |#####################################| ^ (2.1%, 2.98, 2.7, 0%) 
0 >>  3/20 <<   Training | 0.9130 (38, 36,  6,  7) | 1.053 | 3.824 | 84.97 | 65.70 |-------------------------------------|
0             Validation | 0.9175 (38, 36,  6,  7) | 1.043 | 1.928 | 84.81 | 65.26 |################################| ^ (2.8%, 3.10, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (469 batches)
0 >>  4/20 <<   Training | 0.9063 (37, 35,  6,  8) | 1.051 | 5.814 | 85.50 | 66.47 |--------------------------------------------|
0             Validation | 0.9116 (37, 35,  6,  8) | 1.041 | 2.623 | 85.34 | 65.92 |#######################################| ^ (3.3%, 2.66, 2.0, 0%) 
0 >>  5/20 <<   Training | 0.9090 (35, 38,  6,  8) | 0.908 | 19.803 | 85.68 | 66.15 |-----------------------------------------|
0             Validation | 0.9150 (35, 38,  6,  8) | 0.900 | 13.302 | 85.49 | 65.60 |####################################| ^ (3.4%, 3.41, 2.0, 0%) 
0 >>  6/20 <<   Training | 0.9077 (39, 35,  5,  8) | 1.124 | 13.306 | 85.87 | 66.07 |----------------------------------------|
0             Validation | 0.9131 (39, 35,  5,  8) | 1.114 | 5.512 | 85.70 | 65.47 |##################################| ^ (3.7%, 2.99, 2.4, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (234 batches)
0 >>  7/20 <<   Training | 0.9022 (35, 39,  5,  8) | 0.928 | 5.221 | 85.95 | 66.71 |-----------------------------------------------|
0             Validation | 0.9097 (35, 38,  5,  8) | 0.919 | 4.567 | 85.77 | 65.95 |#######################################| ^ (4.6%, 3.19, 2.0, 0%) 
0 >>  8/20 <<   Training | 0.9032 (34, 39,  5,  8) | 0.887 | 19.951 | 86.01 | 66.74 |-----------------------------------------------|
0             Validation | 0.9116 (35, 39,  5,  8) | 0.878 | 13.819 | 85.80 | 65.90 |#######################################| ^ (5.0%, 2.78, 1.8, 1%) 
0 >>  9/20 <<   Training | 0.9001 (36, 37,  6,  8) | 0.977 | 1.352 | 86.12 | 66.93 |-------------------------------------------------|
0             Validation | 0.9084 (36, 37,  6,  8) | 0.968 | 1.334 | 85.93 | 66.05 |########################################| ^ (5.2%, 3.39, 2.2, 0%) 
0 >> 10/20 <<   Training | 0.9020 (34, 39,  5,  8) | 0.928 | 5.591 | 86.14 | 66.71 |-----------------------------------------------|
0             Validation | 0.9106 (35, 39,  5,  8) | 0.919 | 4.446 | 85.93 | 65.85 |######################################| ^ (5.1%, 3.07, 1.8, 1%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (117 batches)
0 >> 11/20 <<   Training | 0.8982 (36, 37,  5,  8) | 0.997 | 1.363 | 86.23 | 67.06 |--------------------------------------------------|
0             Validation | 0.9076 (36, 37,  5,  8) | 0.987 | 1.862 | 86.04 | 66.06 |########################################| ^ (5.9%, 3.50, 2.3, 0%) 
0 >> 12/20 <<   Training | 0.8979 (36, 37,  5,  8) | 0.996 | 1.518 | 86.24 | 67.19 |---------------------------------------------------|
0             Validation | 0.9077 (36, 37,  5,  8) | 0.986 | 1.960 | 86.03 | 66.11 |#########################################| ^ (6.3%, 3.03, 1.8, 1%) 
0 >> 13/20 <<   Training | 0.8988 (38, 35,  5,  8) | 1.050 | 4.352 | 86.18 | 67.10 |---------------------------------------------------|
0             Validation | 0.9090 (38, 35,  5,  8) | 1.039 | 3.549 | 85.99 | 66.08 |########################################| ^ (6.0%, 3.22, 2.0, 0%) 
0 >> 14/20 <<   Training | 0.8995 (34, 39,  6,  8) | 0.897 | 10.270 | 86.24 | 67.12 |---------------------------------------------------|
0             Validation | 0.9094 (34, 39,  6,  8) | 0.888 | 7.143 | 86.06 | 66.00 |#######################################| ^ (6.6%, 3.02, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.8972 (37, 37,  5,  8) | 1.013 | 1.265 | 86.27 | 67.21 |----------------------------------------------------|
0             Validation | 0.9081 (37, 36,  5,  8) | 1.003 | 2.550 | 86.07 | 66.00 |#######################################| ^ (7.0%, 3.29, 2.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8958 (36, 37,  6,  8) | 0.981 | 2.699 | 86.40 | 67.45 |------------------------------------------------------|
0             Validation | 0.9070 (36, 37,  6,  8) | 0.971 | 1.539 | 86.18 | 66.19 |#########################################| ^ (7.2%, 3.35, 1.8, 1%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8949 (36, 37,  5,  8) | 0.996 | 0.963 | 86.41 | 67.46 |------------------------------------------------------|
0             Validation | 0.9071 (37, 37,  5,  8) | 0.986 | 2.419 | 86.19 | 66.14 |#########################################| ^ (7.5%, 3.60, 1.8, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8947 (37, 37,  5,  8) | 1.007 | 1.132 | 86.42 | 67.47 |------------------------------------------------------|
0             Validation | 0.9069 (37, 36,  5,  8) | 0.997 | 1.893 | 86.19 | 66.16 |#########################################| ^ (7.5%, 3.56, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8947 (36, 37,  6,  8) | 1.001 | 1.038 | 86.42 | 67.47 |------------------------------------------------------|
0             Validation | 0.9068 (37, 36,  5,  8) | 0.991 | 2.017 | 86.19 | 66.17 |#########################################| ^ (7.4%, 3.54, 1.9, 1%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8947 (36, 37,  6,  8) | 1.001 | 1.038 | 86.42 | 67.47 |------------------------------------------------------|
0             Validation | 0.9068 (37, 36,  5,  8) | 0.991 | 1.895 | 86.19 | 66.17 |#########################################| ^ (7.5%, 3.54, 1.9, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8947 (36, 37,  6,  8) | 1.003 | 1.040 | 86.42 | 67.47 |------------------------------------------------------|
0             Validation | 0.9068 (37, 36,  5,  8) | 0.992 | 2.063 | 86.19 | 66.17 |#########################################| ^ (7.4%, 3.56, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9049 (37, 38,  6, 16) | 0.954 | 14.343 | 83.26 | 64.74 |---------------------------|
0             Validation | 0.9089 (37, 38,  6, 16) | 0.945 | 8.805 | 83.24 | 64.31 |#######################| ^ (2.9%, 6.20, 2.0, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
0 >>  2/20 <<   Training | 0.8925 (39, 37,  5, 15) | 1.072 | 8.730 | 84.57 | 65.47 |----------------------------------|
0             Validation | 0.8949 (39, 37,  5, 15) | 1.062 | 3.831 | 84.51 | 65.08 |##############################| ^ (2.6%, 3.07, 2.0, 0%) 
0 >>  3/20 <<   Training | 0.8896 (37, 37,  6, 16) | 1.013 | 4.630 | 84.85 | 65.82 |--------------------------------------|
0             Validation | 0.8933 (37, 37,  6, 16) | 1.005 | 2.925 | 84.83 | 65.36 |#################################| ^ (2.9%, 4.20, 2.3, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
0 >>  4/20 <<   Training | 0.8906 (38, 40,  5, 13) | 0.989 | 8.583 | 85.18 | 65.55 |-----------------------------------|
0             Validation | 0.8957 (38, 40,  5, 13) | 0.980 | 6.153 | 85.09 | 64.92 |#############################| ^ (4.1%, 4.93, 1.6, 4%) 
0 >>  5/20 <<   Training | 0.8828 (38, 38,  6, 15) | 1.014 | 4.551 | 85.24 | 66.47 |--------------------------------------------|
0             Validation | 0.8872 (38, 38,  6, 15) | 1.005 | 2.016 | 85.15 | 65.79 |#####################################| ^ (4.2%, 3.58, 1.4, 10%) 
0 >>  6/20 <<   Training | 0.8829 (39, 37,  5, 15) | 1.032 | 6.633 | 85.31 | 66.58 |---------------------------------------------|
0             Validation | 0.8889 (39, 37,  5, 15) | 1.023 | 5.282 | 85.20 | 65.77 |#####################################| ^ (4.9%, 3.88, 2.0, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
0 >>  7/20 <<   Training | 0.8796 (38, 36,  6, 16) | 1.044 | 3.452 | 85.57 | 67.00 |-------------------------------------------------|
0             Validation | 0.8852 (38, 36,  6, 16) | 1.034 | 1.613 | 85.44 | 66.17 |#########################################| ^ (4.9%, 4.30, 1.6, 3%) 
0 >>  8/20 <<   Training | 0.8795 (38, 38,  6, 15) | 1.018 | 4.535 | 85.61 | 66.64 |----------------------------------------------|
0             Validation | 0.8849 (38, 37,  6, 15) | 1.009 | 1.956 | 85.47 | 65.82 |######################################| ^ (4.9%, 4.42, 2.6, 0%) 
0 >>  9/20 <<   Training | 0.8786 (38, 37,  6, 16) | 1.025 | 1.766 | 85.59 | 67.01 |--------------------------------------------------|
0             Validation | 0.8848 (38, 37,  6, 16) | 1.015 | 1.304 | 85.46 | 66.13 |#########################################| ^ (5.2%, 3.81, 2.0, 0%) 
0 >> 10/20 <<   Training | 0.8796 (36, 39,  6, 15) | 0.944 | 4.639 | 85.64 | 66.46 |--------------------------------------------|
0             Validation | 0.8860 (37, 39,  6, 15) | 0.935 | 3.678 | 85.52 | 65.61 |####################################| ^ (5.2%, 5.01, 2.1, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
0 >> 11/20 <<   Training | 0.8766 (37, 38,  6, 15) | 0.989 | 2.155 | 85.71 | 67.01 |--------------------------------------------------|
0             Validation | 0.8832 (37, 38,  6, 15) | 0.980 | 1.320 | 85.57 | 66.06 |########################################| ^ (5.6%, 4.17, 1.7, 2%) 
0 >> 12/20 <<   Training | 0.8771 (36, 40,  5, 15) | 0.921 | 6.240 | 85.68 | 67.07 |--------------------------------------------------|
0             Validation | 0.8843 (36, 40,  5, 15) | 0.912 | 5.273 | 85.53 | 66.07 |########################################| ^ (5.9%, 4.47, 1.6, 2%) 
0 >> 13/20 <<   Training | 0.8769 (36, 39,  6, 15) | 0.922 | 6.884 | 85.77 | 66.96 |-------------------------------------------------|
0             Validation | 0.8847 (36, 39,  6, 15) | 0.913 | 6.817 | 85.63 | 65.96 |#######################################| ^ (5.9%, 4.37, 2.4, 0%) 
0 >> 14/20 <<   Training | 0.8766 (37, 38,  6, 15) | 0.967 | 4.656 | 85.72 | 66.96 |-------------------------------------------------|
0             Validation | 0.8845 (38, 38,  6, 15) | 0.958 | 5.382 | 85.57 | 65.95 |#######################################| ^ (6.0%, 4.37, 1.8, 1%) 
0 >> 15/20 <<   Training | 0.8755 (36, 39,  5, 15) | 0.962 | 2.648 | 85.72 | 67.27 |----------------------------------------------------|
0             Validation | 0.8828 (37, 39,  5, 15) | 0.953 | 2.618 | 85.55 | 66.21 |##########################################| ^ (6.1%, 4.15, 2.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.8742 (39, 37,  6, 15) | 1.057 | 3.192 | 85.85 | 67.30 |----------------------------------------------------|
0             Validation | 0.8822 (39, 37,  5, 15) | 1.047 | 2.143 | 85.68 | 66.17 |#########################################| ^ (6.6%, 4.39, 2.0, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.8737 (38, 37,  5, 15) | 1.025 | 1.412 | 85.87 | 67.38 |-----------------------------------------------------|
0             Validation | 0.8819 (39, 37,  5, 15) | 1.015 | 2.027 | 85.69 | 66.24 |##########################################| ^ (6.5%, 4.40, 1.8, 1%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.8736 (38, 38,  5, 15) | 1.004 | 0.957 | 85.87 | 67.36 |-----------------------------------------------------|
0             Validation | 0.8818 (38, 38,  5, 15) | 0.994 | 1.925 | 85.70 | 66.23 |##########################################| ^ (6.5%, 4.44, 2.0, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.8736 (38, 38,  6, 15) | 1.001 | 0.993 | 85.87 | 67.37 |-----------------------------------------------------|
0             Validation | 0.8818 (38, 38,  5, 15) | 0.991 | 1.896 | 85.70 | 66.24 |##########################################| ^ (6.5%, 4.44, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.8736 (38, 38,  6, 15) | 1.002 | 1.050 | 85.87 | 67.37 |-----------------------------------------------------|
0             Validation | 0.8818 (38, 38,  5, 15) | 0.993 | 1.861 | 85.70 | 66.24 |##########################################| ^ (6.5%, 4.44, 2.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.8736 (38, 38,  6, 15) | 1.006 | 0.991 | 85.87 | 67.38 |-----------------------------------------------------|
0             Validation | 0.8818 (38, 38,  5, 15) | 0.996 | 1.755 | 85.70 | 66.25 |##########################################| ^ (6.5%, 4.43, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
