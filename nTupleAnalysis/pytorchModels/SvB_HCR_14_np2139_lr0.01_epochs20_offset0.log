0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8138 (30, 29,  6, 35) | 0.578 |   0.0 | 86.66 | 84.55 |-----|
0             Validation | 0.8255 (32, 28,  6, 34) | 0.548 |   0.0 | 86.05 | 84.23 |##| ^ (0.9%, 82.87, 1.8, 1%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (857 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7626 (31, 30,  6, 32) | 0.747 |   0.0 | 88.30 | 86.30 |-----------------------|
0             Validation | 0.7755 (32, 30,  6, 32) | 0.703 |   0.0 | 87.69 | 85.83 |##################| ^ (1.3%, 48.96, 2.7, 0%) 
0 >>  3/20 <<   Training | 0.7710 (27, 30,  6, 36) | 0.744 |   0.0 | 88.46 | 86.11 |---------------------|
0             Validation | 0.7821 (28, 29,  6, 36) | 0.714 |   0.0 | 87.84 | 85.66 |################| ^ (1.3%, 42.16, 2.3, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (428 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7519 (30, 32,  6, 31) | 0.788 |   0.0 | 88.69 | 86.58 |-------------------------|
0             Validation | 0.7647 (32, 31,  6, 31) | 0.759 |   0.0 | 88.04 | 86.09 |####################| ^ (1.3%, 42.63, 1.9, 1%) 
0 >>  5/20 <<   Training | 0.7496 (30, 31,  6, 33) | 0.775 |   0.0 | 89.01 | 86.56 |-------------------------|
0             Validation | 0.7622 (31, 30,  6, 32) | 0.742 |   0.0 | 88.38 | 86.10 |####################| ^ (1.3%, 46.13, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.7487 (29, 32,  6, 33) | 0.778 |   0.0 | 89.00 | 86.65 |--------------------------|
0             Validation | 0.7619 (30, 31,  6, 32) | 0.749 |   0.0 | 88.36 | 86.13 |#####################| ^ (1.4%, 41.82, 2.3, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (214 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.7445 (31, 30,  7, 33) | 0.796 |   0.0 | 89.26 | 86.73 |---------------------------|
0             Validation | 0.7571 (32, 29,  6, 32) | 0.774 |   0.0 | 88.67 | 86.27 |######################| ^ (1.3%, 43.49, 2.7, 0%) 
0 >>  8/20 <<   Training | 0.7435 (30, 30,  7, 33) | 0.794 |   0.0 | 89.29 | 86.75 |---------------------------|
0             Validation | 0.7562 (31, 30,  6, 32) | 0.767 |   0.0 | 88.69 | 86.29 |######################| ^ (1.3%, 42.66, 2.8, 0%) 
0 >>  9/20 <<   Training | 0.7424 (30, 31,  6, 32) | 0.797 |   0.0 | 89.33 | 86.77 |---------------------------|
0             Validation | 0.7558 (32, 31,  6, 31) | 0.768 |   0.0 | 88.69 | 86.29 |######################| ^ (1.3%, 41.65, 2.5, 0%) 
0 >> 10/20 <<   Training | 0.7421 (31, 31,  7, 32) | 0.806 |   0.0 | 89.38 | 86.77 |---------------------------|
0             Validation | 0.7554 (32, 30,  6, 31) | 0.783 |   0.0 | 88.77 | 86.27 |######################| ^ (1.4%, 39.61, 2.1, 0%) 
Increase training batch size: 8192 -> 16384 (107 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.7412 (31, 31,  6, 32) | 0.804 |   0.0 | 89.39 | 86.79 |---------------------------|
0             Validation | 0.7552 (32, 30,  6, 31) | 0.781 |   0.0 | 88.75 | 86.29 |######################| ^ (1.4%, 41.34, 2.2, 0%) 
0 >> 12/20 <<   Training | 0.7410 (30, 31,  6, 32) | 0.801 |   0.0 | 89.42 | 86.80 |---------------------------|
0             Validation | 0.7546 (32, 30,  6, 32) | 0.777 |   0.0 | 88.78 | 86.31 |#######################| ^ (1.3%, 40.92, 2.6, 0%) 
0 >> 13/20 <<   Training | 0.7407 (30, 31,  6, 32) | 0.804 |   0.0 | 89.42 | 86.80 |----------------------------|
0             Validation | 0.7546 (31, 30,  6, 32) | 0.781 |   0.0 | 88.76 | 86.31 |#######################| ^ (1.3%, 41.19, 2.7, 0%) 
0 >> 14/20 <<   Training | 0.7405 (30, 32,  7, 32) | 0.802 |   0.0 | 89.43 | 86.81 |----------------------------|
0             Validation | 0.7544 (31, 31,  6, 32) | 0.780 |   0.0 | 88.76 | 86.31 |#######################| ^ (1.4%, 40.82, 2.5, 0%) 
0 >> 15/20 <<   Training | 0.7404 (30, 31,  6, 32) | 0.802 |   0.0 | 89.44 | 86.81 |----------------------------|
0             Validation | 0.7542 (31, 31,  6, 32) | 0.779 |   0.0 | 88.78 | 86.32 |#######################| ^ (1.3%, 41.64, 2.5, 0%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.7402 (30, 31,  6, 32) | 0.803 |   0.0 | 89.44 | 86.81 |----------------------------|
0             Validation | 0.7543 (32, 31,  6, 31) | 0.783 |   0.0 | 88.78 | 86.32 |#######################| ^ (1.4%, 41.24, 2.6, 0%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.7402 (30, 31,  6, 32) | 0.804 |   0.0 | 89.44 | 86.81 |----------------------------|
0             Validation | 0.7545 (32, 31,  6, 31) | 0.783 |   0.0 | 88.76 | 86.31 |#######################| ^ (1.4%, 40.98, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.7401 (30, 31,  6, 32) | 0.803 |   0.0 | 89.45 | 86.82 |----------------------------|
0             Validation | 0.7542 (31, 31,  6, 32) | 0.785 |   0.0 | 88.78 | 86.32 |#######################| ^ (1.4%, 40.89, 2.4, 0%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.7401 (30, 31,  6, 32) | 0.803 |   0.0 | 89.45 | 86.82 |----------------------------|
0             Validation | 0.7542 (32, 31,  6, 31) | 0.785 |   0.0 | 88.78 | 86.32 |#######################| ^ (1.4%, 40.88, 2.3, 0%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.7401 (30, 31,  6, 32) | 0.803 |   0.0 | 89.45 | 86.82 |----------------------------|
0             Validation | 0.7542 (32, 31,  6, 31) | 0.785 |   0.0 | 88.78 | 86.31 |#######################| ^ (1.4%, 40.90, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7601 (33, 32,  5, 30) | 0.521 |   0.0 | 86.26 | 87.51 |-----------------------------------|
0             Validation | 0.7594 (33, 32,  5, 29) | 0.531 |   0.0 | 86.11 | 87.71 |#####################################| ^ (0.6%, 85.68, 1.1, 26%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (279 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7175 (32, 31,  5, 32) | 0.567 |   0.0 | 88.09 | 88.40 |--------------------------------------------|
0             Validation | 0.7203 (33, 31,  5, 31) | 0.602 |   0.0 | 87.69 | 88.59 |#############################################| ^ (0.5%, 64.50, 2.7, 0%) 
0 >>  3/20 <<   Training | 0.7121 (29, 33,  5, 33) | 0.591 |   0.0 | 88.39 | 88.55 |---------------------------------------------|
0             Validation | 0.7163 (29, 33,  5, 32) | 0.642 |   0.0 | 87.99 | 88.69 |##############################################| ^ (0.4%, 68.08, 1.5, 4%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (139 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7064 (31, 33,  5, 31) | 0.601 |   0.0 | 88.60 | 88.67 |----------------------------------------------|
0             Validation | 0.7106 (31, 33,  5, 30) | 0.630 |   0.0 | 88.21 | 88.78 |###############################################| ^ (0.4%, 64.20, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.7033 (33, 30,  5, 31) | 0.620 |   0.0 | 88.75 | 88.75 |-----------------------------------------------|
0             Validation | 0.7083 (34, 30,  5, 31) | 0.667 |   0.0 | 88.30 | 88.88 |################################################| ^ (0.4%, 71.33, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.7017 (30, 32,  5, 32) | 0.618 |   0.0 | 88.89 | 88.75 |-----------------------------------------------|
0             Validation | 0.7075 (31, 32,  5, 32) | 0.672 |   0.0 | 88.38 | 88.86 |################################################| ^ (0.4%, 71.39, 1.7, 2%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (69 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6989 (32, 32,  5, 31) | 0.636 |   0.0 | 89.00 | 88.81 |------------------------------------------------|
0             Validation | 0.7048 (32, 32,  5, 31) | 0.652 |   0.0 | 88.50 | 88.90 |################################################| ^ (0.4%, 69.20, 2.2, 0%) 
0 >>  8/20 <<   Training | 0.6991 (32, 30,  6, 32) | 0.633 |   0.0 | 89.04 | 88.81 |------------------------------------------------|
0             Validation | 0.7055 (33, 30,  6, 31) | 0.656 |   0.0 | 88.51 | 88.90 |################################################| ^ (0.4%, 67.18, 1.6, 3%) 
0 >>  9/20 <<   Training | 0.6984 (31, 33,  5, 31) | 0.638 |   0.0 | 88.99 | 88.82 |------------------------------------------------|
0             Validation | 0.7046 (32, 33,  5, 30) | 0.643 |   0.0 | 88.47 | 88.90 |################################################| ^ (0.4%, 67.87, 1.2, 23%) 
0 >> 10/20 <<   Training | 0.6985 (33, 31,  5, 30) | 0.647 |   0.0 | 89.05 | 88.83 |------------------------------------------------|
0             Validation | 0.7048 (34, 31,  5, 30) | 0.649 |   0.0 | 88.54 | 88.91 |#################################################| ^ (0.3%, 70.45, 2.0, 0%) 
Increase training batch size: 8192 -> 16384 (34 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6973 (31, 32,  6, 31) | 0.638 |   0.0 | 89.08 | 88.85 |------------------------------------------------|
0             Validation | 0.7038 (31, 32,  6, 31) | 0.658 |   0.0 | 88.55 | 88.91 |#################################################| ^ (0.3%, 67.70, 1.5, 5%) 
0 >> 12/20 <<   Training | 0.6972 (31, 31,  5, 32) | 0.638 |   0.0 | 89.11 | 88.85 |------------------------------------------------|
0             Validation | 0.7038 (32, 31,  5, 32) | 0.657 |   0.0 | 88.58 | 88.92 |#################################################| ^ (0.3%, 68.23, 1.7, 1%) 
0 >> 13/20 <<   Training | 0.6972 (31, 32,  6, 31) | 0.638 |   0.0 | 89.05 | 88.84 |------------------------------------------------|
0             Validation | 0.7039 (32, 32,  5, 31) | 0.663 |   0.0 | 88.52 | 88.91 |#################################################| ^ (0.4%, 66.69, 1.3, 12%) 
0 >> 14/20 <<   Training | 0.6968 (32, 31,  6, 31) | 0.636 |   0.0 | 89.10 | 88.86 |------------------------------------------------|
0             Validation | 0.7036 (32, 31,  6, 31) | 0.661 |   0.0 | 88.57 | 88.92 |#################################################| ^ (0.3%, 68.46, 1.3, 12%) 
0 >> 15/20 <<   Training | 0.6970 (32, 32,  5, 30) | 0.646 |   0.0 | 89.06 | 88.85 |------------------------------------------------|
0             Validation | 0.7035 (33, 32,  5, 30) | 0.657 |   0.0 | 88.54 | 88.92 |#################################################| ^ (0.4%, 69.20, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.6967 (32, 31,  5, 31) | 0.639 |   0.0 | 89.10 | 88.86 |------------------------------------------------|
0             Validation | 0.7035 (33, 31,  5, 31) | 0.662 |   0.0 | 88.56 | 88.92 |#################################################| ^ (0.4%, 69.11, 1.6, 4%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.6966 (32, 31,  5, 32) | 0.641 |   0.0 | 89.11 | 88.86 |------------------------------------------------|
0             Validation | 0.7035 (32, 31,  5, 31) | 0.662 |   0.0 | 88.58 | 88.92 |#################################################| ^ (0.3%, 68.39, 1.4, 9%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.6965 (32, 32,  6, 31) | 0.643 |   0.0 | 89.10 | 88.86 |------------------------------------------------|
0             Validation | 0.7034 (32, 32,  5, 31) | 0.664 |   0.0 | 88.57 | 88.92 |#################################################| ^ (0.3%, 68.30, 1.7, 2%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.6965 (32, 32,  6, 31) | 0.643 |   0.0 | 89.11 | 88.86 |------------------------------------------------|
0             Validation | 0.7034 (32, 31,  5, 31) | 0.664 |   0.0 | 88.57 | 88.92 |#################################################| ^ (0.3%, 68.22, 1.6, 3%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.6965 (32, 32,  6, 31) | 0.643 |   0.0 | 89.10 | 88.86 |------------------------------------------------|
0             Validation | 0.7034 (32, 31,  5, 31) | 0.666 |   0.0 | 88.57 | 88.92 |#################################################| ^ (0.3%, 68.23, 1.6, 3%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7558 (26, 36,  5, 33) | 0.485 |   0.0 | 86.84 | 87.65 |------------------------------------|
0             Validation | 0.7559 (27, 36,  5, 32) | 0.506 |   0.0 | 86.77 | 87.59 |###################################| ^ (0.4%, 64.29, 3.1, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (277 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7181 (32, 30,  5, 32) | 0.568 |   0.0 | 87.98 | 88.44 |--------------------------------------------|
0             Validation | 0.7224 (33, 30,  5, 32) | 0.568 |   0.0 | 87.76 | 88.37 |###########################################| ^ (0.3%, 62.27, 3.6, 0%) 
0 >>  3/20 <<   Training | 0.7171 (33, 30,  5, 32) | 0.589 |   0.0 | 88.11 | 88.33 |-------------------------------------------|
0             Validation | 0.7221 (34, 29,  5, 31) | 0.599 |   0.0 | 87.84 | 88.28 |##########################################| ^ (0.4%, 61.80, 3.9, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (138 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7062 (33, 31,  5, 31) | 0.610 |   0.0 | 88.42 | 88.71 |-----------------------------------------------|
0             Validation | 0.7122 (34, 31,  5, 30) | 0.649 |   0.0 | 88.10 | 88.61 |##############################################| ^ (0.3%, 60.85, 4.5, 0%) 
0 >>  5/20 <<   Training | 0.7058 (32, 31,  5, 32) | 0.612 |   0.0 | 88.55 | 88.70 |----------------------------------------------|
0             Validation | 0.7134 (33, 30,  5, 32) | 0.630 |   0.0 | 88.17 | 88.60 |#############################################| ^ (0.4%, 58.79, 3.3, 0%) 
0 >>  6/20 <<   Training | 0.7029 (32, 32,  5, 30) | 0.621 |   0.0 | 88.66 | 88.73 |-----------------------------------------------|
0             Validation | 0.7108 (34, 31,  5, 30) | 0.646 |   0.0 | 88.24 | 88.63 |##############################################| ^ (0.4%, 56.02, 3.6, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (69 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.7003 (32, 31,  5, 31) | 0.613 |   0.0 | 88.79 | 88.81 |------------------------------------------------|
0             Validation | 0.7082 (34, 30,  5, 31) | 0.648 |   0.0 | 88.36 | 88.70 |###############################################| ^ (0.4%, 56.40, 3.2, 0%) 
0 >>  8/20 <<   Training | 0.6998 (31, 31,  5, 32) | 0.618 |   0.0 | 88.85 | 88.82 |------------------------------------------------|
0             Validation | 0.7073 (33, 31,  5, 31) | 0.662 |   0.0 | 88.42 | 88.72 |###############################################| ^ (0.4%, 55.28, 3.4, 0%) 
0 >>  9/20 <<   Training | 0.6996 (31, 32,  5, 31) | 0.621 |   0.0 | 88.80 | 88.82 |------------------------------------------------|
0             Validation | 0.7074 (33, 31,  5, 30) | 0.640 |   0.0 | 88.37 | 88.71 |###############################################| ^ (0.4%, 56.20, 3.6, 0%) 
0 >> 10/20 <<   Training | 0.6996 (32, 33,  6, 30) | 0.619 |   0.0 | 88.83 | 88.82 |------------------------------------------------|
0             Validation | 0.7076 (34, 32,  5, 29) | 0.632 |   0.0 | 88.38 | 88.71 |###############################################| ^ (0.4%, 56.07, 3.8, 0%) 
Increase training batch size: 8192 -> 16384 (34 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6984 (31, 31,  5, 32) | 0.619 |   0.0 | 88.94 | 88.84 |------------------------------------------------|
0             Validation | 0.7066 (33, 31,  5, 31) | 0.657 |   0.0 | 88.48 | 88.72 |###############################################| ^ (0.4%, 55.06, 3.2, 0%) 
0 >> 12/20 <<   Training | 0.6984 (31, 31,  5, 32) | 0.621 |   0.0 | 88.96 | 88.84 |------------------------------------------------|
0             Validation | 0.7069 (33, 30,  5, 31) | 0.653 |   0.0 | 88.50 | 88.72 |###############################################| ^ (0.4%, 55.07, 2.9, 0%) 
0 >> 13/20 <<   Training | 0.6981 (31, 32,  5, 31) | 0.617 |   0.0 | 88.93 | 88.84 |------------------------------------------------|
0             Validation | 0.7064 (33, 31,  5, 31) | 0.657 |   0.0 | 88.47 | 88.73 |###############################################| ^ (0.4%, 55.59, 3.4, 0%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7220 (27, 31,  8, 35) | 0.530 |   0.0 | 87.15 | 87.94 |---------------------------------------|
0             Validation | 0.7248 (28, 30,  8, 34) | 0.536 |   0.0 | 87.02 | 87.88 |######################################| ^ (0.4%, 33.96, 2.2, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (277 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.7008 (30, 29,  7, 34) | 0.594 |   0.0 | 87.75 | 88.51 |---------------------------------------------|
0             Validation | 0.7057 (32, 28,  7, 33) | 0.589 |   0.0 | 87.61 | 88.39 |###########################################| ^ (0.4%, 31.16, 2.8, 0%) 
0 >>  3/20 <<   Training | 0.7000 (31, 28,  8, 33) | 0.582 |   0.0 | 87.85 | 88.47 |--------------------------------------------|
0             Validation | 0.7068 (33, 27,  7, 32) | 0.579 |   0.0 | 87.57 | 88.34 |###########################################| ^ (0.4%, 32.11, 2.4, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (138 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6886 (30, 30,  8, 32) | 0.629 |   0.0 | 88.17 | 88.86 |------------------------------------------------|
0             Validation | 0.6953 (32, 29,  8, 31) | 0.638 |   0.0 | 87.89 | 88.71 |###############################################| ^ (0.4%, 31.61, 2.8, 0%) 
0 >>  5/20 <<   Training | 0.6875 (29, 29,  8, 34) | 0.623 |   0.0 | 88.32 | 88.89 |------------------------------------------------|
0             Validation | 0.6945 (31, 29,  8, 33) | 0.648 |   0.0 | 88.02 | 88.72 |###############################################| ^ (0.5%, 33.04, 2.9, 0%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8700 (27, 30,  8, 35) | 0.513 |   0.0 | 86.66 | 77.53 ||
0             Validation | 0.8725 (28, 29,  8, 35) | 0.493 |   0.0 | 86.85 | 77.44 || ^ (1.1%, 10.74, 1.2, 21%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (191 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8251 (28, 31,  8, 33) | 0.625 |   0.0 | 87.99 | 80.09 ||
0             Validation | 0.8313 (28, 31,  8, 33) | 0.593 |   0.0 | 88.03 | 79.78 || ^ (1.2%, 9.07, 1.3, 11%) 
0 >>  3/20 <<   Training | 0.8188 (27, 31,  8, 34) | 0.636 |   0.0 | 88.16 | 80.49 ||
0             Validation | 0.8245 (28, 31,  8, 34) | 0.617 |   0.0 | 88.26 | 80.18 || ^ (1.1%, 12.19, 2.0, 0%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (95 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8143 (26, 30,  8, 37) | 0.654 |   0.0 | 88.36 | 80.63 ||
0             Validation | 0.8211 (26, 29,  8, 37) | 0.614 |   0.0 | 88.41 | 80.29 || ^ (1.2%, 10.45, 2.2, 0%) 
0 >>  5/20 <<   Training | 0.8102 (26, 31,  8, 35) | 0.666 |   0.0 | 88.48 | 80.85 ||
0             Validation | 0.8181 (27, 30,  8, 35) | 0.628 |   0.0 | 88.50 | 80.42 || ^ (1.4%, 11.04, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.8095 (26, 32,  8, 35) | 0.661 |   0.0 | 88.49 | 80.87 ||
0             Validation | 0.8182 (27, 31,  8, 35) | 0.615 |   0.0 | 88.47 | 80.38 || ^ (1.7%, 11.27, 2.0, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (47 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8082 (26, 32,  8, 34) | 0.679 |   0.0 | 88.54 | 80.97 ||
0             Validation | 0.8169 (27, 31,  8, 33) | 0.624 |   0.0 | 88.51 | 80.49 || ^ (1.6%, 11.41, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.8070 (28, 31,  8, 33) | 0.671 |   0.0 | 88.52 | 81.03 ||
0             Validation | 0.8163 (29, 30,  8, 33) | 0.630 |   0.0 | 88.52 | 80.56 || ^ (1.5%, 11.50, 1.8, 1%) 
0 >>  9/20 <<   Training | 0.8064 (27, 30,  8, 35) | 0.676 |   0.0 | 88.58 | 81.03 ||
0             Validation | 0.8155 (27, 30,  8, 35) | 0.625 |   0.0 | 88.58 | 80.56 || ^ (1.5%, 11.63, 1.8, 1%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7376 (28, 29,  9, 34) | 0.741 |   0.0 | 87.29 | 87.70 |------------------------------------|
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | Sig.  |       | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 |   0.0 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 |   0.0 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6117 (26, 28, 13, 32) | 0.783 |   0.0 | 87.94 | 88.57 |---------------------------------------------|
0             Validation | 0.6161 (27, 28, 13, 31) | 0.776 |   0.0 | 87.55 | 88.40 |############################################| ^ (0.5%, 15.34, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6191 (24, 25, 14, 37) | 0.766 |   0.0 | 87.94 | 88.45 |--------------------------------------------|
0             Validation | 0.6242 (25, 25, 14, 37) | 0.757 |   0.0 | 87.44 | 88.29 |##########################################| ^ (0.5%, 16.04, 1.0, 35%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6039 (27, 29, 14, 31) | 0.819 |   0.0 | 88.41 | 88.85 |------------------------------------------------|
0             Validation | 0.6092 (27, 29, 13, 31) | 0.816 |   0.0 | 88.04 | 88.64 |##############################################| ^ (0.6%, 14.48, 1.0, 39%) 
0 >>  5/20 <<   Training | 0.6043 (26, 28, 13, 33) | 0.816 |   0.0 | 88.42 | 88.84 |------------------------------------------------|
0             Validation | 0.6101 (27, 28, 13, 32) | 0.823 |   0.0 | 87.97 | 88.63 |##############################################| ^ (0.6%, 15.43, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6021 (26, 30, 14, 30) | 0.830 |   0.0 | 88.43 | 88.92 |-------------------------------------------------|
0             Validation | 0.6080 (26, 30, 14, 30) | 0.834 |   0.0 | 88.03 | 88.69 |##############################################| ^ (0.7%, 15.70, 1.6, 3%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6011 (25, 29, 14, 32) | 0.825 |   0.0 | 88.52 | 88.94 |-------------------------------------------------|
0             Validation | 0.6069 (26, 29, 14, 32) | 0.837 |   0.0 | 88.11 | 88.72 |###############################################| ^ (0.6%, 15.10, 1.4, 8%) 
0 >>  8/20 <<   Training | 0.6012 (27, 28, 14, 31) | 0.828 |   0.0 | 88.57 | 88.93 |-------------------------------------------------|
0             Validation | 0.6072 (28, 28, 14, 31) | 0.837 |   0.0 | 88.15 | 88.70 |##############################################| ^ (0.7%, 15.02, 1.1, 28%) 
0 >>  9/20 <<   Training | 0.6012 (27, 30, 14, 29) | 0.834 |   0.0 | 88.52 | 88.96 |-------------------------------------------------|
0             Validation | 0.6073 (28, 30, 14, 29) | 0.845 |   0.0 | 88.08 | 88.73 |###############################################| ^ (0.7%, 15.58, 0.6, 87%) 
0 >> 10/20 <<   Training | 0.6017 (28, 27, 14, 31) | 0.823 |   0.0 | 88.52 | 88.95 |-------------------------------------------------|
0             Validation | 0.6078 (29, 27, 14, 31) | 0.824 |   0.0 | 88.10 | 88.71 |###############################################| ^ (0.7%, 15.34, 1.6, 4%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6000 (27, 28, 14, 31) | 0.837 |   0.0 | 88.59 | 88.99 |-------------------------------------------------|
0             Validation | 0.6062 (27, 28, 14, 31) | 0.838 |   0.0 | 88.16 | 88.75 |###############################################| ^ (0.7%, 15.21, 1.4, 10%) 
0 >> 12/20 <<   Training | 0.5998 (26, 29, 14, 32) | 0.840 |   0.0 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6059 (26, 29, 14, 31) | 0.841 |   0.0 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.04, 1.3, 14%) 
0 >> 13/20 <<   Training | 0.5999 (25, 29, 14, 32) | 0.835 |   0.0 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6060 (26, 29, 14, 32) | 0.837 |   0.0 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.25, 1.4, 9%) 
0 >> 14/20 <<   Training | 0.5996 (26, 29, 14, 31) | 0.838 |   0.0 | 88.64 | 89.00 |-------------------------------------------------|
0             Validation | 0.6058 (27, 29, 14, 31) | 0.843 |   0.0 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.24, 0.9, 50%) 
0 >> 15/20 <<   Training | 0.5996 (26, 29, 14, 32) | 0.837 |   0.0 | 88.63 | 89.00 |-------------------------------------------------|
0             Validation | 0.6059 (27, 29, 14, 31) | 0.839 |   0.0 | 88.17 | 88.76 |###############################################| ^ (0.7%, 15.04, 1.4, 9%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.837 |   0.0 | 88.64 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (27, 29, 14, 31) | 0.846 |   0.0 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.06, 1.1, 25%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.840 |   0.0 | 88.63 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (26, 29, 14, 31) | 0.844 |   0.0 | 88.19 | 88.76 |###############################################| ^ (0.7%, 15.07, 1.0, 34%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5994 (26, 29, 14, 31) | 0.839 |   0.0 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.844 |   0.0 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.09, 0.9, 49%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 |   0.0 | 88.65 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 28, 14, 31) | 0.843 |   0.0 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.2, 19%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 |   0.0 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.843 |   0.0 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.3, 13%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.443 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.439 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6117 (26, 28, 13, 32) | 0.783 | 0.495 | 87.94 | 88.57 |---------------------------------------------|
0             Validation | 0.6161 (27, 28, 13, 31) | 0.776 | 0.495 | 87.55 | 88.40 |############################################| ^ (0.5%, 15.34, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6191 (24, 25, 14, 37) | 0.766 | 0.490 | 87.94 | 88.45 |--------------------------------------------|
0             Validation | 0.6242 (25, 25, 14, 37) | 0.757 | 0.482 | 87.44 | 88.29 |##########################################| ^ (0.5%, 16.04, 1.0, 35%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6039 (27, 29, 14, 31) | 0.819 | 0.521 | 88.41 | 88.85 |------------------------------------------------|
0             Validation | 0.6092 (27, 29, 13, 31) | 0.816 | 0.521 | 88.04 | 88.64 |##############################################| ^ (0.6%, 14.48, 1.0, 39%) 
0 >>  5/20 <<   Training | 0.6043 (26, 28, 13, 33) | 0.816 | 0.526 | 88.42 | 88.84 |------------------------------------------------|
0             Validation | 0.6101 (27, 28, 13, 32) | 0.823 | 0.522 | 87.97 | 88.63 |##############################################| ^ (0.6%, 15.43, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6021 (26, 30, 14, 30) | 0.830 | 0.536 | 88.43 | 88.92 |-------------------------------------------------|
0             Validation | 0.6080 (26, 30, 14, 30) | 0.834 | 0.528 | 88.03 | 88.69 |##############################################| ^ (0.7%, 15.70, 1.6, 3%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6011 (25, 29, 14, 32) | 0.825 | 0.538 | 88.52 | 88.94 |-------------------------------------------------|
0             Validation | 0.6069 (26, 29, 14, 32) | 0.837 | 0.530 | 88.11 | 88.72 |###############################################| ^ (0.6%, 15.10, 1.4, 8%) 
0 >>  8/20 <<   Training | 0.6012 (27, 28, 14, 31) | 0.828 | 0.528 | 88.57 | 88.93 |-------------------------------------------------|
0             Validation | 0.6072 (28, 28, 14, 31) | 0.837 | 0.522 | 88.15 | 88.70 |##############################################| ^ (0.7%, 15.02, 1.1, 28%) 
0 >>  9/20 <<   Training | 0.6012 (27, 30, 14, 29) | 0.834 | 0.531 | 88.52 | 88.96 |-------------------------------------------------|
0             Validation | 0.6073 (28, 30, 14, 29) | 0.845 | 0.529 | 88.08 | 88.73 |###############################################| ^ (0.7%, 15.58, 0.6, 87%) 
0 >> 10/20 <<   Training | 0.6017 (28, 27, 14, 31) | 0.823 | 0.523 | 88.52 | 88.95 |-------------------------------------------------|
0             Validation | 0.6078 (29, 27, 14, 31) | 0.824 | 0.520 | 88.10 | 88.71 |###############################################| ^ (0.7%, 15.34, 1.6, 4%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6000 (27, 28, 14, 31) | 0.837 | 0.532 | 88.59 | 88.99 |-------------------------------------------------|
0             Validation | 0.6062 (27, 28, 14, 31) | 0.838 | 0.525 | 88.16 | 88.75 |###############################################| ^ (0.7%, 15.21, 1.4, 10%) 
0 >> 12/20 <<   Training | 0.5998 (26, 29, 14, 32) | 0.840 | 0.535 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6059 (26, 29, 14, 31) | 0.841 | 0.526 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.04, 1.3, 14%) 
0 >> 13/20 <<   Training | 0.5999 (25, 29, 14, 32) | 0.835 | 0.536 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6060 (26, 29, 14, 32) | 0.837 | 0.531 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.25, 1.4, 9%) 
0 >> 14/20 <<   Training | 0.5996 (26, 29, 14, 31) | 0.838 | 0.536 | 88.64 | 89.00 |-------------------------------------------------|
0             Validation | 0.6058 (27, 29, 14, 31) | 0.843 | 0.530 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.24, 0.9, 50%) 
0 >> 15/20 <<   Training | 0.5996 (26, 29, 14, 32) | 0.837 | 0.536 | 88.63 | 89.00 |-------------------------------------------------|
0             Validation | 0.6059 (27, 29, 14, 31) | 0.839 | 0.531 | 88.17 | 88.76 |###############################################| ^ (0.7%, 15.04, 1.4, 9%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.837 | 0.534 | 88.64 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (27, 29, 14, 31) | 0.846 | 0.530 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.06, 1.1, 25%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.840 | 0.535 | 88.63 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (26, 29, 14, 31) | 0.844 | 0.529 | 88.19 | 88.76 |###############################################| ^ (0.7%, 15.07, 1.0, 34%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5994 (26, 29, 14, 31) | 0.839 | 0.535 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.844 | 0.529 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.09, 0.9, 49%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 | 0.534 | 88.65 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 28, 14, 31) | 0.843 | 0.530 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.2, 19%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 | 0.534 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.843 | 0.529 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.3, 13%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6117 (26, 28, 13, 32) | 0.783 | 0.940 | 87.94 | 88.57 |---------------------------------------------|
0             Validation | 0.6161 (27, 28, 13, 31) | 0.776 | 0.928 | 87.55 | 88.40 |############################################| ^ (0.5%, 15.34, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6191 (24, 25, 14, 37) | 0.766 | 0.919 | 87.94 | 88.45 |--------------------------------------------|
0             Validation | 0.6242 (25, 25, 14, 37) | 0.757 | 0.907 | 87.44 | 88.29 |##########################################| ^ (0.5%, 16.04, 1.0, 35%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6039 (27, 29, 14, 31) | 0.819 | 0.983 | 88.41 | 88.85 |------------------------------------------------|
0             Validation | 0.6092 (27, 29, 13, 31) | 0.816 | 0.973 | 88.04 | 88.64 |##############################################| ^ (0.6%, 14.48, 1.0, 39%) 
0 >>  5/20 <<   Training | 0.6043 (26, 28, 13, 33) | 0.816 | 0.978 | 88.42 | 88.84 |------------------------------------------------|
0             Validation | 0.6101 (27, 28, 13, 32) | 0.823 | 0.983 | 87.97 | 88.63 |##############################################| ^ (0.6%, 15.43, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6021 (26, 30, 14, 30) | 0.830 | 0.995 | 88.43 | 88.92 |-------------------------------------------------|
0             Validation | 0.6080 (26, 30, 14, 30) | 0.834 | 0.996 | 88.03 | 88.69 |##############################################| ^ (0.7%, 15.70, 1.6, 3%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6011 (25, 29, 14, 32) | 0.825 | 0.989 | 88.52 | 88.94 |-------------------------------------------------|
0             Validation | 0.6069 (26, 29, 14, 32) | 0.837 | 0.997 | 88.11 | 88.72 |###############################################| ^ (0.6%, 15.10, 1.4, 8%) 
0 >>  8/20 <<   Training | 0.6012 (27, 28, 14, 31) | 0.828 | 0.993 | 88.57 | 88.93 |-------------------------------------------------|
0             Validation | 0.6072 (28, 28, 14, 31) | 0.837 | 0.999 | 88.15 | 88.70 |##############################################| ^ (0.7%, 15.02, 1.1, 28%) 
0 >>  9/20 <<   Training | 0.6012 (27, 30, 14, 29) | 0.834 | 0.999 | 88.52 | 88.96 |-------------------------------------------------|
0             Validation | 0.6073 (28, 30, 14, 29) | 0.845 | 1.008 | 88.08 | 88.73 |###############################################| ^ (0.7%, 15.58, 0.6, 87%) 
0 >> 10/20 <<   Training | 0.6017 (28, 27, 14, 31) | 0.823 | 0.988 | 88.52 | 88.95 |-------------------------------------------------|
0             Validation | 0.6078 (29, 27, 14, 31) | 0.824 | 0.983 | 88.10 | 88.71 |###############################################| ^ (0.7%, 15.34, 1.6, 4%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6000 (27, 28, 14, 31) | 0.837 | 1.003 | 88.59 | 88.99 |-------------------------------------------------|
0             Validation | 0.6062 (27, 28, 14, 31) | 0.838 | 0.999 | 88.16 | 88.75 |###############################################| ^ (0.7%, 15.21, 1.4, 10%) 
0 >> 12/20 <<   Training | 0.5998 (26, 29, 14, 32) | 0.840 | 1.006 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6059 (26, 29, 14, 31) | 0.841 | 1.003 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.04, 1.3, 14%) 
0 >> 13/20 <<   Training | 0.5999 (25, 29, 14, 32) | 0.835 | 1.000 | 88.63 | 88.99 |-------------------------------------------------|
0             Validation | 0.6060 (26, 29, 14, 32) | 0.837 | 0.998 | 88.20 | 88.75 |###############################################| ^ (0.7%, 15.25, 1.4, 9%) 
0 >> 14/20 <<   Training | 0.5996 (26, 29, 14, 31) | 0.838 | 1.005 | 88.64 | 89.00 |-------------------------------------------------|
0             Validation | 0.6058 (27, 29, 14, 31) | 0.843 | 1.005 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.24, 0.9, 50%) 
0 >> 15/20 <<   Training | 0.5996 (26, 29, 14, 32) | 0.837 | 1.004 | 88.63 | 89.00 |-------------------------------------------------|
0             Validation | 0.6059 (27, 29, 14, 31) | 0.839 | 1.000 | 88.17 | 88.76 |###############################################| ^ (0.7%, 15.04, 1.4, 9%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.837 | 1.004 | 88.64 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (27, 29, 14, 31) | 0.846 | 1.008 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.06, 1.1, 25%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5995 (26, 29, 14, 31) | 0.840 | 1.008 | 88.63 | 89.00 |--------------------------------------------------|
0             Validation | 0.6057 (26, 29, 14, 31) | 0.844 | 1.008 | 88.19 | 88.76 |###############################################| ^ (0.7%, 15.07, 1.0, 34%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5994 (26, 29, 14, 31) | 0.839 | 1.006 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.844 | 1.007 | 88.20 | 88.76 |###############################################| ^ (0.7%, 15.09, 0.9, 49%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 | 1.006 | 88.65 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 28, 14, 31) | 0.843 | 1.006 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.2, 19%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5994 (26, 29, 14, 32) | 0.839 | 1.006 | 88.64 | 89.01 |--------------------------------------------------|
0             Validation | 0.6056 (27, 29, 14, 31) | 0.843 | 1.006 | 88.21 | 88.76 |###############################################| ^ (0.7%, 15.08, 1.3, 13%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8359 (28, 34,  9, 29) | 0.704 | 0.704 | 87.49 | 81.42 ||
0             Validation | 0.8367 (28, 33,  9, 29) | 0.710 | 0.710 | 87.76 | 81.26 || ^ (0.7%, 8.24, 1.5, 5%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (503 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8020 (28, 30,  9, 34) | 0.760 | 0.760 | 88.62 | 82.89 ||
0             Validation | 0.8078 (28, 29,  9, 33) | 0.759 | 0.759 | 88.54 | 82.52 || ^ (1.1%, 7.58, 1.5, 5%) 
0 >>  3/20 <<   Training | 0.8002 (25, 30,  9, 37) | 0.779 | 0.779 | 88.97 | 82.97 ||
0             Validation | 0.8059 (25, 29,  9, 36) | 0.761 | 0.761 | 88.94 | 82.59 || ^ (1.1%, 7.43, 1.5, 5%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (251 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.7909 (25, 31, 10, 35) | 0.799 | 0.799 | 89.16 | 83.22 ||
0             Validation | 0.7985 (26, 30, 10, 34) | 0.792 | 0.792 | 89.00 | 82.84 || ^ (1.1%, 7.91, 1.9, 1%) 
0 >>  5/20 <<   Training | 0.7886 (26, 31,  9, 34) | 0.803 | 0.803 | 89.40 | 83.28 ||
0             Validation | 0.7961 (27, 30,  9, 34) | 0.786 | 0.786 | 89.24 | 82.89 || ^ (1.2%, 8.23, 1.6, 4%) 
0 >>  6/20 <<   Training | 0.7886 (26, 30,  9, 34) | 0.808 | 0.808 | 89.41 | 83.23 ||
0             Validation | 0.7968 (27, 30,  9, 34) | 0.791 | 0.791 | 89.23 | 82.82 || ^ (1.2%, 7.99, 1.3, 14%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (125 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.7861 (25, 31,  9, 35) | 0.809 | 0.809 | 89.54 | 83.38 ||
0             Validation | 0.7938 (26, 30,  9, 34) | 0.802 | 0.802 | 89.35 | 82.97 || ^ (1.2%, 8.31, 1.9, 1%) 
0 >>  8/20 <<   Training | 0.7863 (26, 29,  9, 35) | 0.813 | 0.813 | 89.45 | 83.36 ||
0             Validation | 0.7951 (27, 29,  9, 34) | 0.797 | 0.797 | 89.23 | 82.95 || ^ (1.2%, 8.17, 1.6, 3%) 
0 >>  9/20 <<   Training | 0.7852 (26, 31, 10, 34) | 0.818 | 0.818 | 89.57 | 83.41 ||
0             Validation | 0.7930 (26, 30,  9, 34) | 0.802 | 0.802 | 89.38 | 83.00 || ^ (1.2%, 8.43, 1.6, 3%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6142 (28, 27, 13, 32) | 0.768 | 0.922 | 87.72 | 88.57 |---------------------------------------------|
0             Validation | 0.6188 (29, 26, 13, 32) | 0.769 | 0.919 | 87.36 | 88.37 |###########################################| ^ (0.6%, 14.89, 1.1, 26%) 
0 >>  3/20 <<   Training | 0.6132 (24, 30, 14, 32) | 0.763 | 0.916 | 87.92 | 88.47 |--------------------------------------------|
0             Validation | 0.6183 (25, 30, 14, 31) | 0.748 | 0.898 | 87.48 | 88.29 |##########################################| ^ (0.5%, 16.44, 1.4, 10%) 
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6045 (27, 29, 14, 31) | 0.818 | 0.984 | 88.32 | 88.83 |------------------------------------------------|
0             Validation | 0.6099 (27, 29, 14, 31) | 0.803 | 0.958 | 87.93 | 88.62 |##############################################| ^ (0.6%, 14.92, 1.0, 38%) 
0 >>  5/20 <<   Training | 0.6055 (27, 30, 13, 30) | 0.812 | 0.973 | 88.29 | 88.81 |------------------------------------------------|
0             Validation | 0.6111 (27, 30, 13, 29) | 0.811 | 0.967 | 87.88 | 88.60 |#############################################| ^ (0.6%, 15.36, 1.2, 21%) 
0 >>  6/20 <<   Training | 0.6029 (26, 29, 14, 30) | 0.824 | 0.988 | 88.41 | 88.88 |------------------------------------------------|
0             Validation | 0.6087 (27, 29, 14, 30) | 0.822 | 0.983 | 88.01 | 88.65 |##############################################| ^ (0.7%, 15.77, 0.9, 49%) 
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6018 (26, 29, 14, 31) | 0.830 | 0.997 | 88.41 | 88.92 |-------------------------------------------------|
0             Validation | 0.6076 (27, 29, 13, 30) | 0.834 | 0.995 | 88.00 | 88.70 |##############################################| ^ (0.6%, 15.21, 1.2, 23%) 
0 >>  8/20 <<   Training | 0.6015 (27, 28, 14, 31) | 0.827 | 0.992 | 88.52 | 88.92 |-------------------------------------------------|
0             Validation | 0.6074 (27, 28, 14, 31) | 0.831 | 0.991 | 88.08 | 88.69 |##############################################| ^ (0.7%, 14.96, 1.2, 19%) 
0 >>  9/20 <<   Training | 0.6012 (26, 29, 14, 31) | 0.831 | 0.996 | 88.48 | 88.94 |-------------------------------------------------|
0             Validation | 0.6071 (27, 29, 14, 30) | 0.833 | 0.994 | 88.07 | 88.71 |###############################################| ^ (0.7%, 15.42, 0.9, 55%) 
0 >> 10/20 <<   Training | 0.6018 (28, 28, 14, 31) | 0.826 | 0.992 | 88.48 | 88.93 |-------------------------------------------------|
0             Validation | 0.6078 (28, 28, 13, 30) | 0.821 | 0.979 | 88.04 | 88.70 |##############################################| ^ (0.7%, 15.16, 1.4, 8%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6006 (27, 29, 14, 31) | 0.832 | 0.998 | 88.53 | 88.96 |-------------------------------------------------|
0             Validation | 0.6067 (27, 28, 14, 31) | 0.829 | 0.989 | 88.09 | 88.72 |###############################################| ^ (0.7%, 15.07, 0.8, 72%) 
0 >> 12/20 <<   Training | 0.6005 (26, 29, 14, 32) | 0.834 | 1.000 | 88.58 | 88.96 |-------------------------------------------------|
0             Validation | 0.6065 (26, 29, 14, 31) | 0.833 | 0.995 | 88.14 | 88.72 |###############################################| ^ (0.7%, 14.94, 1.2, 19%) 
0 >> 13/20 <<   Training | 0.6005 (25, 29, 14, 32) | 0.832 | 0.998 | 88.58 | 88.96 |-------------------------------------------------|
0             Validation | 0.6066 (26, 29, 14, 32) | 0.831 | 0.991 | 88.13 | 88.72 |###############################################| ^ (0.7%, 15.11, 1.0, 35%) 
0 >> 14/20 <<   Training | 0.6003 (26, 29, 14, 31) | 0.833 | 0.999 | 88.59 | 88.97 |-------------------------------------------------|
0             Validation | 0.6064 (27, 29, 14, 31) | 0.832 | 0.994 | 88.14 | 88.73 |###############################################| ^ (0.7%, 15.05, 0.7, 76%) 
0 >> 15/20 <<   Training | 0.6003 (26, 29, 14, 32) | 0.833 | 0.999 | 88.59 | 88.97 |-------------------------------------------------|
0             Validation | 0.6065 (26, 29, 14, 31) | 0.829 | 0.990 | 88.12 | 88.73 |###############################################| ^ (0.7%, 14.90, 0.8, 72%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.6001 (26, 29, 14, 32) | 0.835 | 1.001 | 88.59 | 88.97 |-------------------------------------------------|
0             Validation | 0.6063 (27, 29, 14, 31) | 0.833 | 0.995 | 88.15 | 88.73 |###############################################| ^ (0.7%, 14.90, 0.9, 57%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.6001 (26, 29, 14, 31) | 0.836 | 1.002 | 88.59 | 88.98 |-------------------------------------------------|
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(-1)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6126 (28, 27, 13, 32) | 0.775 | 0.930 | 87.87 | 88.59 |---------------------------------------------|
0             Validation | 0.6173 (28, 27, 13, 31) | 0.770 | 0.919 | 87.49 | 88.40 |############################################| ^ (0.6%, 14.11, 0.9, 52%) 
0 >>  3/20 <<   Training | 0.6126 (24, 31, 14, 31) | 0.751 | 0.902 | 87.89 | 88.51 |---------------------------------------------|
0             Validation | 0.6176 (25, 30, 14, 31) | 0.746 | 0.894 | 87.44 | 88.33 |###########################################| ^ (0.5%, 15.96, 1.2, 17%) 
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6043 (26, 29, 14, 30) | 0.817 | 0.981 | 88.34 | 88.84 |------------------------------------------------|
0             Validation | 0.6096 (27, 29, 14, 30) | 0.808 | 0.963 | 87.95 | 88.62 |##############################################| ^ (0.7%, 13.59, 1.5, 7%) 
0 >>  5/20 <<   Training | 0.6046 (26, 30, 14, 30) | 0.817 | 0.979 | 88.25 | 88.84 |------------------------------------------------|
0             Validation | 0.6103 (27, 30, 13, 30) | 0.815 | 0.973 | 87.82 | 88.63 |##############################################| ^ (0.6%, 14.54, 0.9, 49%) 
0 >>  6/20 <<   Training | 0.6028 (26, 30, 14, 30) | 0.824 | 0.988 | 88.35 | 88.90 |-------------------------------------------------|
0             Validation | 0.6083 (27, 30, 14, 30) | 0.819 | 0.976 | 87.96 | 88.67 |##############################################| ^ (0.7%, 15.19, 1.1, 28%) 
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6017 (26, 29, 14, 31) | 0.828 | 0.993 | 88.41 | 88.93 |-------------------------------------------------|
0             Validation | 0.6073 (27, 29, 13, 31) | 0.829 | 0.990 | 88.00 | 88.70 |###############################################| ^ (0.7%, 14.33, 1.3, 16%) 
0 >>  8/20 <<   Training | 0.6015 (27, 29, 14, 31) | 0.830 | 0.994 | 88.51 | 88.93 |-------------------------------------------------|
0             Validation | 0.6071 (27, 29, 14, 31) | 0.827 | 0.987 | 88.06 | 88.71 |###############################################| ^ (0.7%, 14.30, 1.3, 15%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
setGhostBatches(-1)
0 >>  1/20 <<   Training | 0.6600 (28, 33, 14, 25) | 0.665 | 0.802 | 85.93 | 87.19 |-------------------------------|
0             Validation | 0.6613 (28, 33, 14, 25) | 0.662 | 0.793 | 85.72 | 87.14 |###############################| ^ (0.5%, 13.22, 1.1, 31%) 
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8124 (23, 21, 12, 44) | 0.650 | 0.855 | 86.97 | 87.86 |--------------------------------------|
0             Validation | 0.8460 (24, 21, 12, 44) | 0.649 | 0.851 | 86.78 | 87.68 |####################################| ^ (0.6%, 215.44, 1.0, 36%) 
0 >>  3/20 <<   Training | 0.6291 (25, 30, 12, 33) | 0.673 | 0.809 | 87.54 | 87.84 |--------------------------------------|
0             Validation | 0.6324 (26, 30, 12, 32) | 0.662 | 0.798 | 87.32 | 87.71 |#####################################| ^ (0.4%, 15.59, 0.8, 71%) 
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6108 (27, 29, 13, 30) | 0.781 | 0.938 | 87.77 | 88.62 |----------------------------------------------|
0             Validation | 0.6147 (28, 29, 13, 30) | 0.775 | 0.925 | 87.62 | 88.39 |###########################################| ^ (0.6%, 13.72, 1.3, 14%) 
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6185 (25, 28, 13, 33) | 0.762 | 0.915 | 87.75 | 88.29 |------------------------------------------|
0             Validation | 0.6226 (26, 28, 13, 33) | 0.751 | 0.900 | 87.38 | 88.13 |#########################################| ^ (0.5%, 14.54, 0.8, 64%) 
0 >>  3/20 <<   Training | 0.6277 (22, 25, 14, 39) | 0.749 | 0.900 | 87.77 | 88.29 |------------------------------------------|
0             Validation | 0.6330 (23, 25, 13, 39) | 0.740 | 0.887 | 87.25 | 88.11 |#########################################| ^ (0.5%, 14.27, 0.5, 93%) 
setGhostBatches(-1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6061 (27, 30, 14, 30) | 0.811 | 0.974 | 88.19 | 88.79 |-----------------------------------------------|
0             Validation | 0.6115 (27, 30, 14, 30) | 0.808 | 0.964 | 87.78 | 88.57 |#############################################| ^ (0.6%, 13.97, 1.3, 12%) 
0 >>  5/20 <<   Training | 0.6054 (27, 30, 14, 30) | 0.815 | 0.978 | 88.20 | 88.82 |------------------------------------------------|
0             Validation | 0.6116 (27, 30, 13, 29) | 0.808 | 0.964 | 87.73 | 88.60 |#############################################| ^ (0.6%, 14.73, 1.5, 5%) 
0 >>  6/20 <<   Training | 0.6034 (26, 30, 14, 30) | 0.829 | 0.994 | 88.31 | 88.87 |------------------------------------------------|
0             Validation | 0.6094 (27, 30, 14, 30) | 0.815 | 0.972 | 87.88 | 88.63 |##############################################| ^ (0.7%, 15.11, 1.2, 17%) 
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6431 (24, 26, 12, 37) | 0.703 | 0.848 | 86.41 | 87.59 |-----------------------------------|
0             Validation | 0.6463 (25, 26, 12, 37) | 0.688 | 0.827 | 85.96 | 87.47 |##################################| ^ (0.4%, 14.03, 1.4, 10%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6117 (26, 28, 13, 32) | 0.783 | 0.940 | 87.94 | 88.57 |---------------------------------------------|
0             Validation | 0.6161 (27, 28, 13, 31) | 0.776 | 0.928 | 87.55 | 88.40 |############################################| ^ (0.5%, 15.34, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6191 (24, 25, 14, 37) | 0.766 | 0.919 | 87.94 | 88.45 |--------------------------------------------|
0             Validation | 0.6242 (25, 25, 14, 37) | 0.757 | 0.907 | 87.44 | 88.29 |##########################################| ^ (0.5%, 16.04, 1.0, 35%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6290 (26, 30, 13, 30) | 0.736 | 0.881 | 86.42 | 88.05 |----------------------------------------|
0             Validation | 0.6308 (26, 31, 13, 30) | 0.749 | 0.902 | 86.40 | 87.81 |######################################| ^ (0.7%, 16.85, 1.2, 21%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6147 (25, 29, 13, 33) | 0.756 | 0.904 | 87.61 | 88.55 |---------------------------------------------|
0             Validation | 0.6174 (25, 29, 13, 33) | 0.767 | 0.922 | 87.58 | 88.34 |###########################################| ^ (0.6%, 18.92, 1.2, 21%) 
0 >>  3/20 <<   Training | 0.6104 (26, 28, 14, 32) | 0.777 | 0.929 | 87.64 | 88.75 |-----------------------------------------------|
0             Validation | 0.6132 (26, 28, 14, 32) | 0.791 | 0.950 | 87.59 | 88.52 |#############################################| ^ (0.6%, 16.34, 0.8, 59%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6076 (27, 30, 14, 29) | 0.790 | 0.946 | 87.85 | 88.83 |------------------------------------------------|
0             Validation | 0.6108 (27, 30, 14, 29) | 0.805 | 0.968 | 87.75 | 88.59 |#############################################| ^ (0.6%, 15.67, 0.9, 58%) 
0 >>  5/20 <<   Training | 0.6100 (24, 28, 14, 33) | 0.763 | 0.913 | 87.85 | 88.70 |----------------------------------------------|
0             Validation | 0.6138 (24, 28, 14, 33) | 0.773 | 0.931 | 87.81 | 88.43 |############################################| ^ (0.7%, 17.40, 1.0, 39%) 
0 >>  6/20 <<   Training | 0.6060 (27, 29, 14, 30) | 0.793 | 0.950 | 87.95 | 88.88 |------------------------------------------------|
0             Validation | 0.6097 (27, 29, 14, 30) | 0.807 | 0.968 | 87.95 | 88.61 |##############################################| ^ (0.7%, 15.56, 1.3, 13%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.6039 (26, 29, 14, 31) | 0.801 | 0.959 | 88.09 | 88.95 |-------------------------------------------------|
0             Validation | 0.6078 (26, 29, 14, 31) | 0.819 | 0.981 | 87.98 | 88.68 |##############################################| ^ (0.7%, 15.91, 0.9, 50%) 
0 >>  8/20 <<   Training | 0.6039 (27, 29, 14, 31) | 0.803 | 0.962 | 88.10 | 88.94 |-------------------------------------------------|
0             Validation | 0.6078 (27, 29, 14, 31) | 0.823 | 0.987 | 87.99 | 88.68 |##############################################| ^ (0.7%, 15.92, 1.4, 9%) 
0 >>  9/20 <<   Training | 0.6036 (26, 29, 14, 32) | 0.799 | 0.955 | 88.12 | 88.96 |-------------------------------------------------|
0             Validation | 0.6077 (26, 29, 14, 32) | 0.821 | 0.984 | 88.02 | 88.68 |##############################################| ^ (0.7%, 15.11, 0.9, 56%) 
0 >> 10/20 <<   Training | 0.6036 (26, 28, 14, 32) | 0.807 | 0.965 | 88.15 | 88.97 |-------------------------------------------------|
0             Validation | 0.6075 (27, 28, 14, 32) | 0.816 | 0.979 | 88.12 | 88.69 |##############################################| ^ (0.7%, 15.34, 1.1, 26%) 
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.6030 (26, 28, 14, 32) | 0.807 | 0.965 | 88.16 | 88.99 |-------------------------------------------------|
0             Validation | 0.6071 (27, 28, 14, 32) | 0.821 | 0.984 | 88.11 | 88.70 |###############################################| ^ (0.7%, 15.36, 0.8, 68%) 
0 >> 12/20 <<   Training | 0.6029 (26, 28, 14, 32) | 0.805 | 0.961 | 88.17 | 88.99 |-------------------------------------------------|
0             Validation | 0.6071 (26, 28, 14, 32) | 0.823 | 0.988 | 88.08 | 88.70 |##############################################| ^ (0.7%, 15.42, 0.9, 53%) 
0 >> 13/20 <<   Training | 0.6027 (26, 28, 14, 32) | 0.811 | 0.969 | 88.19 | 88.99 |-------------------------------------------------|
0             Validation | 0.6069 (26, 28, 14, 31) | 0.821 | 0.985 | 88.11 | 88.71 |###############################################| ^ (0.7%, 15.51, 0.8, 64%) 
0 >> 14/20 <<   Training | 0.6029 (25, 29, 14, 32) | 0.807 | 0.963 | 88.22 | 88.99 |-------------------------------------------------|
0             Validation | 0.6071 (25, 29, 14, 32) | 0.819 | 0.983 | 88.12 | 88.71 |###############################################| ^ (0.7%, 15.37, 0.5, 93%) 
0 >> 15/20 <<   Training | 0.6027 (25, 29, 14, 32) | 0.806 | 0.963 | 88.20 | 88.99 |-------------------------------------------------|
0             Validation | 0.6070 (26, 29, 14, 32) | 0.821 | 0.984 | 88.09 | 88.71 |###############################################| ^ (0.7%, 15.56, 0.9, 49%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.6025 (26, 29, 14, 32) | 0.807 | 0.963 | 88.20 | 88.99 |-------------------------------------------------|
0             Validation | 0.6069 (26, 29, 14, 32) | 0.823 | 0.988 | 88.10 | 88.71 |###############################################| ^ (0.7%, 15.65, 1.1, 29%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.6025 (26, 29, 14, 31) | 0.808 | 0.965 | 88.19 | 89.00 |-------------------------------------------------|
0             Validation | 0.6068 (26, 29, 14, 31) | 0.821 | 0.985 | 88.09 | 88.71 |###############################################| ^ (0.7%, 15.56, 0.8, 62%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.6025 (26, 29, 14, 32) | 0.808 | 0.965 | 88.20 | 89.00 |-------------------------------------------------|
0             Validation | 0.6069 (26, 29, 14, 32) | 0.820 | 0.983 | 88.11 | 88.71 |###############################################| ^ (0.8%, 15.57, 0.8, 59%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.6025 (26, 29, 14, 32) | 0.808 | 0.965 | 88.20 | 89.00 |-------------------------------------------------|
0             Validation | 0.6068 (26, 29, 14, 31) | 0.820 | 0.983 | 88.11 | 88.71 |###############################################| ^ (0.8%, 15.61, 0.9, 51%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.6025 (26, 29, 14, 32) | 0.807 | 0.965 | 88.20 | 89.00 |-------------------------------------------------|
0             Validation | 0.6068 (26, 29, 14, 31) | 0.820 | 0.984 | 88.10 | 88.71 |###############################################| ^ (0.8%, 15.59, 0.9, 52%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6317 (26, 30, 13, 30) | 0.702 | 0.847 | 86.68 | 87.77 |-------------------------------------|
0             Validation | 0.6339 (26, 30, 13, 30) | 0.697 | 0.835 | 86.61 | 87.85 |######################################| ^ (0.4%, 11.11, 2.7, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1098 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.6111 (26, 29, 13, 32) | 0.787 | 0.947 | 87.65 | 88.61 |----------------------------------------------|
0             Validation | 0.6143 (27, 29, 13, 31) | 0.762 | 0.913 | 87.40 | 88.64 |##############################################| ^ (0.3%, 15.00, 3.8, 0%) 
0 >>  3/20 <<   Training | 0.6094 (28, 28, 14, 30) | 0.792 | 0.952 | 88.04 | 88.59 |---------------------------------------------|
0             Validation | 0.6140 (29, 28, 13, 30) | 0.790 | 0.943 | 87.71 | 88.58 |#############################################| ^ (0.3%, 13.82, 4.0, 0%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (549 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.6039 (26, 30, 14, 30) | 0.822 | 0.986 | 88.26 | 88.80 |-----------------------------------------------|
0             Validation | 0.6085 (27, 29, 14, 30) | 0.787 | 0.941 | 87.89 | 88.77 |###############################################| ^ (0.3%, 14.13, 3.2, 0%) 
0 >>  5/20 <<   Training | 0.6028 (26, 27, 14, 33) | 0.826 | 0.991 | 88.37 | 88.83 |------------------------------------------------|
0             Validation | 0.6072 (27, 27, 14, 32) | 0.816 | 0.972 | 88.05 | 88.81 |################################################| ^ (0.3%, 15.25, 3.1, 0%) 
0 >>  6/20 <<   Training | 0.6025 (24, 30, 14, 32) | 0.832 | 0.998 | 88.46 | 88.85 |------------------------------------------------|
0             Validation | 0.6065 (25, 29, 14, 32) | 0.799 | 0.954 | 88.11 | 88.85 |################################################| ^ (0.3%, 15.21, 3.7, 0%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (274 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.5998 (26, 29, 14, 31) | 0.833 | 0.999 | 88.57 | 88.92 |-------------------------------------------------|
0             Validation | 0.6048 (27, 29, 13, 31) | 0.817 | 0.972 | 88.19 | 88.89 |################################################| ^ (0.2%, 14.61, 3.4, 0%) 
0 >>  8/20 <<   Training | 0.5995 (26, 29, 14, 32) | 0.839 | 1.005 | 88.60 | 88.93 |-------------------------------------------------|
0             Validation | 0.6047 (27, 28, 14, 31) | 0.828 | 0.986 | 88.19 | 88.89 |################################################| ^ (0.2%, 15.47, 3.1, 0%) 
0 >>  9/20 <<   Training | 0.5994 (26, 29, 14, 31) | 0.842 | 1.009 | 88.60 | 88.94 |-------------------------------------------------|
0             Validation | 0.6046 (27, 28, 14, 31) | 0.828 | 0.987 | 88.18 | 88.90 |#################################################| ^ (0.2%, 14.49, 3.3, 0%) 
0 >> 10/20 <<   Training | 0.5993 (26, 29, 14, 32) | 0.834 | 0.999 | 88.64 | 88.93 |-------------------------------------------------|
0             Validation | 0.6044 (26, 29, 14, 31) | 0.820 | 0.977 | 88.22 | 88.90 |#################################################| ^ (0.2%, 15.08, 3.5, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (137 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.5987 (26, 28, 14, 32) | 0.844 | 1.013 | 88.66 | 88.96 |-------------------------------------------------|
0             Validation | 0.6040 (27, 28, 13, 32) | 0.823 | 0.980 | 88.24 | 88.92 |#################################################| ^ (0.2%, 15.12, 3.5, 0%) 
0 >> 12/20 <<   Training | 0.5987 (26, 29, 14, 32) | 0.841 | 1.008 | 88.62 | 88.96 |-------------------------------------------------|
0             Validation | 0.6040 (27, 28, 14, 31) | 0.823 | 0.979 | 88.21 | 88.93 |#################################################| ^ (0.3%, 15.01, 3.9, 0%) 
0 >> 13/20 <<   Training | 0.5987 (26, 28, 14, 32) | 0.845 | 1.013 | 88.67 | 88.97 |-------------------------------------------------|
0             Validation | 0.6040 (27, 28, 14, 32) | 0.827 | 0.985 | 88.25 | 88.92 |#################################################| ^ (0.3%, 15.15, 3.5, 0%) 
0 >> 14/20 <<   Training | 0.5986 (26, 28, 14, 32) | 0.846 | 1.014 | 88.66 | 88.96 |-------------------------------------------------|
0             Validation | 0.6041 (27, 28, 13, 31) | 0.830 | 0.988 | 88.22 | 88.92 |#################################################| ^ (0.2%, 15.09, 3.4, 0%) 
0 >> 15/20 <<   Training | 0.5984 (26, 29, 14, 31) | 0.846 | 1.014 | 88.65 | 88.97 |-------------------------------------------------|
0             Validation | 0.6038 (27, 29, 14, 31) | 0.828 | 0.985 | 88.23 | 88.93 |#################################################| ^ (0.2%, 15.04, 3.6, 0%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.5984 (26, 29, 14, 31) | 0.845 | 1.013 | 88.64 | 88.97 |-------------------------------------------------|
0             Validation | 0.6040 (27, 29, 14, 30) | 0.830 | 0.988 | 88.21 | 88.92 |#################################################| ^ (0.2%, 15.35, 3.3, 0%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.5984 (26, 28, 14, 31) | 0.844 | 1.011 | 88.65 | 88.97 |-------------------------------------------------|
0             Validation | 0.6039 (27, 28, 14, 31) | 0.828 | 0.985 | 88.22 | 88.93 |#################################################| ^ (0.3%, 15.32, 3.2, 0%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.5983 (26, 29, 14, 32) | 0.846 | 1.013 | 88.67 | 88.97 |-------------------------------------------------|
0             Validation | 0.6037 (27, 28, 14, 31) | 0.828 | 0.985 | 88.24 | 88.93 |#################################################| ^ (0.2%, 15.15, 3.8, 0%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.5983 (26, 29, 14, 31) | 0.845 | 1.013 | 88.66 | 88.97 |-------------------------------------------------|
0             Validation | 0.6037 (27, 28, 14, 31) | 0.829 | 0.987 | 88.24 | 88.93 |#################################################| ^ (0.2%, 15.19, 3.5, 0%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.5983 (26, 29, 14, 32) | 0.845 | 1.013 | 88.67 | 88.97 |-------------------------------------------------|
0             Validation | 0.6037 (27, 28, 14, 31) | 0.829 | 0.986 | 88.24 | 88.93 |#################################################| ^ (0.2%, 15.18, 3.5, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2139_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
