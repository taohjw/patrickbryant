0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8934 (34, 40,  7, 20) | 0.869 | -20.0 | 82.36 | 65.00 |------------------------------|
0             Validation | 0.8945 (34, 40,  7, 19) | 0.875 | -20.0 | 82.20 | 64.70 |###########################| ^ (2.0%, 4.02, 1.7, 2%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8749 (35, 40,  7, 18) | 0.869 |  12.9 | 83.78 | 65.89 |--------------------------------------|
0             Validation | 0.8770 (36, 40,  7, 18) | 0.873 | -20.0 | 83.63 | 65.61 |####################################| ^ (1.7%, 3.52, 2.0, 0%) 
0 >>  3/20 <<   Training | 0.8690 (38, 38,  7, 17) | 0.996 | -20.0 | 84.03 | 65.96 |---------------------------------------|
0             Validation | 0.8721 (39, 38,  7, 17) | 1.002 | -20.0 | 83.87 | 65.55 |###################################| ^ (2.6%, 4.20, 1.5, 5%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8648 (38, 39,  6, 17) | 0.961 | -20.0 | 84.44 | 66.42 |--------------------------------------------|
0             Validation | 0.8680 (38, 39,  6, 16) | 0.967 | -20.0 | 84.27 | 65.96 |#######################################| ^ (2.8%, 3.76, 1.6, 3%) 
0 >>  5/20 <<   Training | 0.8646 (39, 38,  6, 17) | 1.018 | -20.0 | 84.56 | 66.42 |--------------------------------------------|
0             Validation | 0.8678 (39, 38,  6, 17) | 1.024 | -20.0 | 84.39 | 65.94 |#######################################| ^ (2.9%, 3.43, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.8647 (40, 37,  6, 16) | 1.065 | -20.0 | 84.59 | 66.35 |-------------------------------------------|
0             Validation | 0.8686 (41, 37,  6, 16) | 1.071 | -20.0 | 84.41 | 65.85 |######################################| ^ (3.1%, 4.02, 2.3, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8591 (39, 40,  6, 16) | 0.977 | -20.0 | 85.22 | 66.53 |---------------------------------------------|
0             Validation | 0.8636 (39, 39,  6, 15) | 0.983 | -20.0 | 85.01 | 65.93 |#######################################| ^ (3.6%, 3.64, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.8587 (40, 38,  6, 16) | 1.039 | -20.0 | 85.24 | 66.61 |----------------------------------------------|
0             Validation | 0.8632 (40, 38,  6, 16) | 1.045 | -20.0 | 85.03 | 65.99 |#######################################| ^ (3.7%, 3.71, 1.7, 2%) 
0 >>  9/20 <<   Training | 0.8582 (39, 39,  7, 16) | 1.011 | -20.0 | 85.28 | 66.59 |---------------------------------------------|
0             Validation | 0.8628 (39, 38,  7, 16) | 1.017 | -20.0 | 85.06 | 65.95 |#######################################| ^ (3.9%, 3.84, 1.9, 1%) 
0 >> 10/20 <<   Training | 0.8581 (38, 39,  6, 16) | 0.983 | -20.0 | 85.29 | 66.55 |---------------------------------------------|
0             Validation | 0.8630 (39, 39,  6, 16) | 0.988 | -20.0 | 85.06 | 65.90 |#######################################| ^ (3.9%, 3.38, 1.9, 1%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.8578 (40, 38,  6, 16) | 1.037 | -20.0 | 85.33 | 66.67 |----------------------------------------------|
0             Validation | 0.8626 (40, 38,  6, 16) | 1.043 | -20.0 | 85.10 | 66.03 |########################################| ^ (3.9%, 3.69, 2.2, 0%) 
0 >> 12/20 <<   Training | 0.8576 (39, 39,  6, 16) | 0.993 | -20.0 | 85.33 | 66.70 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.999 | -20.0 | 85.11 | 66.06 |########################################| ^ (3.8%, 3.68, 1.9, 1%) 
0 >> 13/20 <<   Training | 0.8575 (38, 39,  6, 16) | 0.985 | -20.0 | 85.34 | 66.66 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.991 | -20.0 | 85.11 | 66.00 |########################################| ^ (4.0%, 3.66, 2.1, 0%) 
0 >> 14/20 <<   Training | 0.8575 (39, 38,  7, 16) | 1.019 | -20.0 | 85.34 | 66.71 |-----------------------------------------------|
0             Validation | 0.8624 (40, 38,  7, 16) | 1.025 | -20.0 | 85.11 | 66.04 |########################################| ^ (4.0%, 3.66, 2.1, 0%) 
0 >> 15/20 <<   Training | 0.8574 (38, 39,  6, 16) | 0.983 | -20.0 | 85.35 | 66.73 |-----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.989 | -20.0 | 85.12 | 66.08 |########################################| ^ (3.9%, 3.67, 2.2, 0%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.8573 (39, 39,  6, 16) | 0.987 | -20.0 | 85.36 | 66.68 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.992 | -20.0 | 85.12 | 66.01 |########################################| ^ (4.0%, 3.70, 2.2, 0%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.8572 (39, 39,  6, 16) | 1.000 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.005 | -20.0 | 85.13 | 66.03 |########################################| ^ (4.0%, 3.72, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.8572 (39, 39,  6, 16) | 0.991 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 39,  6, 16) | 0.997 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.71, 2.4, 0%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.8572 (39, 39,  6, 16) | 0.998 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.003 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.70, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.8572 (39, 39,  6, 16) | 1.000 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.006 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.70, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
