0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8934 (34, 40,  7, 20) | 0.869 | -20.0 | 82.36 | 65.00 |------------------------------|
0             Validation | 0.8945 (34, 40,  7, 19) | 0.875 | -20.0 | 82.20 | 64.70 |###########################| ^ (2.0%, 4.02, 1.7, 2%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8749 (35, 40,  7, 18) | 0.869 |  12.9 | 83.78 | 65.89 |--------------------------------------|
0             Validation | 0.8770 (36, 40,  7, 18) | 0.873 | -20.0 | 83.63 | 65.61 |####################################| ^ (1.7%, 3.52, 2.0, 0%) 
0 >>  3/20 <<   Training | 0.8690 (38, 38,  7, 17) | 0.996 | -20.0 | 84.03 | 65.96 |---------------------------------------|
0             Validation | 0.8721 (39, 38,  7, 17) | 1.002 | -20.0 | 83.87 | 65.55 |###################################| ^ (2.6%, 4.20, 1.5, 5%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8648 (38, 39,  6, 17) | 0.961 | -20.0 | 84.44 | 66.42 |--------------------------------------------|
0             Validation | 0.8680 (38, 39,  6, 16) | 0.967 | -20.0 | 84.27 | 65.96 |#######################################| ^ (2.8%, 3.76, 1.6, 3%) 
0 >>  5/20 <<   Training | 0.8646 (39, 38,  6, 17) | 1.018 | -20.0 | 84.56 | 66.42 |--------------------------------------------|
0             Validation | 0.8678 (39, 38,  6, 17) | 1.024 | -20.0 | 84.39 | 65.94 |#######################################| ^ (2.9%, 3.43, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.8647 (40, 37,  6, 16) | 1.065 | -20.0 | 84.59 | 66.35 |-------------------------------------------|
0             Validation | 0.8686 (41, 37,  6, 16) | 1.071 | -20.0 | 84.41 | 65.85 |######################################| ^ (3.1%, 4.02, 2.3, 0%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8591 (39, 40,  6, 16) | 0.977 | -20.0 | 85.22 | 66.53 |---------------------------------------------|
0             Validation | 0.8636 (39, 39,  6, 15) | 0.983 | -20.0 | 85.01 | 65.93 |#######################################| ^ (3.6%, 3.64, 1.7, 2%) 
0 >>  8/20 <<   Training | 0.8587 (40, 38,  6, 16) | 1.039 | -20.0 | 85.24 | 66.61 |----------------------------------------------|
0             Validation | 0.8632 (40, 38,  6, 16) | 1.045 | -20.0 | 85.03 | 65.99 |#######################################| ^ (3.7%, 3.71, 1.7, 2%) 
0 >>  9/20 <<   Training | 0.8582 (39, 39,  7, 16) | 1.011 | -20.0 | 85.28 | 66.59 |---------------------------------------------|
0             Validation | 0.8628 (39, 38,  7, 16) | 1.017 | -20.0 | 85.06 | 65.95 |#######################################| ^ (3.9%, 3.84, 1.9, 1%) 
0 >> 10/20 <<   Training | 0.8581 (38, 39,  6, 16) | 0.983 | -20.0 | 85.29 | 66.55 |---------------------------------------------|
0             Validation | 0.8630 (39, 39,  6, 16) | 0.988 | -20.0 | 85.06 | 65.90 |#######################################| ^ (3.9%, 3.38, 1.9, 1%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.8578 (40, 38,  6, 16) | 1.037 | -20.0 | 85.33 | 66.67 |----------------------------------------------|
0             Validation | 0.8626 (40, 38,  6, 16) | 1.043 | -20.0 | 85.10 | 66.03 |########################################| ^ (3.9%, 3.69, 2.2, 0%) 
0 >> 12/20 <<   Training | 0.8576 (39, 39,  6, 16) | 0.993 | -20.0 | 85.33 | 66.70 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.999 | -20.0 | 85.11 | 66.06 |########################################| ^ (3.8%, 3.68, 1.9, 1%) 
0 >> 13/20 <<   Training | 0.8575 (38, 39,  6, 16) | 0.985 | -20.0 | 85.34 | 66.66 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.991 | -20.0 | 85.11 | 66.00 |########################################| ^ (4.0%, 3.66, 2.1, 0%) 
0 >> 14/20 <<   Training | 0.8575 (39, 38,  7, 16) | 1.019 | -20.0 | 85.34 | 66.71 |-----------------------------------------------|
0             Validation | 0.8624 (40, 38,  7, 16) | 1.025 | -20.0 | 85.11 | 66.04 |########################################| ^ (4.0%, 3.66, 2.1, 0%) 
0 >> 15/20 <<   Training | 0.8574 (38, 39,  6, 16) | 0.983 | -20.0 | 85.35 | 66.73 |-----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.989 | -20.0 | 85.12 | 66.08 |########################################| ^ (3.9%, 3.67, 2.2, 0%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.8573 (39, 39,  6, 16) | 0.987 | -20.0 | 85.36 | 66.68 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.992 | -20.0 | 85.12 | 66.01 |########################################| ^ (4.0%, 3.70, 2.2, 0%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.8572 (39, 39,  6, 16) | 1.000 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.005 | -20.0 | 85.13 | 66.03 |########################################| ^ (4.0%, 3.72, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.8572 (39, 39,  6, 16) | 0.991 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 39,  6, 16) | 0.997 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.71, 2.4, 0%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.8572 (39, 39,  6, 16) | 0.998 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.003 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.70, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.8572 (39, 39,  6, 16) | 1.000 | -20.0 | 85.36 | 66.69 |----------------------------------------------|
0             Validation | 0.8622 (39, 38,  6, 16) | 1.006 | -20.0 | 85.13 | 66.02 |########################################| ^ (4.0%, 3.70, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9074 (30, 42,  8, 21) | 0.720 | -20.0 | 82.31 | 64.64 |--------------------------|
0             Validation | 0.9087 (31, 41,  8, 21) | 0.725 | -20.0 | 82.17 | 64.33 |#######################| ^ (2.1%, 2.94, 1.5, 5%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8821 (36, 37,  8, 20) | 0.945 |   7.3 | 83.47 | 65.93 |---------------------------------------|
0             Validation | 0.8836 (36, 37,  8, 20) | 0.951 | -11.0 | 83.37 | 65.56 |###################################| ^ (2.3%, 2.51, 2.2, 0%) 
0 >>  3/20 <<   Training | 0.8787 (35, 38,  7, 19) | 0.927 | -20.0 | 83.77 | 66.05 |----------------------------------------|
0             Validation | 0.8808 (35, 38,  7, 19) | 0.933 | -20.0 | 83.64 | 65.53 |###################################| ^ (3.3%, 2.70, 1.3, 14%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8786 (35, 38,  8, 20) | 0.898 | -20.0 | 83.91 | 66.33 |-------------------------------------------|
0             Validation | 0.8810 (35, 38,  7, 20) | 0.903 | -20.0 | 83.78 | 65.84 |######################################| ^ (3.0%, 4.12, 1.4, 8%) 
0 >>  5/20 <<   Training | 0.8766 (35, 38,  8, 19) | 0.920 | -20.0 | 84.06 | 66.30 |-------------------------------------------|
0             Validation | 0.8793 (36, 37,  8, 19) | 0.926 | -20.0 | 83.92 | 65.76 |#####################################| ^ (3.3%, 3.91, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.8746 (37, 37,  7, 19) | 0.977 | -20.0 | 83.86 | 66.41 |--------------------------------------------|
0             Validation | 0.8772 (37, 36,  7, 19) | 0.983 | -20.0 | 83.72 | 65.88 |######################################| ^ (3.2%, 3.81, 1.3, 11%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8597 (38, 39,  7, 16) | 0.989 | -20.0 | 85.16 | 66.51 |---------------------------------------------|
0             Validation | 0.8638 (39, 38,  7, 16) | 0.996 | -20.0 | 84.95 | 65.86 |######################################| ^ (3.9%, 4.19, 1.9, 1%) 
0 >>  8/20 <<   Training | 0.8593 (39, 38,  6, 16) | 1.025 | -20.0 | 85.19 | 66.60 |----------------------------------------------|
0             Validation | 0.8636 (40, 38,  6, 16) | 1.031 | -20.0 | 84.98 | 65.94 |#######################################| ^ (4.0%, 4.11, 1.7, 2%) 
0 >>  9/20 <<   Training | 0.8591 (39, 39,  6, 16) | 1.021 | -20.0 | 85.21 | 66.57 |---------------------------------------------|
0             Validation | 0.8635 (40, 38,  6, 16) | 1.028 | -20.0 | 85.00 | 65.91 |#######################################| ^ (4.0%, 4.11, 1.4, 7%) 
0 >> 10/20 <<   Training | 0.8592 (40, 38,  7, 16) | 1.038 | -20.0 | 85.23 | 66.56 |---------------------------------------------|
0             Validation | 0.8637 (40, 38,  6, 15) | 1.044 | -20.0 | 85.02 | 65.88 |######################################| ^ (4.1%, 4.06, 1.6, 3%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.8585 (39, 39,  6, 16) | 1.002 | -20.0 | 85.25 | 66.65 |----------------------------------------------|
0             Validation | 0.8630 (39, 38,  6, 16) | 1.008 | -20.0 | 85.04 | 65.96 |#######################################| ^ (4.1%, 4.06, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.8587 (39, 38,  6, 16) | 1.031 | -20.0 | 85.25 | 66.69 |----------------------------------------------|
0             Validation | 0.8632 (40, 38,  6, 16) | 1.037 | -20.0 | 85.04 | 65.99 |#######################################| ^ (4.2%, 4.04, 1.4, 10%) 
0 >> 13/20 <<   Training | 0.8586 (39, 39,  6, 16) | 1.003 | -20.0 | 85.26 | 66.56 |---------------------------------------------|
0             Validation | 0.8632 (39, 39,  6, 16) | 1.009 | -20.0 | 85.04 | 65.85 |######################################| ^ (4.3%, 4.22, 1.6, 3%) 
0 >> 14/20 <<   Training | 0.8584 (39, 39,  6, 16) | 0.997 | -20.0 | 85.26 | 66.62 |----------------------------------------------|
0             Validation | 0.8629 (39, 38,  6, 16) | 1.003 | -20.0 | 85.04 | 65.93 |#######################################| ^ (4.2%, 4.20, 1.2, 22%) 
0 >> 15/20 <<   Training | 0.8584 (38, 39,  6, 16) | 0.986 | -20.0 | 85.26 | 66.63 |----------------------------------------------|
0             Validation | 0.8630 (39, 39,  6, 16) | 0.992 | -20.0 | 85.05 | 65.94 |#######################################| ^ (4.2%, 4.26, 1.3, 11%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.8584 (38, 39,  6, 16) | 0.986 | -20.0 | 85.27 | 66.68 |----------------------------------------------|
0             Validation | 0.8629 (39, 39,  6, 16) | 0.992 | -20.0 | 85.05 | 65.98 |#######################################| ^ (4.2%, 4.19, 1.3, 13%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.8582 (39, 39,  6, 16) | 0.998 | -20.0 | 85.27 | 66.65 |----------------------------------------------|
0             Validation | 0.8628 (39, 38,  6, 16) | 1.004 | -20.0 | 85.05 | 65.96 |#######################################| ^ (4.2%, 4.22, 1.0, 34%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.8583 (39, 38,  6, 16) | 1.005 | -20.0 | 85.27 | 66.68 |----------------------------------------------|
0             Validation | 0.8628 (39, 38,  6, 16) | 1.011 | -20.0 | 85.05 | 65.98 |#######################################| ^ (4.2%, 4.22, 1.4, 8%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.8582 (39, 39,  6, 16) | 1.000 | -20.0 | 85.27 | 66.64 |----------------------------------------------|
0             Validation | 0.8629 (39, 38,  6, 16) | 1.006 | -20.0 | 85.05 | 65.94 |#######################################| ^ (4.2%, 4.26, 1.2, 20%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.8582 (39, 39,  6, 16) | 1.000 | -20.0 | 85.27 | 66.65 |----------------------------------------------|
0             Validation | 0.8629 (39, 38,  6, 16) | 1.006 | -20.0 | 85.06 | 65.94 |#######################################| ^ (4.2%, 4.26, 1.2, 17%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8821 (37, 39,  7, 17) | 0.939 | -20.0 | 82.90 | 65.28 |--------------------------------|
0             Validation | 0.8844 (37, 39,  7, 17) | 0.945 | -20.0 | 82.72 | 64.88 |############################| ^ (2.7%, 3.23, 2.2, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8692 (39, 39,  6, 16) | 0.960 | -20.0 | 84.23 | 65.99 |---------------------------------------|
0             Validation | 0.8719 (39, 38,  6, 16) | 0.966 | -20.0 | 84.08 | 65.62 |####################################| ^ (2.3%, 5.64, 2.2, 0%) 
0 >>  3/20 <<   Training | 0.8661 (39, 39,  7, 15) | 1.010 | -20.0 | 84.58 | 65.77 |-------------------------------------|
0             Validation | 0.8697 (40, 39,  7, 15) | 1.017 | -20.0 | 84.41 | 65.28 |################################| ^ (3.1%, 7.18, 1.4, 7%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8614 (38, 39,  7, 16) | 0.970 | -20.0 | 84.96 | 66.38 |-------------------------------------------|
0             Validation | 0.8651 (39, 39,  7, 16) | 0.976 | -20.0 | 84.79 | 65.85 |######################################| ^ (3.3%, 7.02, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.8614 (39, 38,  6, 16) | 1.026 | -20.0 | 84.99 | 66.47 |--------------------------------------------|
0             Validation | 0.8650 (40, 38,  6, 16) | 1.032 | -20.0 | 84.80 | 65.92 |#######################################| ^ (3.4%, 5.04, 1.6, 3%) 
0 >>  6/20 <<   Training | 0.8604 (39, 38,  7, 16) | 1.020 | -20.0 | 85.11 | 66.49 |--------------------------------------------|
0             Validation | 0.8644 (40, 37,  7, 16) | 1.027 | -20.0 | 84.91 | 65.93 |#######################################| ^ (3.4%, 5.33, 1.2, 21%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8593 (38, 40,  7, 16) | 0.950 | -20.0 | 85.19 | 66.48 |--------------------------------------------|
0             Validation | 0.8634 (38, 40,  7, 16) | 0.956 | -20.0 | 85.00 | 65.89 |######################################| ^ (3.6%, 4.78, 1.6, 4%) 
0 >>  8/20 <<   Training | 0.8592 (40, 38,  6, 16) | 1.050 | -20.0 | 85.21 | 66.54 |---------------------------------------------|
0             Validation | 0.8633 (40, 38,  6, 16) | 1.056 | -20.0 | 85.01 | 65.92 |#######################################| ^ (3.7%, 4.54, 1.3, 13%) 
0 >>  9/20 <<   Training | 0.8588 (38, 39,  7, 16) | 0.995 | -20.0 | 85.24 | 66.59 |---------------------------------------------|
0             Validation | 0.8629 (39, 38,  7, 16) | 1.001 | -20.0 | 85.03 | 65.97 |#######################################| ^ (3.8%, 4.00, 1.3, 15%) 
0 >> 10/20 <<   Training | 0.8589 (39, 40,  6, 16) | 0.983 | -20.0 | 85.25 | 66.42 |--------------------------------------------|
0             Validation | 0.8635 (39, 39,  6, 15) | 0.989 | -20.0 | 85.04 | 65.77 |#####################################| ^ (4.0%, 3.93, 1.4, 8%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.8583 (39, 38,  6, 16) | 1.027 | -20.0 | 85.27 | 66.64 |----------------------------------------------|
0             Validation | 0.8626 (40, 38,  6, 16) | 1.033 | -20.0 | 85.06 | 66.01 |########################################| ^ (3.8%, 3.87, 1.4, 7%) 
0 >> 12/20 <<   Training | 0.8581 (39, 39,  6, 16) | 0.993 | -20.0 | 85.28 | 66.63 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 0.999 | -20.0 | 85.07 | 66.00 |#######################################| ^ (3.7%, 3.82, 1.6, 3%) 
0 >> 13/20 <<   Training | 0.8581 (38, 39,  6, 16) | 0.980 | -20.0 | 85.28 | 66.58 |---------------------------------------------|
0             Validation | 0.8624 (39, 39,  6, 16) | 0.986 | -20.0 | 85.07 | 65.94 |#######################################| ^ (3.9%, 3.85, 1.5, 5%) 
0 >> 14/20 <<   Training | 0.8581 (39, 38,  7, 16) | 1.013 | -20.0 | 85.28 | 66.64 |----------------------------------------------|
0             Validation | 0.8624 (39, 38,  7, 16) | 1.019 | -20.0 | 85.07 | 65.99 |#######################################| ^ (3.9%, 3.89, 1.5, 5%) 
0 >> 15/20 <<   Training | 0.8581 (38, 39,  6, 16) | 0.974 | -20.0 | 85.29 | 66.67 |----------------------------------------------|
0             Validation | 0.8623 (38, 39,  6, 16) | 0.980 | -20.0 | 85.07 | 66.03 |########################################| ^ (3.8%, 3.79, 1.9, 1%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.8579 (38, 39,  6, 16) | 0.983 | -20.0 | 85.29 | 66.61 |----------------------------------------------|
0             Validation | 0.8624 (39, 39,  6, 16) | 0.989 | -20.0 | 85.08 | 65.97 |#######################################| ^ (3.8%, 3.83, 1.7, 2%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.8579 (39, 39,  6, 16) | 0.998 | -20.0 | 85.30 | 66.62 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 1.004 | -20.0 | 85.08 | 65.98 |#######################################| ^ (3.9%, 3.88, 1.6, 2%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.8579 (39, 39,  6, 16) | 0.994 | -20.0 | 85.30 | 66.62 |----------------------------------------------|
0             Validation | 0.8623 (39, 39,  6, 16) | 1.000 | -20.0 | 85.08 | 65.97 |#######################################| ^ (3.9%, 3.87, 1.5, 6%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.8578 (39, 39,  6, 16) | 0.997 | -20.0 | 85.30 | 66.63 |----------------------------------------------|
0             Validation | 0.8623 (39, 38,  6, 16) | 1.003 | -20.0 | 85.08 | 65.98 |#######################################| ^ (3.9%, 3.88, 1.5, 4%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.8578 (39, 39,  6, 16) | 1.000 | -20.0 | 85.30 | 66.62 |----------------------------------------------|
0             Validation | 0.8623 (39, 38,  6, 16) | 1.006 | -20.0 | 85.08 | 65.97 |#######################################| ^ (3.9%, 3.88, 1.6, 3%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.8821 (37, 39,  7, 17) | 0.939 | 0.001 | 82.90 | 65.28 |--------------------------------|
0             Validation | 0.8844 (37, 39,  7, 17) | 0.945 | 0.001 | 82.72 | 64.88 |############################| ^ (2.7%, 3.23, 2.2, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
0 >>  2/20 <<   Training | 0.8686 (39, 38,  6, 16) | 0.979 | 0.001 | 84.30 | 66.00 |----------------------------------------|
0             Validation | 0.8712 (39, 38,  6, 16) | 0.985 | 0.001 | 84.15 | 65.64 |####################################| ^ (2.3%, 5.04, 3.3, 0%) 
0 >>  3/20 <<   Training | 0.8649 (39, 39,  7, 16) | 1.001 | 0.001 | 84.68 | 65.83 |--------------------------------------|
0             Validation | 0.8681 (39, 39,  7, 16) | 1.007 | 0.001 | 84.52 | 65.35 |#################################| ^ (3.0%, 7.83, 1.5, 5%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
0 >>  4/20 <<   Training | 0.8613 (38, 39,  7, 16) | 0.968 | 0.001 | 85.01 | 66.35 |-------------------------------------------|
0             Validation | 0.8649 (39, 39,  7, 16) | 0.975 | 0.002 | 84.84 | 65.82 |######################################| ^ (3.3%, 7.46, 1.9, 1%) 
0 >>  5/20 <<   Training | 0.8612 (39, 38,  6, 16) | 1.025 | 0.001 | 85.04 | 66.47 |--------------------------------------------|
0             Validation | 0.8648 (40, 38,  6, 16) | 1.031 | 0.001 | 84.86 | 65.93 |#######################################| ^ (3.3%, 5.52, 1.7, 2%) 
0 >>  6/20 <<   Training | 0.8602 (40, 38,  7, 16) | 1.026 | 0.001 | 85.15 | 66.48 |--------------------------------------------|
0             Validation | 0.8641 (40, 37,  7, 16) | 1.032 | 0.001 | 84.96 | 65.92 |#######################################| ^ (3.4%, 5.99, 1.2, 19%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
0 >>  7/20 <<   Training | 0.8587 (39, 39,  6, 16) | 0.999 | 0.001 | 85.25 | 66.54 |---------------------------------------------|
0             Validation | 0.8628 (39, 39,  6, 16) | 1.005 | 0.001 | 85.06 | 65.95 |#######################################| ^ (3.6%, 4.93, 1.6, 3%) 
0 >>  8/20 <<   Training | 0.8585 (39, 38,  6, 16) | 1.027 | 0.001 | 85.27 | 66.57 |---------------------------------------------|
0             Validation | 0.8626 (40, 38,  6, 16) | 1.033 | 0.001 | 85.07 | 65.97 |#######################################| ^ (3.7%, 4.82, 1.6, 4%) 
0 >>  9/20 <<   Training | 0.8582 (39, 39,  6, 16) | 1.003 | 0.001 | 85.29 | 66.56 |---------------------------------------------|
0             Validation | 0.8626 (39, 39,  6, 16) | 1.009 | 0.001 | 85.08 | 65.92 |#######################################| ^ (3.8%, 4.10, 1.2, 19%) 
0 >> 10/20 <<   Training | 0.8581 (39, 38,  6, 16) | 1.014 | 0.001 | 85.31 | 66.53 |---------------------------------------------|
0             Validation | 0.8627 (40, 38,  6, 16) | 1.020 | 0.001 | 85.09 | 65.88 |######################################| ^ (3.9%, 3.88, 1.6, 3%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
0 >> 11/20 <<   Training | 0.8582 (40, 38,  6, 16) | 1.055 | 0.001 | 85.33 | 66.62 |----------------------------------------------|
0             Validation | 0.8627 (41, 38,  6, 16) | 1.062 | 0.001 | 85.12 | 65.97 |#######################################| ^ (3.9%, 3.89, 1.6, 3%) 
0 >> 12/20 <<   Training | 0.8576 (39, 39,  6, 16) | 0.998 | 0.001 | 85.35 | 66.65 |----------------------------------------------|
0             Validation | 0.8620 (39, 39,  6, 16) | 1.004 | 0.001 | 85.13 | 66.02 |########################################| ^ (3.8%, 3.73, 1.5, 4%) 
0 >> 13/20 <<   Training | 0.8577 (38, 39,  7, 16) | 0.985 | 0.001 | 85.34 | 66.61 |----------------------------------------------|
0             Validation | 0.8621 (39, 39,  7, 16) | 0.991 | 0.001 | 85.13 | 65.97 |#######################################| ^ (3.9%, 3.81, 1.6, 4%) 
0 >> 14/20 <<   Training | 0.8576 (39, 38,  6, 16) | 1.024 | 0.001 | 85.35 | 66.65 |----------------------------------------------|
0             Validation | 0.8621 (40, 38,  6, 16) | 1.030 | 0.001 | 85.14 | 66.00 |#######################################| ^ (3.9%, 3.85, 1.3, 13%) 
0 >> 15/20 <<   Training | 0.8576 (38, 40,  6, 16) | 0.971 | 0.001 | 85.35 | 66.67 |----------------------------------------------|
0             Validation | 0.8620 (38, 39,  6, 16) | 0.977 | 0.001 | 85.13 | 66.03 |########################################| ^ (3.8%, 3.70, 1.3, 13%) 
Decay learning rate: 0.000625 -> 0.000313
0 >> 16/20 <<   Training | 0.8574 (39, 39,  6, 16) | 0.987 | 0.001 | 85.36 | 66.63 |----------------------------------------------|
0             Validation | 0.8620 (39, 39,  6, 16) | 0.993 | 0.001 | 85.14 | 65.98 |#######################################| ^ (3.9%, 3.77, 1.3, 11%) 
Decay learning rate: 0.000313 -> 0.000156
0 >> 17/20 <<   Training | 0.8573 (39, 39,  6, 16) | 0.995 | 0.001 | 85.36 | 66.65 |----------------------------------------------|
0             Validation | 0.8619 (39, 39,  6, 16) | 1.001 | 0.001 | 85.15 | 66.00 |#######################################| ^ (3.9%, 3.81, 1.4, 7%) 
Decay learning rate: 0.000156 -> 0.000078
0 >> 18/20 <<   Training | 0.8573 (39, 39,  6, 16) | 0.992 | 0.001 | 85.36 | 66.65 |----------------------------------------------|
0             Validation | 0.8619 (39, 39,  6, 16) | 0.998 | 0.001 | 85.15 | 65.99 |#######################################| ^ (3.9%, 3.79, 1.5, 6%) 
Decay learning rate: 0.000078 -> 0.000039
0 >> 19/20 <<   Training | 0.8573 (39, 39,  6, 16) | 0.997 | 0.001 | 85.36 | 66.65 |----------------------------------------------|
0             Validation | 0.8619 (39, 38,  6, 16) | 1.003 | 0.001 | 85.15 | 66.00 |########################################| ^ (3.9%, 3.81, 1.4, 7%) 
Decay learning rate: 0.000039 -> 0.000020
0 >> 20/20 <<   Training | 0.8573 (39, 39,  6, 16) | 1.000 | 0.001 | 85.36 | 66.65 |----------------------------------------------|
0             Validation | 0.8619 (39, 38,  6, 16) | 1.006 | 0.001 | 85.15 | 65.99 |#######################################| ^ (3.9%, 3.81, 1.4, 10%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
