0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9243 (36, 36,  6, 18) | 0.959 | 5.507 | 83.23 | 66.01 |----------------------------------------|
0             Validation | 0.9256 (36, 36,  6, 18) | 0.958 | 3.140 | 83.17 | 65.87 |######################################| ^ (0.9%, 6.83, 1.0, 42%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
0 >>  2/20 <<   Training | 0.9230 (38, 35,  6, 18) | 1.063 | 5.978 | 83.76 | 65.68 |------------------------------------|
0             Validation | 0.9242 (38, 35,  6, 18) | 1.061 | 3.493 | 83.78 | 65.51 |###################################| ^ (1.2%, 4.86, 1.9, 1%) 
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9243 (36, 36,  6, 18) | 0.959 | 5.507 | 83.23 | 66.01 |----------------------------------------|
0             Validation | 0.9256 (36, 36,  6, 18) | 0.958 | 3.140 | 83.17 | 65.87 |######################################| ^ (0.9%, 6.83, 1.0, 42%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
0 >>  2/20 <<   Training | 0.9230 (38, 35,  6, 18) | 1.063 | 5.978 | 83.76 | 65.68 |------------------------------------|
0             Validation | 0.9242 (38, 35,  6, 18) | 1.061 | 3.493 | 83.78 | 65.51 |###################################| ^ (1.2%, 4.86, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.9038 (36, 37,  6, 17) | 0.958 | 10.893 | 84.13 | 68.88 |--------------------------------------------------------------------|
0             Validation | 0.9072 (36, 37,  6, 17) | 0.946 | 7.615 | 84.19 | 68.49 |################################################################| ^ (2.1%, 4.79, 2.4, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
0 >>  4/20 <<   Training | 0.9101 (36, 37,  6, 17) | 0.977 | 2.383 | 84.50 | 66.98 |-------------------------------------------------|
0             Validation | 0.9135 (36, 37,  6, 17) | 0.958 | 2.625 | 84.43 | 66.62 |##############################################| ^ (2.1%, 5.63, 6.5, 0%) 
0 >>  5/20 <<   Training | 0.9178 (35, 37,  6, 19) | 0.948 | 6.406 | 84.32 | 65.96 |---------------------------------------|
0             Validation | 0.9214 (35, 37,  6, 19) | 0.940 | 3.844 | 84.24 | 65.43 |##################################| ^ (3.3%, 4.95, 2.4, 0%) 
0 >>  6/20 <<   Training | 0.9178 (38, 35,  6, 17) | 1.068 | 7.569 | 84.47 | 65.89 |--------------------------------------|
0             Validation | 0.9209 (38, 35,  6, 17) | 1.065 | 4.401 | 84.39 | 65.44 |##################################| ^ (2.8%, 5.41, 1.9, 1%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
0 >>  7/20 <<   Training | 0.9072 (35, 38,  6, 17) | 0.953 | 4.842 | 84.83 | 67.01 |--------------------------------------------------|
0             Validation | 0.9090 (35, 38,  6, 17) | 0.948 | 3.494 | 84.82 | 66.82 |################################################| ^ (1.3%, 5.51, 1.8, 1%) 
0 >>  8/20 <<   Training | 0.9147 (37, 35,  6, 18) | 1.033 | 2.198 | 84.50 | 66.44 |--------------------------------------------|
0             Validation | 0.9177 (37, 35,  6, 18) | 1.035 | 1.608 | 84.37 | 66.03 |########################################| ^ (2.5%, 4.64, 1.9, 1%) 
0 >>  9/20 <<   Training | 0.9096 (36, 38,  6, 16) | 0.970 | 2.537 | 85.09 | 66.28 |------------------------------------------|
0             Validation | 0.9113 (36, 38,  6, 16) | 0.966 | 2.002 | 85.05 | 66.06 |########################################| ^ (1.4%, 4.17, 1.9, 0%) 
0 >> 10/20 <<   Training | 0.9070 (36, 37,  6, 17) | 0.984 | 1.518 | 85.07 | 66.85 |------------------------------------------------|
0             Validation | 0.9099 (36, 37,  6, 17) | 0.980 | 1.828 | 85.00 | 66.47 |############################################| ^ (2.2%, 5.10, 2.7, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
0 >> 11/20 <<   Training | 0.9053 (37, 37,  6, 16) | 1.001 | 3.553 | 85.07 | 67.39 |-----------------------------------------------------|
0             Validation | 0.9084 (37, 37,  6, 16) | 0.995 | 2.221 | 84.91 | 66.99 |#################################################| ^ (2.3%, 7.01, 2.9, 0%) 
0 >> 12/20 <<   Training | 0.9111 (36, 37,  6, 17) | 0.990 | 2.329 | 85.02 | 66.07 |----------------------------------------|
0             Validation | 0.9143 (36, 37,  6, 17) | 0.983 | 1.114 | 84.94 | 65.54 |###################################| ^ (3.3%, 4.78, 2.0, 0%) 
0 >> 13/20 <<   Training | 0.9127 (36, 38,  6, 17) | 0.973 | 2.323 | 84.87 | 65.84 |--------------------------------------|
0             Validation | 0.9150 (36, 38,  6, 17) | 0.978 | 1.404 | 84.76 | 65.56 |###################################| ^ (1.8%, 4.84, 1.5, 4%) 
0 >> 14/20 <<   Training | 0.9070 (36, 37,  6, 17) | 0.978 | 2.545 | 84.93 | 67.06 |--------------------------------------------------|
0             Validation | 0.9099 (36, 37,  6, 17) | 0.980 | 2.182 | 84.80 | 66.69 |##############################################| ^ (2.2%, 6.82, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.9023 (36, 37,  6, 17) | 0.969 | 2.340 | 85.28 | 67.42 |------------------------------------------------------|
0             Validation | 0.9049 (36, 37,  6, 17) | 0.965 | 2.033 | 85.23 | 67.16 |###################################################| ^ (1.6%, 5.42, 2.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.9039 (37, 37,  6, 17) | 1.008 | 0.892 | 85.22 | 67.28 |----------------------------------------------------|
0             Validation | 0.9076 (37, 37,  6, 17) | 1.008 | 1.894 | 85.08 | 66.80 |################################################| ^ (2.8%, 5.60, 2.0, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.9051 (36, 37,  6, 17) | 0.992 | 1.187 | 85.17 | 67.09 |--------------------------------------------------|
0             Validation | 0.9085 (37, 37,  6, 17) | 0.996 | 1.428 | 85.09 | 66.58 |#############################################| ^ (3.0%, 5.90, 2.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.9046 (36, 37,  6, 17) | 1.005 | 0.969 | 85.16 | 67.21 |----------------------------------------------------|
0             Validation | 0.9092 (37, 37,  6, 17) | 0.996 | 1.482 | 85.01 | 66.54 |#############################################| ^ (3.9%, 5.19, 3.4, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.9053 (36, 37,  6, 17) | 1.001 | 1.269 | 85.13 | 67.15 |---------------------------------------------------|
0             Validation | 0.9090 (37, 37,  6, 17) | 0.999 | 1.273 | 85.02 | 66.64 |##############################################| ^ (3.0%, 5.77, 2.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.9048 (36, 37,  6, 17) | 0.999 | 1.086 | 85.16 | 67.19 |---------------------------------------------------|
0             Validation | 0.9075 (37, 37,  6, 17) | 1.007 | 1.025 | 85.05 | 66.84 |################################################| ^ (2.1%, 5.63, 3.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.9051 (36, 37,  6, 17) | 0.998 | 0.848 | 85.13 | 67.15 |---------------------------------------------------|
0             Validation | 0.9084 (36, 37,  6, 17) | 0.994 | 1.132 | 84.98 | 66.73 |###############################################| ^ (2.5%, 5.40, 2.7, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.9443 (21, 29,  7, 18) | 0.919 | 16.932 | 83.71 | 68.26 |--------------------------------|
0             Validation | 0.9458 (21, 29,  7, 18) | 0.911 | 10.679 | 83.63 | 67.78 |###########################| ^ (2.7%, 3.89, 2.4, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
0 >>  2/20 <<   Training | 0.9368 (22, 27,  7, 19) | 1.015 | 2.025 | 84.22 | 68.63 |------------------------------------|
0             Validation | 0.9387 (22, 27,  7, 20) | 1.006 | 1.741 | 84.13 | 68.13 |###############################| ^ (2.7%, 6.00, 1.7, 2%) 
0 >>  3/20 <<   Training | 0.9337 (22, 29,  7, 18) | 0.989 | 4.666 | 84.26 | 69.63 |----------------------------------------------|
0             Validation | 0.9349 (22, 29,  7, 18) | 0.980 | 3.940 | 84.20 | 69.26 |##########################################| ^ (2.0%, 4.36, 1.8, 1%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
0 >>  4/20 <<   Training | 0.9313 (21, 27,  7, 20) | 0.991 | 3.364 | 84.67 | 69.39 |-------------------------------------------|
0             Validation | 0.9332 (21, 27,  7, 20) | 0.983 | 2.539 | 84.58 | 68.93 |#######################################| ^ (2.4%, 5.06, 2.4, 0%) 
0 >>  5/20 <<   Training | 0.9293 (22, 29,  7, 18) | 0.992 | 1.976 | 84.75 | 69.29 |------------------------------------------|
0             Validation | 0.9314 (22, 29,  7, 18) | 0.983 | 2.081 | 84.67 | 68.75 |#####################################| ^ (2.8%, 5.09, 1.8, 1%) 
0 >>  6/20 <<   Training | 0.9288 (22, 27,  7, 18) | 1.063 | 3.427 | 84.78 | 69.54 |---------------------------------------------|
0             Validation | 0.9308 (22, 27,  7, 18) | 1.053 | 1.940 | 84.68 | 69.02 |########################################| ^ (2.7%, 3.59, 1.4, 8%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
0 >>  7/20 <<   Training | 0.9265 (21, 29,  7, 18) | 0.934 | 6.612 | 84.86 | 69.72 |-----------------------------------------------|
0             Validation | 0.9297 (21, 29,  7, 18) | 0.925 | 6.107 | 84.75 | 69.11 |#########################################| ^ (3.1%, 4.26, 1.5, 4%) 
0 >>  8/20 <<   Training | 0.9256 (22, 27,  7, 19) | 1.039 | 2.719 | 84.99 | 69.98 |-------------------------------------------------|
0             Validation | 0.9281 (22, 27,  7, 19) | 1.030 | 1.292 | 84.88 | 69.38 |###########################################| ^ (3.0%, 3.88, 1.9, 1%) 
0 >>  9/20 <<   Training | 0.9264 (20, 29,  7, 20) | 0.901 | 9.292 | 84.89 | 70.18 |---------------------------------------------------|
0             Validation | 0.9294 (20, 29,  7, 20) | 0.892 | 6.454 | 84.78 | 69.55 |#############################################| ^ (3.1%, 4.35, 1.9, 1%) 
0 >> 10/20 <<   Training | 0.9257 (22, 29,  7, 18) | 1.016 | 1.398 | 84.97 | 69.80 |-----------------------------------------------|
0             Validation | 0.9281 (22, 29,  7, 18) | 1.006 | 1.274 | 84.86 | 69.21 |##########################################| ^ (3.0%, 4.12, 1.3, 16%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
0 >> 11/20 <<   Training | 0.9244 (23, 28,  7, 18) | 1.071 | 3.949 | 85.02 | 70.04 |--------------------------------------------------|
0             Validation | 0.9272 (23, 28,  7, 18) | 1.061 | 2.632 | 84.90 | 69.41 |############################################| ^ (3.2%, 3.45, 1.1, 32%) 
0 >> 12/20 <<   Training | 0.9250 (23, 28,  7, 17) | 1.121 | 8.963 | 85.07 | 69.87 |------------------------------------------------|
0             Validation | 0.9286 (23, 28,  7, 17) | 1.111 | 4.854 | 84.94 | 69.13 |#########################################| ^ (3.7%, 4.59, 2.0, 0%) 
0 >> 13/20 <<   Training | 0.9258 (23, 29,  6, 17) | 1.061 | 3.722 | 85.06 | 70.33 |-----------------------------------------------------|
0             Validation | 0.9292 (23, 29,  6, 17) | 1.051 | 2.083 | 84.94 | 69.57 |#############################################| ^ (3.7%, 4.59, 1.7, 1%) 
0 >> 14/20 <<   Training | 0.9234 (22, 28,  7, 18) | 1.051 | 3.216 | 85.06 | 70.44 |------------------------------------------------------|
0             Validation | 0.9261 (22, 28,  7, 18) | 1.042 | 1.514 | 84.95 | 69.78 |###############################################| ^ (3.2%, 3.78, 1.4, 8%) 
0 >> 15/20 <<   Training | 0.9235 (21, 29,  7, 19) | 0.962 | 2.257 | 85.04 | 70.31 |-----------------------------------------------------|
0             Validation | 0.9269 (21, 29,  7, 19) | 0.954 | 2.636 | 84.92 | 69.61 |##############################################| ^ (3.5%, 4.12, 2.3, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.9214 (21, 29,  7, 18) | 0.987 | 0.977 | 85.16 | 70.41 |------------------------------------------------------|
0             Validation | 0.9250 (21, 29,  7, 18) | 0.978 | 1.604 | 85.03 | 69.65 |##############################################| ^ (3.8%, 4.56, 1.7, 2%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.9209 (22, 29,  7, 19) | 0.997 | 0.924 | 85.14 | 70.45 |------------------------------------------------------|
0             Validation | 0.9245 (22, 28,  7, 19) | 0.988 | 1.569 | 85.01 | 69.69 |##############################################| ^ (3.7%, 4.36, 1.7, 2%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.9209 (22, 29,  7, 18) | 0.994 | 1.172 | 85.15 | 70.49 |------------------------------------------------------|
0             Validation | 0.9245 (22, 29,  7, 18) | 0.985 | 1.662 | 85.03 | 69.73 |###############################################| ^ (3.7%, 4.42, 1.7, 2%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.9208 (22, 29,  7, 18) | 1.000 | 0.910 | 85.15 | 70.48 |------------------------------------------------------|
0             Validation | 0.9245 (22, 29,  7, 18) | 0.991 | 1.627 | 85.03 | 69.72 |###############################################| ^ (3.7%, 4.43, 1.7, 2%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.9208 (22, 29,  7, 18) | 1.000 | 0.885 | 85.15 | 70.48 |------------------------------------------------------|
0             Validation | 0.9245 (22, 29,  7, 18) | 0.990 | 1.623 | 85.03 | 69.72 |###############################################| ^ (3.7%, 4.43, 1.7, 2%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset0_epoch20_before_finetuning.pkl
Run Finetuning
0 >> 20/20 <<   Training | 0.9208 (22, 29,  7, 18) | 0.999 | 0.967 | 85.15 | 70.49 |------------------------------------------------------|
0             Validation | 0.9245 (22, 29,  7, 18) | 0.989 | 1.678 | 85.03 | 69.73 |###############################################| ^ (3.7%, 4.44, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_12_np2076_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
