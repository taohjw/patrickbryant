1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9066 (41, 37,  5, 15) | 1.125 | 0.005 | 83.28 | 64.41 |------------------------|
1             Validation | 0.9063 (41, 37,  5, 15) | 1.124 | 0.005 | 83.30 | 64.36 |#######################| ^ (0.7%, 3.62, 2.0, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
1 >>  2/20 <<   Training | 0.8883 (34, 41,  6, 16) | 0.840 | 0.000 | 84.95 | 65.61 |------------------------------------|
1             Validation | 0.8884 (34, 41,  6, 16) | 0.840 | 0.000 | 84.92 | 65.56 |###################################| ^ (0.6%, 3.47, 1.8, 1%) 
1 >>  3/20 <<   Training | 0.8889 (42, 33,  6, 16) | 1.209 | 0.001 | 85.39 | 65.99 |---------------------------------------|
1             Validation | 0.8894 (42, 33,  7, 16) | 1.208 | 0.001 | 85.37 | 65.80 |######################################| ^ (1.4%, 3.08, 2.1, 0%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
1 >>  4/20 <<   Training | 0.8804 (37, 37,  7, 17) | 0.968 | 0.001 | 85.75 | 66.32 |-------------------------------------------|
1             Validation | 0.8809 (37, 37,  7, 17) | 0.968 | 0.000 | 85.76 | 66.13 |#########################################| ^ (1.5%, 3.24, 1.5, 6%) 
1 >>  5/20 <<   Training | 0.8780 (36, 38,  7, 16) | 0.940 | 0.001 | 85.93 | 66.32 |-------------------------------------------|
1             Validation | 0.8788 (36, 38,  7, 16) | 0.939 | 0.001 | 85.91 | 66.09 |########################################| ^ (1.5%, 3.76, 2.6, 0%) 
1 >>  6/20 <<   Training | 0.8853 (35, 38,  7, 18) | 0.943 | 0.001 | 85.87 | 66.11 |-----------------------------------------|
1             Validation | 0.8860 (35, 38,  7, 18) | 0.942 | 0.000 | 85.86 | 65.88 |######################################| ^ (1.5%, 3.10, 1.4, 9%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
1 >>  7/20 <<   Training | 0.8757 (40, 38,  6, 14) | 1.060 | 0.002 | 86.26 | 66.30 |-------------------------------------------|
1             Validation | 0.8776 (40, 38,  6, 14) | 1.059 | 0.002 | 86.19 | 65.93 |#######################################| ^ (2.4%, 4.00, 1.5, 6%) 
1 >>  8/20 <<   Training | 0.8740 (40, 37,  6, 15) | 1.061 | 0.001 | 86.21 | 66.54 |---------------------------------------------|
1             Validation | 0.8755 (40, 37,  6, 15) | 1.059 | 0.000 | 86.18 | 66.19 |#########################################| ^ (2.2%, 3.77, 1.4, 8%) 
1 >>  9/20 <<   Training | 0.8743 (36, 39,  6, 17) | 0.902 | 0.001 | 86.36 | 66.74 |-----------------------------------------------|
1             Validation | 0.8766 (36, 39,  6, 16) | 0.901 | 0.001 | 86.27 | 66.34 |###########################################| ^ (2.5%, 4.46, 0.9, 58%) 
1 >> 10/20 <<   Training | 0.8749 (40, 38,  6, 14) | 1.046 | 0.000 | 86.27 | 66.22 |------------------------------------------|
1             Validation | 0.8773 (40, 38,  6, 14) | 1.044 | 0.000 | 86.24 | 65.74 |#####################################| ^ (3.0%, 4.09, 1.6, 3%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
1 >> 11/20 <<   Training | 0.8724 (40, 37,  6, 15) | 1.074 | 0.001 | 86.43 | 66.47 |--------------------------------------------|
1             Validation | 0.8747 (40, 37,  6, 15) | 1.073 | 0.001 | 86.36 | 66.01 |########################################| ^ (2.8%, 4.36, 1.4, 9%) 
1 >> 12/20 <<   Training | 0.8711 (37, 39,  6, 15) | 0.988 | 0.001 | 86.44 | 66.73 |-----------------------------------------------|
1             Validation | 0.8735 (37, 39,  6, 15) | 0.986 | 0.001 | 86.37 | 66.23 |##########################################| ^ (3.0%, 3.96, 2.0, 0%) 
1 >> 13/20 <<   Training | 0.8710 (39, 38,  6, 15) | 1.058 | 0.001 | 86.44 | 66.77 |-----------------------------------------------|
1             Validation | 0.8737 (39, 38,  6, 15) | 1.057 | 0.001 | 86.36 | 66.23 |##########################################| ^ (3.2%, 3.81, 1.5, 4%) 
1 >> 14/20 <<   Training | 0.8703 (37, 39,  6, 15) | 0.967 | 0.000 | 86.45 | 66.80 |-----------------------------------------------|
1             Validation | 0.8732 (37, 39,  6, 15) | 0.966 | 0.000 | 86.36 | 66.28 |##########################################| ^ (3.1%, 3.55, 1.0, 45%) 
1 >> 15/20 <<   Training | 0.8705 (38, 37,  7, 15) | 1.004 | 0.000 | 86.44 | 66.80 |------------------------------------------------|
1             Validation | 0.8736 (39, 37,  7, 15) | 1.003 | 0.000 | 86.37 | 66.28 |##########################################| ^ (3.1%, 4.18, 1.0, 34%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8689 (39, 37,  6, 15) | 1.046 | 0.001 | 86.56 | 66.97 |-------------------------------------------------|
1             Validation | 0.8723 (39, 37,  6, 15) | 1.045 | 0.001 | 86.46 | 66.40 |############################################| ^ (3.4%, 3.98, 1.2, 21%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8684 (38, 38,  6, 15) | 1.011 | 0.001 | 86.58 | 67.04 |--------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 1.010 | 0.000 | 86.48 | 66.47 |############################################| ^ (3.3%, 3.85, 1.8, 1%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8684 (38, 38,  6, 15) | 0.995 | 0.001 | 86.58 | 66.99 |-------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 0.994 | 0.001 | 86.48 | 66.41 |############################################| ^ (3.4%, 3.85, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8684 (38, 38,  6, 15) | 1.004 | 0.001 | 86.58 | 67.00 |-------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 1.002 | 0.001 | 86.48 | 66.42 |############################################| ^ (3.4%, 3.86, 1.8, 1%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8684 (38, 38,  6, 15) | 1.002 | 0.001 | 86.58 | 67.00 |--------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 1.001 | 0.001 | 86.48 | 66.43 |############################################| ^ (3.4%, 3.85, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
1 >> 20/20 <<   Training | 0.8681 (38, 38,  6, 15) | 1.002 | 0.873 | 86.60 | 67.00 |-------------------------------------------------|
1             Validation | 0.8716 (38, 38,  6, 15) | 1.001 | 0.952 | 86.54 | 66.43 |############################################| ^ (3.4%, 3.85, 1.6, 4%) 
Run Finetuning
1 >> 20/20 <<   Training | 0.8681 (38, 38,  6, 15) | 1.001 | 0.776 | 86.60 | 67.00 |--------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 1.000 | 0.873 | 86.54 | 66.43 |############################################| ^ (3.4%, 3.87, 1.6, 3%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20_finetune01.pkl
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
Change training batch size: 1024 -> 16384 (151 batches)
1 >> 20/20 <<   Training | 0.8681 (38, 38,  6, 15) | 1.002 | 0.873 | 86.60 | 67.00 |-------------------------------------------------|
1             Validation | 0.8716 (38, 38,  6, 15) | 1.001 | 0.952 | 86.54 | 66.43 |############################################| ^ (3.4%, 3.85, 1.6, 4%) 
Run Finetuning
1 >> 20/20 <<   Training | 0.8681 (38, 38,  6, 15) | 1.001 | 0.799 | 86.60 | 67.00 |--------------------------------------------------|
1             Validation | 0.8717 (38, 38,  6, 15) | 1.000 | 0.896 | 86.54 | 66.43 |############################################| ^ (3.4%, 3.88, 1.6, 3%) 
* ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20_finetune01.pkl
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9056 (39, 36,  5, 15) | 1.053 | 6.856 | 83.17 | 64.84 |----------------------------|
1             Validation | 0.9080 (40, 36,  5, 15) | 1.063 | 5.210 | 83.03 | 64.43 |########################| ^ (2.8%, 4.85, 2.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
1 >>  2/20 <<   Training | 0.8943 (38, 35,  6, 17) | 1.052 | 5.310 | 84.32 | 66.12 |-----------------------------------------|
1             Validation | 0.8972 (38, 35,  6, 17) | 1.062 | 2.797 | 84.14 | 65.52 |###################################| ^ (3.7%, 2.70, 2.6, 0%) 
1 >>  3/20 <<   Training | 0.8898 (39, 36,  5, 16) | 1.004 | 10.511 | 84.69 | 66.30 |-------------------------------------------|
1             Validation | 0.8937 (39, 36,  5, 15) | 1.013 | 7.298 | 84.51 | 65.69 |####################################| ^ (3.8%, 3.34, 2.5, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
1 >>  4/20 <<   Training | 0.8863 (38, 38,  5, 15) | 0.960 | 8.513 | 84.82 | 66.56 |---------------------------------------------|
1             Validation | 0.8904 (38, 38,  5, 15) | 0.969 | 6.174 | 84.65 | 65.89 |######################################| ^ (4.2%, 3.71, 2.2, 0%) 
1 >>  5/20 <<   Training | 0.8852 (40, 35,  6, 15) | 1.107 | 8.077 | 85.01 | 66.70 |----------------------------------------------|
1             Validation | 0.8901 (40, 35,  6, 15) | 1.118 | 4.575 | 84.79 | 65.94 |#######################################| ^ (4.6%, 4.16, 2.5, 0%) 
1 >>  6/20 <<   Training | 0.8865 (40, 35,  6, 15) | 1.107 | 8.432 | 85.24 | 66.49 |--------------------------------------------|
1             Validation | 0.8904 (40, 35,  6, 15) | 1.117 | 4.526 | 85.03 | 65.81 |######################################| ^ (4.2%, 3.80, 2.3, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
1 >>  7/20 <<   Training | 0.8801 (37, 38,  5, 15) | 0.970 | 1.756 | 85.41 | 66.79 |-----------------------------------------------|
1             Validation | 0.8849 (38, 38,  5, 15) | 0.980 | 1.584 | 85.21 | 65.99 |#######################################| ^ (4.8%, 4.46, 2.5, 0%) 
1 >>  8/20 <<   Training | 0.8817 (39, 38,  5, 14) | 1.045 | 3.416 | 85.55 | 66.54 |---------------------------------------------|
1             Validation | 0.8868 (39, 38,  5, 14) | 1.054 | 2.503 | 85.34 | 65.72 |#####################################| ^ (5.0%, 4.12, 2.4, 0%) 
1 >>  9/20 <<   Training | 0.8802 (37, 37,  6, 16) | 0.994 | 1.548 | 85.53 | 66.87 |------------------------------------------------|
1             Validation | 0.8855 (38, 37,  6, 16) | 1.004 | 0.950 | 85.31 | 65.95 |#######################################| ^ (5.5%, 4.35, 2.8, 0%) 
1 >> 10/20 <<   Training | 0.8792 (37, 39,  5, 15) | 0.961 | 5.748 | 85.50 | 67.07 |--------------------------------------------------|
1             Validation | 0.8849 (38, 38,  5, 15) | 0.970 | 4.240 | 85.30 | 66.15 |#########################################| ^ (5.4%, 4.80, 2.5, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
1 >> 11/20 <<   Training | 0.8767 (38, 38,  6, 14) | 0.979 | 1.841 | 85.77 | 67.14 |---------------------------------------------------|
1             Validation | 0.8826 (38, 38,  6, 14) | 0.988 | 2.250 | 85.53 | 66.24 |##########################################| ^ (5.3%, 4.03, 2.8, 0%) 
1 >> 12/20 <<   Training | 0.8775 (39, 38,  5, 15) | 1.033 | 1.769 | 85.68 | 66.85 |------------------------------------------------|
1             Validation | 0.8840 (39, 37,  5, 14) | 1.043 | 1.454 | 85.43 | 65.89 |######################################| ^ (5.8%, 4.76, 2.3, 0%) 
1 >> 13/20 <<   Training | 0.8768 (38, 37,  5, 15) | 1.040 | 2.507 | 85.73 | 67.06 |--------------------------------------------------|
1             Validation | 0.8831 (39, 37,  5, 15) | 1.050 | 1.820 | 85.51 | 66.07 |########################################| ^ (5.8%, 5.23, 3.6, 0%) 
1 >> 14/20 <<   Training | 0.8765 (37, 38,  6, 16) | 0.975 | 1.854 | 85.75 | 67.26 |----------------------------------------------------|
1             Validation | 0.8825 (37, 38,  6, 15) | 0.984 | 1.113 | 85.51 | 66.26 |##########################################| ^ (5.8%, 4.31, 2.6, 0%) 
1 >> 15/20 <<   Training | 0.8781 (35, 41,  5, 15) | 0.895 | 12.352 | 85.79 | 66.94 |-------------------------------------------------|
1             Validation | 0.8838 (35, 40,  5, 15) | 0.903 | 5.218 | 85.56 | 65.97 |#######################################| ^ (5.7%, 4.70, 3.1, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.8743 (37, 38,  6, 15) | 0.991 | 1.216 | 85.89 | 67.32 |-----------------------------------------------------|
1             Validation | 0.8818 (38, 37,  6, 15) | 1.000 | 1.465 | 85.63 | 66.19 |#########################################| ^ (6.5%, 4.80, 3.1, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.8739 (38, 38,  6, 15) | 1.004 | 1.280 | 85.91 | 67.38 |-----------------------------------------------------|
1             Validation | 0.8815 (38, 37,  5, 15) | 1.013 | 1.438 | 85.65 | 66.22 |##########################################| ^ (6.7%, 4.69, 3.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.8738 (38, 38,  6, 15) | 0.997 | 1.357 | 85.91 | 67.39 |-----------------------------------------------------|
1             Validation | 0.8815 (38, 38,  5, 15) | 1.006 | 1.370 | 85.65 | 66.23 |##########################################| ^ (6.7%, 4.67, 3.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.8738 (38, 38,  6, 15) | 1.006 | 1.372 | 85.91 | 67.38 |-----------------------------------------------------|
1             Validation | 0.8815 (38, 37,  5, 15) | 1.015 | 1.335 | 85.65 | 66.22 |##########################################| ^ (6.7%, 4.68, 3.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.8738 (38, 38,  6, 15) | 1.004 | 1.345 | 85.91 | 67.38 |-----------------------------------------------------|
1             Validation | 0.8815 (38, 37,  5, 15) | 1.013 | 1.322 | 85.65 | 66.22 |##########################################| ^ (6.7%, 4.68, 3.2, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.8738 (38, 38,  6, 15) | 1.000 | 1.393 | 85.91 | 67.38 |-----------------------------------------------------|
1             Validation | 0.8815 (38, 37,  5, 15) | 1.009 | 1.484 | 85.65 | 66.22 |##########################################| ^ (6.7%, 4.70, 3.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | rchi2 | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9434 (22, 27,  7, 20) | 1.036 | 3.479 | 83.49 | 67.85 |----------------------------|
1             Validation | 0.9462 (22, 27,  7, 20) | 1.046 | 2.406 | 83.41 | 67.52 |#########################| ^ (2.0%, 4.41, 3.1, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (1269 batches)
1 >>  2/20 <<   Training | 0.9386 (22, 27,  7, 20) | 1.002 | 4.079 | 84.03 | 68.14 |-------------------------------|
1             Validation | 0.9419 (22, 27,  7, 20) | 1.012 | 2.906 | 83.94 | 67.86 |############################| ^ (1.5%, 4.05, 2.5, 0%) 
1 >>  3/20 <<   Training | 0.9312 (23, 27,  7, 19) | 1.094 | 8.981 | 84.53 | 69.46 |--------------------------------------------|
1             Validation | 0.9351 (23, 27,  7, 19) | 1.104 | 4.722 | 84.44 | 69.01 |########################################| ^ (2.4%, 3.47, 2.2, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (634 batches)
1 >>  4/20 <<   Training | 0.9261 (22, 29,  7, 18) | 0.983 | 4.187 | 84.81 | 70.01 |--------------------------------------------------|
1             Validation | 0.9307 (22, 28,  7, 18) | 0.992 | 3.378 | 84.74 | 69.47 |############################################| ^ (2.8%, 4.73, 2.5, 0%) 
1 >>  5/20 <<   Training | 0.9261 (21, 29,  7, 18) | 0.953 | 4.709 | 84.82 | 69.85 |------------------------------------------------|
1             Validation | 0.9306 (21, 29,  7, 18) | 0.962 | 2.741 | 84.72 | 69.25 |##########################################| ^ (3.0%, 4.27, 2.6, 0%) 
1 >>  6/20 <<   Training | 0.9253 (21, 28,  7, 20) | 0.999 | 3.395 | 84.99 | 70.63 |--------------------------------------------------------|
1             Validation | 0.9297 (21, 28,  7, 19) | 1.008 | 1.518 | 84.91 | 70.04 |##################################################| ^ (2.9%, 4.58, 2.3, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (317 batches)
1 >>  7/20 <<   Training | 0.9238 (22, 29,  7, 17) | 0.993 | 3.538 | 85.00 | 69.79 |-----------------------------------------------|
1             Validation | 0.9299 (22, 29,  7, 17) | 1.002 | 3.059 | 84.89 | 69.01 |########################################| ^ (3.9%, 4.43, 2.6, 0%) 
1 >>  8/20 <<   Training | 0.9228 (22, 29,  7, 17) | 1.022 | 3.068 | 85.06 | 70.56 |-------------------------------------------------------|
1             Validation | 0.9285 (23, 29,  7, 17) | 1.031 | 2.658 | 84.96 | 69.86 |################################################| ^ (3.4%, 4.77, 2.7, 0%) 
1 >>  9/20 <<   Training | 0.9228 (23, 29,  7, 17) | 1.087 | 5.326 | 85.07 | 70.39 |-----------------------------------------------------|
1             Validation | 0.9296 (23, 28,  6, 17) | 1.097 | 4.323 | 84.96 | 69.55 |#############################################| ^ (4.2%, 4.21, 2.2, 0%) 
1 >> 10/20 <<   Training | 0.9272 (21, 27,  7, 21) | 0.948 | 4.661 | 84.91 | 70.33 |-----------------------------------------------------|
1             Validation | 0.9336 (21, 27,  7, 21) | 0.957 | 4.169 | 84.80 | 69.49 |############################################| ^ (4.1%, 4.03, 2.7, 0%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (158 batches)
1 >> 11/20 <<   Training | 0.9219 (20, 29,  7, 19) | 0.924 | 6.272 | 85.12 | 70.48 |------------------------------------------------------|
1             Validation | 0.9278 (21, 29,  7, 19) | 0.933 | 3.071 | 84.99 | 69.61 |##############################################| ^ (4.3%, 4.08, 2.3, 0%) 
1 >> 12/20 <<   Training | 0.9202 (22, 28,  7, 18) | 1.052 | 3.152 | 85.16 | 70.47 |------------------------------------------------------|
1             Validation | 0.9271 (23, 28,  7, 18) | 1.061 | 1.733 | 85.04 | 69.49 |############################################| ^ (4.8%, 5.01, 2.3, 0%) 
1 >> 13/20 <<   Training | 0.9208 (23, 28,  7, 18) | 1.075 | 5.462 | 85.19 | 70.94 |-----------------------------------------------------------|
1             Validation | 0.9279 (23, 27,  7, 18) | 1.084 | 2.991 | 85.06 | 69.91 |#################################################| ^ (4.9%, 5.00, 2.2, 0%) 
1 >> 14/20 <<   Training | 0.9199 (22, 28,  7, 18) | 1.044 | 3.930 | 85.14 | 70.76 |---------------------------------------------------------|
1             Validation | 0.9272 (22, 28,  7, 18) | 1.053 | 1.870 | 85.00 | 69.67 |##############################################| ^ (5.3%, 4.53, 2.4, 0%) 
1 >> 15/20 <<   Training | 0.9192 (21, 29,  7, 18) | 0.957 | 4.414 | 85.18 | 70.77 |---------------------------------------------------------|
1             Validation | 0.9270 (22, 29,  7, 18) | 0.965 | 4.005 | 85.05 | 69.71 |###############################################| ^ (5.1%, 4.84, 2.0, 0%) 
Decay learning rate: 0.010000 -> 0.002500
1 >> 16/20 <<   Training | 0.9178 (21, 29,  7, 18) | 0.986 | 1.183 | 85.24 | 70.95 |-----------------------------------------------------------|
1             Validation | 0.9256 (22, 29,  7, 18) | 0.995 | 1.379 | 85.11 | 69.81 |################################################| ^ (5.5%, 4.45, 2.3, 0%) 
Decay learning rate: 0.002500 -> 0.000625
1 >> 17/20 <<   Training | 0.9174 (21, 29,  7, 18) | 0.991 | 1.327 | 85.23 | 70.91 |-----------------------------------------------------------|
1             Validation | 0.9254 (22, 28,  7, 18) | 1.000 | 1.735 | 85.10 | 69.75 |###############################################| ^ (5.5%, 4.61, 2.3, 0%) 
Decay learning rate: 0.000625 -> 0.000156
1 >> 18/20 <<   Training | 0.9174 (22, 29,  7, 18) | 0.997 | 1.193 | 85.24 | 70.90 |----------------------------------------------------------|
1             Validation | 0.9255 (22, 28,  7, 18) | 1.006 | 1.593 | 85.10 | 69.73 |###############################################| ^ (5.6%, 4.67, 2.3, 0%) 
Decay learning rate: 0.000156 -> 0.000039
1 >> 19/20 <<   Training | 0.9174 (22, 29,  7, 18) | 1.000 | 1.247 | 85.24 | 70.89 |----------------------------------------------------------|
1             Validation | 0.9255 (22, 28,  7, 18) | 1.009 | 1.486 | 85.10 | 69.72 |###############################################| ^ (5.6%, 4.67, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
1 >> 20/20 <<   Training | 0.9174 (22, 29,  7, 18) | 1.000 | 1.175 | 85.24 | 70.89 |----------------------------------------------------------|
1             Validation | 0.9255 (22, 28,  7, 18) | 1.009 | 1.541 | 85.10 | 69.72 |###############################################| ^ (5.6%, 4.68, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20_before_finetuning.pkl
Run Finetuning
1 >> 20/20 <<   Training | 0.9174 (22, 29,  7, 18) | 1.000 | 1.217 | 85.24 | 70.89 |----------------------------------------------------------|
1             Validation | 0.9255 (22, 28,  7, 18) | 1.009 | 1.437 | 85.10 | 69.71 |###############################################| ^ (5.6%, 4.69, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2714_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
