0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.7171 (21, 24, 11, 35) | 0.684 | 0.820 | 87.01 | 87.15 |-------------------------------|
0             Validation | 0.7219 (22, 23, 11, 34) | 0.694 | 0.834 | 86.89 | 87.03 |##############################| ^ (0.4%, 16.26, 2.5, 0%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1096 batches)
0 >>  2/20 <<   Training | 0.6758 (24, 26, 12, 29) | 0.782 | 0.937 | 87.85 | 88.46 |--------------------------------------------|
0             Validation | 0.6818 (24, 25, 12, 28) | 0.752 | 0.904 | 87.62 | 88.34 |###########################################| ^ (0.4%, 15.89, 2.4, 0%) 
0 >>  3/20 <<   Training | 0.6760 (25, 24, 12, 29) | 0.762 | 0.916 | 87.80 | 88.56 |---------------------------------------------|
0             Validation | 0.6830 (26, 24, 12, 29) | 0.760 | 0.912 | 87.42 | 88.41 |############################################| ^ (0.4%, 16.11, 1.8, 1%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (548 batches)
0 >>  4/20 <<   Training | 0.6665 (24, 26, 12, 29) | 0.815 | 0.974 | 88.20 | 88.79 |-----------------------------------------------|
0             Validation | 0.6741 (25, 25, 12, 28) | 0.783 | 0.940 | 87.85 | 88.62 |##############################################| ^ (0.5%, 15.59, 2.1, 0%) 
0 >>  5/20 <<   Training | 0.6651 (24, 26, 13, 28) | 0.833 | 0.996 | 88.25 | 88.83 |------------------------------------------------|
0             Validation | 0.6728 (25, 25, 12, 27) | 0.786 | 0.943 | 87.94 | 88.65 |##############################################| ^ (0.5%, 15.46, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6654 (25, 25, 13, 28) | 0.837 | 1.002 | 88.30 | 88.80 |------------------------------------------------|
0             Validation | 0.6739 (26, 25, 12, 27) | 0.790 | 0.946 | 87.92 | 88.59 |#############################################| ^ (0.6%, 13.44, 2.6, 0%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (274 batches)
0 >>  7/20 <<   Training | 0.6627 (22, 25, 12, 31) | 0.832 | 0.994 | 88.62 | 88.89 |------------------------------------------------|
0             Validation | 0.6712 (24, 25, 12, 30) | 0.787 | 0.942 | 88.18 | 88.68 |##############################################| ^ (0.6%, 15.09, 2.6, 0%) 
0 >>  8/20 <<   Training | 0.6618 (22, 25, 12, 32) | 0.833 | 0.996 | 88.80 | 88.97 |-------------------------------------------------|
0             Validation | 0.6705 (23, 24, 12, 31) | 0.795 | 0.953 | 88.30 | 88.73 |###############################################| ^ (0.6%, 15.08, 1.9, 1%) 
0 >>  9/20 <<   Training | 0.6611 (25, 27, 12, 27) | 0.843 | 1.006 | 88.57 | 88.99 |-------------------------------------------------|
0             Validation | 0.6712 (26, 26, 12, 26) | 0.809 | 0.971 | 88.07 | 88.73 |###############################################| ^ (0.7%, 14.67, 2.3, 0%) 
0 >> 10/20 <<   Training | 0.6626 (23, 28, 13, 27) | 0.827 | 0.990 | 88.45 | 88.93 |-------------------------------------------------|
0             Validation | 0.6718 (24, 28, 12, 27) | 0.794 | 0.952 | 87.96 | 88.69 |##############################################| ^ (0.6%, 13.81, 2.0, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (137 batches)
0 >> 11/20 <<   Training | 0.6614 (23, 29, 12, 26) | 0.853 | 1.020 | 88.44 | 89.00 |--------------------------------------------------|
0             Validation | 0.6719 (25, 28, 12, 26) | 0.820 | 0.980 | 87.89 | 88.72 |###############################################| ^ (0.7%, 14.64, 1.9, 1%) 
0 >> 12/20 <<   Training | 0.6586 (24, 26, 13, 28) | 0.847 | 1.011 | 88.64 | 89.05 |--------------------------------------------------|
0             Validation | 0.6693 (25, 25, 12, 27) | 0.811 | 0.970 | 88.02 | 88.76 |###############################################| ^ (0.7%, 13.38, 1.8, 1%) 
0 >> 13/20 <<   Training | 0.6600 (26, 26, 12, 26) | 0.853 | 1.018 | 88.60 | 89.00 |-------------------------------------------------|
0             Validation | 0.6719 (27, 26, 12, 26) | 0.809 | 0.967 | 88.01 | 88.69 |##############################################| ^ (0.8%, 14.46, 1.8, 1%) 
0 >> 14/20 <<   Training | 0.6587 (24, 28, 13, 26) | 0.858 | 1.026 | 88.67 | 89.08 |--------------------------------------------------|
0             Validation | 0.6703 (26, 27, 12, 25) | 0.823 | 0.985 | 88.05 | 88.79 |###############################################| ^ (0.8%, 16.12, 2.7, 0%) 
0 >> 15/20 <<   Training | 0.6599 (23, 23, 13, 31) | 0.843 | 1.008 | 88.78 | 89.05 |--------------------------------------------------|
0             Validation | 0.6719 (24, 23, 12, 31) | 0.816 | 0.979 | 88.14 | 88.70 |###############################################| ^ (0.9%, 13.60, 2.6, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6553 (23, 27, 13, 28) | 0.856 | 1.024 | 88.88 | 89.15 |---------------------------------------------------|
0             Validation | 0.6669 (24, 27, 12, 27) | 0.820 | 0.981 | 88.23 | 88.83 |################################################| ^ (0.8%, 14.07, 2.2, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6543 (23, 26, 12, 29) | 0.856 | 1.023 | 88.94 | 89.17 |---------------------------------------------------|
0             Validation | 0.6661 (25, 26, 12, 28) | 0.824 | 0.988 | 88.30 | 88.84 |################################################| ^ (0.8%, 13.78, 2.5, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6542 (23, 26, 12, 28) | 0.859 | 1.026 | 88.92 | 89.17 |---------------------------------------------------|
0             Validation | 0.6662 (25, 26, 12, 28) | 0.828 | 0.992 | 88.27 | 88.84 |################################################| ^ (0.9%, 13.85, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6542 (23, 26, 12, 28) | 0.859 | 1.026 | 88.92 | 89.17 |---------------------------------------------------|
0             Validation | 0.6662 (25, 26, 12, 28) | 0.827 | 0.991 | 88.27 | 88.84 |################################################| ^ (0.9%, 13.86, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6542 (23, 26, 12, 29) | 0.859 | 1.026 | 88.92 | 89.17 |---------------------------------------------------|
0             Validation | 0.6662 (25, 26, 12, 28) | 0.827 | 0.990 | 88.27 | 88.84 |################################################| ^ (0.9%, 13.85, 2.3, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2013_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6962 (25, 27, 12, 27) | 0.734 | 0.886 | 86.99 | 87.82 |--------------------------------------|
0             Validation | 0.6953 (24, 27, 12, 28) | 0.738 | 0.886 | 86.45 | 87.74 |#####################################| ^ (0.3%, 11.97, 1.7, 2%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1096 batches)
0 >>  2/20 <<   Training | 0.6775 (25, 24, 12, 30) | 0.780 | 0.939 | 87.88 | 88.69 |----------------------------------------------|
0             Validation | 0.6767 (24, 24, 12, 30) | 0.790 | 0.944 | 87.60 | 88.56 |#############################################| ^ (0.4%, 16.47, 2.3, 0%) 
0 >>  3/20 <<   Training | 0.6748 (24, 23, 12, 31) | 0.809 | 0.970 | 87.60 | 88.81 |------------------------------------------------|
0             Validation | 0.6746 (24, 24, 12, 31) | 0.809 | 0.968 | 87.22 | 88.73 |###############################################| ^ (0.3%, 13.06, 1.7, 2%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (548 batches)
0 >>  4/20 <<   Training | 0.6681 (23, 25, 12, 30) | 0.808 | 0.969 | 88.42 | 88.87 |------------------------------------------------|
0             Validation | 0.6684 (23, 26, 12, 30) | 0.832 | 0.995 | 87.99 | 88.77 |###############################################| ^ (0.3%, 15.18, 1.7, 2%) 
0 >>  5/20 <<   Training | 0.6712 (24, 24, 12, 30) | 0.801 | 0.961 | 88.13 | 88.74 |-----------------------------------------------|
0             Validation | 0.6723 (24, 24, 12, 30) | 0.804 | 0.959 | 87.60 | 88.62 |##############################################| ^ (0.3%, 17.30, 2.7, 0%) 
0 >>  6/20 <<   Training | 0.6686 (21, 25, 12, 32) | 0.814 | 0.977 | 88.59 | 88.84 |------------------------------------------------|
0             Validation | 0.6712 (22, 25, 12, 32) | 0.824 | 0.984 | 88.00 | 88.69 |##############################################| ^ (0.4%, 16.59, 2.0, 0%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (274 batches)
0 >>  7/20 <<   Training | 0.6616 (23, 26, 12, 29) | 0.821 | 0.984 | 88.59 | 89.02 |--------------------------------------------------|
0             Validation | 0.6637 (23, 26, 12, 29) | 0.828 | 0.989 | 87.96 | 88.88 |################################################| ^ (0.4%, 15.19, 2.1, 0%) 
0 >>  8/20 <<   Training | 0.6616 (24, 26, 12, 29) | 0.820 | 0.984 | 88.61 | 89.04 |--------------------------------------------------|
0             Validation | 0.6641 (24, 26, 12, 29) | 0.823 | 0.983 | 87.98 | 88.88 |################################################| ^ (0.5%, 17.95, 1.9, 1%) 
0 >>  9/20 <<   Training | 0.6616 (25, 26, 12, 27) | 0.835 | 1.004 | 88.67 | 89.03 |--------------------------------------------------|
0             Validation | 0.6635 (25, 26, 12, 27) | 0.854 | 1.021 | 88.10 | 88.88 |################################################| ^ (0.5%, 17.13, 2.0, 0%) 
0 >> 10/20 <<   Training | 0.6612 (25, 25, 12, 28) | 0.826 | 0.991 | 88.58 | 89.05 |--------------------------------------------------|
0             Validation | 0.6637 (25, 25, 12, 28) | 0.843 | 1.006 | 87.99 | 88.90 |################################################| ^ (0.4%, 16.21, 2.0, 0%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (137 batches)
0 >> 11/20 <<   Training | 0.6607 (22, 28, 12, 28) | 0.822 | 0.988 | 88.69 | 89.09 |--------------------------------------------------|
0             Validation | 0.6637 (22, 28, 12, 28) | 0.832 | 0.992 | 88.02 | 88.92 |#################################################| ^ (0.5%, 16.14, 2.2, 0%) 
0 >> 12/20 <<   Training | 0.6594 (24, 27, 13, 27) | 0.835 | 1.002 | 88.76 | 89.11 |---------------------------------------------------|
0             Validation | 0.6623 (24, 27, 12, 27) | 0.848 | 1.012 | 88.08 | 88.94 |#################################################| ^ (0.5%, 15.50, 2.3, 0%) 
0 >> 13/20 <<   Training | 0.6627 (21, 26, 12, 32) | 0.807 | 0.969 | 88.93 | 89.05 |--------------------------------------------------|
0             Validation | 0.6663 (21, 26, 12, 31) | 0.820 | 0.979 | 88.21 | 88.89 |################################################| ^ (0.4%, 16.37, 1.8, 1%) 
0 >> 14/20 <<   Training | 0.6596 (25, 27, 12, 26) | 0.847 | 1.015 | 88.76 | 89.15 |---------------------------------------------------|
0             Validation | 0.6622 (25, 27, 12, 26) | 0.864 | 1.031 | 88.10 | 88.97 |#################################################| ^ (0.5%, 17.12, 1.7, 2%) 
0 >> 15/20 <<   Training | 0.6580 (24, 25, 13, 29) | 0.840 | 1.006 | 88.94 | 89.15 |---------------------------------------------------|
0             Validation | 0.6610 (24, 25, 13, 29) | 0.861 | 1.027 | 88.28 | 88.99 |#################################################| ^ (0.5%, 16.78, 2.6, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6565 (23, 27, 12, 28) | 0.837 | 1.003 | 88.92 | 89.18 |---------------------------------------------------|
0             Validation | 0.6599 (23, 27, 12, 28) | 0.857 | 1.023 | 88.21 | 89.01 |##################################################| ^ (0.5%, 16.24, 2.0, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6558 (23, 26, 13, 28) | 0.850 | 1.019 | 88.93 | 89.21 |----------------------------------------------------|
0             Validation | 0.6593 (24, 26, 12, 28) | 0.869 | 1.038 | 88.23 | 89.03 |##################################################| ^ (0.5%, 16.62, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6557 (24, 26, 12, 29) | 0.851 | 1.019 | 88.94 | 89.21 |----------------------------------------------------|
0             Validation | 0.6592 (24, 26, 12, 29) | 0.871 | 1.039 | 88.24 | 89.04 |##################################################| ^ (0.5%, 16.59, 2.5, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6557 (24, 26, 12, 29) | 0.849 | 1.018 | 88.94 | 89.21 |----------------------------------------------------|
0             Validation | 0.6592 (24, 26, 12, 28) | 0.869 | 1.038 | 88.24 | 89.04 |##################################################| ^ (0.5%, 16.58, 2.8, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6557 (23, 26, 12, 29) | 0.848 | 1.018 | 88.94 | 89.21 |----------------------------------------------------|
0             Validation | 0.6592 (24, 26, 12, 28) | 0.870 | 1.038 | 88.24 | 89.04 |##################################################| ^ (0.5%, 16.57, 2.7, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2013_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6813 (30, 29, 11, 31) | 0.650 | 0.756 | 87.65 | 86.00 |-------------------|
0             Validation | 0.6878 (31, 29, 10, 31) | 0.655 | 0.762 | 87.22 | 85.68 |################| ^ (1.0%, 10.37, 2.3, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (996 batches)
0 >>  2/20 <<   Training | 0.6687 (28, 29, 11, 33) | 0.696 | 0.808 | 88.09 | 86.61 |--------------------------|
0             Validation | 0.6743 (29, 29, 10, 32) | 0.683 | 0.794 | 87.66 | 86.40 |#######################| ^ (0.7%, 13.22, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6686 (30, 27, 11, 33) | 0.687 | 0.796 | 88.12 | 86.49 |------------------------|
0             Validation | 0.6775 (31, 26, 11, 33) | 0.685 | 0.796 | 87.57 | 86.13 |#####################| ^ (1.0%, 12.21, 2.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (498 batches)
0 >>  4/20 <<   Training | 0.6573 (29, 31, 11, 30) | 0.726 | 0.841 | 88.43 | 87.03 |------------------------------|
0             Validation | 0.6660 (30, 31, 10, 30) | 0.710 | 0.823 | 87.83 | 86.72 |###########################| ^ (0.9%, 13.75, 1.8, 1%) 
0 >>  5/20 <<   Training | 0.6546 (26, 29, 11, 35) | 0.738 | 0.855 | 88.81 | 87.08 |------------------------------|
0             Validation | 0.6631 (27, 28, 11, 34) | 0.735 | 0.852 | 88.25 | 86.77 |###########################| ^ (0.9%, 14.63, 1.9, 1%) 
0 >>  6/20 <<   Training | 0.6560 (29, 33, 10, 29) | 0.742 | 0.860 | 88.71 | 87.14 |-------------------------------|
0             Validation | 0.6644 (30, 32, 10, 29) | 0.738 | 0.856 | 88.17 | 86.79 |###########################| ^ (0.9%, 13.65, 1.7, 2%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (249 batches)
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6822 (30, 29, 10, 32) | 0.642 | 0.746 | 87.59 | 86.00 |-------------------|
0             Validation | 0.6884 (31, 29, 10, 31) | 0.645 | 0.750 | 87.13 | 85.69 |################| ^ (0.9%, 11.25, 2.9, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (996 batches)
0 >>  2/20 <<   Training | 0.6681 (28, 30, 11, 32) | 0.699 | 0.811 | 87.96 | 86.68 |--------------------------|
0             Validation | 0.6738 (29, 30, 11, 31) | 0.694 | 0.806 | 87.55 | 86.46 |########################| ^ (0.7%, 13.44, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6694 (30, 28, 11, 32) | 0.682 | 0.791 | 88.07 | 86.47 |------------------------|
0             Validation | 0.6790 (31, 27, 11, 32) | 0.688 | 0.799 | 87.48 | 86.09 |####################| ^ (1.1%, 13.26, 3.4, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (498 batches)
0 >>  4/20 <<   Training | 0.6571 (28, 31, 11, 31) | 0.723 | 0.839 | 88.53 | 86.97 |-----------------------------|
0             Validation | 0.6656 (29, 31, 11, 30) | 0.704 | 0.816 | 87.93 | 86.69 |##########################| ^ (0.8%, 15.32, 2.0, 0%) 
0 >>  5/20 <<   Training | 0.6546 (27, 30, 11, 33) | 0.734 | 0.849 | 88.73 | 87.07 |------------------------------|
0             Validation | 0.6632 (28, 29, 11, 33) | 0.732 | 0.849 | 88.18 | 86.76 |###########################| ^ (0.9%, 15.70, 2.4, 0%) 
0 >>  6/20 <<   Training | 0.6531 (27, 32, 11, 31) | 0.738 | 0.856 | 88.85 | 87.17 |-------------------------------|
0             Validation | 0.6615 (28, 31, 11, 31) | 0.739 | 0.859 | 88.27 | 86.84 |############################| ^ (0.9%, 13.51, 3.0, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (249 batches)
0 >>  7/20 <<   Training | 0.6538 (27, 28, 11, 34) | 0.727 | 0.842 | 88.59 | 87.15 |-------------------------------|
0             Validation | 0.6644 (29, 28, 11, 33) | 0.733 | 0.850 | 87.86 | 86.80 |############################| ^ (0.9%, 13.53, 1.9, 1%) 
0 >>  8/20 <<   Training | 0.6490 (26, 31, 11, 33) | 0.744 | 0.862 | 88.97 | 87.31 |---------------------------------|
0             Validation | 0.6579 (27, 31, 11, 32) | 0.748 | 0.868 | 88.38 | 86.97 |#############################| ^ (0.9%, 14.13, 2.5, 0%) 
0 >>  9/20 <<   Training | 0.6498 (28, 28, 11, 34) | 0.744 | 0.862 | 89.01 | 87.25 |--------------------------------|
0             Validation | 0.6598 (29, 28, 11, 33) | 0.746 | 0.868 | 88.38 | 86.89 |############################| ^ (1.0%, 13.99, 2.8, 0%) 
0 >> 10/20 <<   Training | 0.6503 (27, 31, 11, 32) | 0.743 | 0.860 | 88.93 | 87.18 |-------------------------------|
0             Validation | 0.6608 (29, 30, 11, 31) | 0.725 | 0.842 | 88.28 | 86.78 |###########################| ^ (1.1%, 12.37, 1.6, 3%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (124 batches)
0 >> 11/20 <<   Training | 0.6471 (27, 31, 11, 32) | 0.752 | 0.871 | 89.17 | 87.38 |---------------------------------|
0             Validation | 0.6569 (28, 31, 11, 31) | 0.750 | 0.870 | 88.48 | 87.03 |##############################| ^ (1.0%, 13.90, 3.0, 0%) 
0 >> 12/20 <<   Training | 0.6472 (26, 30, 11, 33) | 0.749 | 0.868 | 89.12 | 87.36 |---------------------------------|
0             Validation | 0.6571 (28, 30, 11, 33) | 0.758 | 0.880 | 88.45 | 87.00 |#############################| ^ (1.0%, 14.11, 2.4, 0%) 
0 >> 13/20 <<   Training | 0.6474 (27, 31, 11, 32) | 0.750 | 0.868 | 89.01 | 87.35 |---------------------------------|
0             Validation | 0.6584 (28, 31, 11, 31) | 0.744 | 0.862 | 88.28 | 86.97 |#############################| ^ (1.0%, 13.64, 1.8, 1%) 
0 >> 14/20 <<   Training | 0.6473 (26, 31, 11, 33) | 0.750 | 0.869 | 89.23 | 87.41 |----------------------------------|
0             Validation | 0.6572 (27, 30, 11, 32) | 0.751 | 0.874 | 88.56 | 87.01 |##############################| ^ (1.1%, 13.14, 2.9, 0%) 
0 >> 15/20 <<   Training | 0.6471 (27, 32, 11, 32) | 0.752 | 0.871 | 89.22 | 87.34 |---------------------------------|
0             Validation | 0.6581 (28, 31, 11, 31) | 0.749 | 0.869 | 88.52 | 86.93 |#############################| ^ (1.1%, 14.74, 2.9, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.6442 (27, 30, 11, 33) | 0.761 | 0.882 | 89.19 | 87.48 |----------------------------------|
0             Validation | 0.6560 (29, 29, 11, 32) | 0.754 | 0.874 | 88.42 | 87.05 |##############################| ^ (1.1%, 13.86, 2.7, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.6436 (27, 30, 11, 33) | 0.765 | 0.885 | 89.23 | 87.49 |----------------------------------|
0             Validation | 0.6555 (28, 30, 11, 32) | 0.759 | 0.880 | 88.49 | 87.06 |##############################| ^ (1.2%, 14.10, 3.1, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.6435 (27, 30, 11, 33) | 0.766 | 0.886 | 89.23 | 87.49 |----------------------------------|
0             Validation | 0.6555 (28, 30, 11, 32) | 0.759 | 0.880 | 88.50 | 87.06 |##############################| ^ (1.2%, 14.05, 2.9, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.6435 (27, 30, 11, 33) | 0.764 | 0.884 | 89.24 | 87.50 |----------------------------------|
0             Validation | 0.6554 (28, 30, 11, 32) | 0.760 | 0.882 | 88.50 | 87.07 |##############################| ^ (1.2%, 14.03, 2.7, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.6435 (27, 30, 11, 33) | 0.764 | 0.885 | 89.24 | 87.50 |----------------------------------|
0             Validation | 0.6554 (28, 30, 11, 32) | 0.760 | 0.882 | 88.50 | 87.07 |##############################| ^ (1.2%, 14.03, 2.8, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_HCR_14_np2013_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6822 (30, 29, 10, 32) | 0.642 | 0.746 | 87.59 | 86.00 |-------------------|
0             Validation | 0.6884 (31, 29, 10, 31) | 0.645 | 0.750 | 87.13 | 85.69 |################| ^ (0.9%, 11.25, 2.9, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (996 batches)
0 >>  2/20 <<   Training | 0.6681 (28, 30, 11, 32) | 0.699 | 0.811 | 87.96 | 86.68 |--------------------------|
0             Validation | 0.6738 (29, 30, 11, 31) | 0.694 | 0.806 | 87.55 | 86.46 |########################| ^ (0.7%, 13.44, 1.9, 1%) 
0 >>  3/20 <<   Training | 0.6694 (30, 28, 11, 32) | 0.682 | 0.791 | 88.07 | 86.47 |------------------------|
0             Validation | 0.6790 (31, 27, 11, 32) | 0.688 | 0.799 | 87.48 | 86.09 |####################| ^ (1.1%, 13.26, 3.4, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (498 batches)
0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
