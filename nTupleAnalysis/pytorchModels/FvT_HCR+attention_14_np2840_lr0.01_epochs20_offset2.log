2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8885 (39, 38,  6, 17) | 1.051 |  -9.5 | 82.16 | 63.94 |-------------------|
2             Validation | 0.8883 (39, 38,  6, 17) | 1.044 |  -6.7 | 82.19 | 63.88 |##################| ^ (0.9%, 3.91, 1.5, 5%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
2 >>  2/20 <<   Training | 0.8760 (38, 39,  7, 16) | 0.926 |  19.1 | 83.30 | 65.96 |---------------------------------------|
2             Validation | 0.8762 (38, 39,  7, 17) | 0.920 |  17.3 | 83.30 | 65.94 |#######################################| ^ (0.7%, 2.59, 1.4, 7%) 
2 >>  3/20 <<   Training | 0.8766 (37, 39,  7, 17) | 0.927 |  13.9 | 83.22 | 66.01 |----------------------------------------|
2             Validation | 0.8775 (37, 39,  6, 17) | 0.922 |  12.7 | 83.19 | 65.85 |######################################| ^ (1.1%, 3.78, 1.4, 10%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
2 >>  4/20 <<   Training | 0.8692 (38, 38,  6, 17) | 0.993 |  10.0 | 83.67 | 66.19 |-----------------------------------------|
2             Validation | 0.8701 (38, 38,  6, 17) | 0.987 |  10.1 | 83.64 | 66.04 |########################################| ^ (1.1%, 2.94, 1.5, 6%) 
2 >>  5/20 <<   Training | 0.8686 (39, 38,  6, 16) | 1.022 |   9.1 | 83.77 | 66.19 |-----------------------------------------|
2             Validation | 0.8691 (39, 38,  6, 16) | 1.016 |   8.7 | 83.75 | 66.05 |########################################| ^ (1.0%, 3.01, 1.4, 9%) 
2 >>  6/20 <<   Training | 0.8702 (37, 40,  6, 16) | 0.931 | -12.2 | 83.52 | 66.15 |-----------------------------------------|
2             Validation | 0.8711 (37, 40,  6, 16) | 0.926 | -15.2 | 83.49 | 66.00 |########################################| ^ (1.0%, 3.25, 1.3, 14%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
2 >>  7/20 <<   Training | 0.8622 (38, 39,  7, 16) | 0.977 |  10.7 | 84.75 | 66.42 |--------------------------------------------|
2             Validation | 0.8635 (38, 39,  7, 16) | 0.970 | -17.4 | 84.69 | 66.17 |#########################################| ^ (1.5%, 3.54, 1.0, 35%) 
2 >>  8/20 <<   Training | 0.8620 (39, 38,  7, 16) | 1.001 | -15.5 | 84.80 | 66.44 |--------------------------------------------|
2             Validation | 0.8633 (38, 38,  7, 16) | 0.995 | -18.0 | 84.74 | 66.19 |#########################################| ^ (1.5%, 3.58, 1.5, 4%) 
2 >>  9/20 <<   Training | 0.8617 (38, 39,  7, 16) | 0.970 | -20.0 | 84.85 | 66.43 |--------------------------------------------|
2             Validation | 0.8632 (38, 39,  6, 16) | 0.964 | -20.0 | 84.79 | 66.18 |#########################################| ^ (1.5%, 3.65, 1.3, 12%) 
2 >> 10/20 <<   Training | 0.8611 (39, 39,  7, 16) | 1.002 | -20.0 | 84.91 | 66.44 |--------------------------------------------|
2             Validation | 0.8626 (39, 39,  6, 16) | 0.996 | -20.0 | 84.84 | 66.16 |#########################################| ^ (1.7%, 3.48, 1.8, 1%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
2 >> 11/20 <<   Training | 0.8609 (39, 38,  6, 16) | 1.024 | -20.0 | 84.93 | 66.48 |--------------------------------------------|
2             Validation | 0.8625 (39, 38,  6, 16) | 1.018 | -20.0 | 84.87 | 66.19 |#########################################| ^ (1.8%, 3.57, 1.7, 2%) 
2 >> 12/20 <<   Training | 0.8608 (39, 38,  6, 16) | 1.009 | -20.0 | 84.94 | 66.51 |---------------------------------------------|
2             Validation | 0.8624 (39, 38,  6, 16) | 1.003 | -20.0 | 84.88 | 66.22 |##########################################| ^ (1.8%, 3.60, 2.3, 0%) 
2 >> 13/20 <<   Training | 0.8607 (39, 39,  7, 16) | 0.993 | -20.0 | 84.96 | 66.48 |--------------------------------------------|
2             Validation | 0.8623 (39, 39,  6, 16) | 0.986 | -20.0 | 84.89 | 66.18 |#########################################| ^ (1.8%, 3.60, 1.8, 1%) 
2 >> 14/20 <<   Training | 0.8607 (38, 39,  6, 16) | 0.986 | -20.0 | 84.96 | 66.43 |--------------------------------------------|
2             Validation | 0.8623 (38, 39,  6, 16) | 0.979 | -20.0 | 84.90 | 66.15 |#########################################| ^ (1.8%, 3.55, 2.2, 0%) 
2 >> 15/20 <<   Training | 0.8606 (39, 39,  6, 16) | 1.008 | -20.0 | 84.98 | 66.48 |--------------------------------------------|
2             Validation | 0.8622 (39, 39,  6, 16) | 1.001 | -20.0 | 84.91 | 66.18 |#########################################| ^ (1.9%, 3.69, 2.3, 0%) 
Decay learning rate: 0.000625 -> 0.000313
2 >> 16/20 <<   Training | 0.8605 (38, 39,  6, 16) | 0.990 | -20.0 | 84.98 | 66.50 |---------------------------------------------|
2             Validation | 0.8621 (38, 39,  6, 16) | 0.983 | -20.0 | 84.92 | 66.20 |#########################################| ^ (1.9%, 3.61, 1.8, 1%) 
Decay learning rate: 0.000313 -> 0.000156
2 >> 17/20 <<   Training | 0.8604 (39, 39,  7, 16) | 1.006 | -20.0 | 84.99 | 66.49 |--------------------------------------------|
2             Validation | 0.8621 (39, 39,  6, 16) | 0.999 | -20.0 | 84.92 | 66.18 |#########################################| ^ (1.9%, 3.65, 2.1, 0%) 
Decay learning rate: 0.000156 -> 0.000078
2 >> 18/20 <<   Training | 0.8604 (39, 39,  6, 16) | 0.996 | -20.0 | 84.99 | 66.50 |---------------------------------------------|
2             Validation | 0.8621 (39, 39,  6, 16) | 0.989 | -20.0 | 84.92 | 66.20 |#########################################| ^ (1.9%, 3.66, 1.9, 1%) 
Decay learning rate: 0.000078 -> 0.000039
2 >> 19/20 <<   Training | 0.8604 (39, 39,  6, 16) | 1.002 | -20.0 | 84.99 | 66.50 |---------------------------------------------|
2             Validation | 0.8621 (39, 39,  6, 16) | 0.995 | -20.0 | 84.92 | 66.20 |#########################################| ^ (1.9%, 3.66, 2.2, 0%) 
Decay learning rate: 0.000039 -> 0.000020
2 >> 20/20 <<   Training | 0.8604 (39, 39,  6, 16) | 0.999 | -20.0 | 84.99 | 66.51 |---------------------------------------------|
2             Validation | 0.8621 (39, 39,  6, 16) | 0.992 | -20.0 | 84.92 | 66.20 |#########################################| ^ (1.9%, 3.66, 2.1, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8885 (39, 38,  6, 17) | 1.051 |  -9.5 | 82.16 | 63.94 |-------------------|
2             Validation | 0.8883 (39, 38,  6, 17) | 1.044 |  -6.7 | 82.19 | 63.88 |##################| ^ (0.9%, 3.91, 1.5, 5%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
2 >>  2/20 <<   Training | 0.8768 (38, 38,  7, 17) | 0.915 |  19.0 | 83.21 | 65.95 |---------------------------------------|
2             Validation | 0.8769 (38, 39,  7, 17) | 0.909 |  17.5 | 83.22 | 65.94 |#######################################| ^ (0.7%, 2.51, 1.3, 15%) 
2 >>  3/20 <<   Training | 0.8783 (37, 38,  7, 18) | 0.932 |  13.2 | 83.15 | 65.95 |---------------------------------------|
2             Validation | 0.8791 (37, 39,  7, 18) | 0.926 |  12.4 | 83.14 | 65.79 |#####################################| ^ (1.0%, 3.65, 1.1, 27%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
2 >>  4/20 <<   Training | 0.8702 (38, 38,  7, 17) | 0.964 |   9.8 | 83.62 | 66.19 |-----------------------------------------|
2             Validation | 0.8709 (37, 39,  7, 17) | 0.958 |  10.2 | 83.60 | 66.05 |########################################| ^ (1.0%, 2.84, 1.4, 8%) 
2 >>  5/20 <<   Training | 0.8694 (38, 38,  7, 17) | 0.992 |   9.1 | 83.69 | 66.20 |-----------------------------------------|
2             Validation | 0.8699 (38, 39,  7, 17) | 0.986 |   8.8 | 83.68 | 66.06 |########################################| ^ (0.9%, 2.97, 1.2, 17%) 
2 >>  6/20 <<   Training | 0.8716 (36, 40,  7, 17) | 0.894 | -12.6 | 83.46 | 66.21 |------------------------------------------|
2             Validation | 0.8724 (36, 40,  7, 17) | 0.888 | -14.2 | 83.44 | 66.06 |########################################| ^ (1.0%, 3.13, 1.9, 1%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
2 >>  7/20 <<   Training | 0.8626 (38, 40,  6, 16) | 0.970 |  10.4 | 84.73 | 66.35 |-------------------------------------------|
2             Validation | 0.8637 (38, 40,  6, 16) | 0.963 | -14.5 | 84.68 | 66.14 |#########################################| ^ (1.3%, 3.43, 1.5, 6%) 
2 >>  8/20 <<   Training | 0.8621 (39, 38,  7, 16) | 1.012 | -13.5 | 84.79 | 66.38 |-------------------------------------------|
2             Validation | 0.8632 (39, 39,  7, 16) | 1.005 | -20.0 | 84.74 | 66.16 |#########################################| ^ (1.4%, 3.41, 1.4, 7%) 
2 >>  9/20 <<   Training | 0.8622 (38, 39,  7, 17) | 0.960 | -20.0 | 84.84 | 66.45 |--------------------------------------------|
2             Validation | 0.8635 (38, 39,  7, 17) | 0.954 | -20.0 | 84.79 | 66.23 |##########################################| ^ (1.3%, 3.37, 1.2, 20%) 
2 >> 10/20 <<   Training | 0.8614 (39, 39,  6, 16) | 1.009 | -19.6 | 84.88 | 66.38 |-------------------------------------------|
2             Validation | 0.8627 (39, 39,  6, 16) | 1.002 | -20.0 | 84.83 | 66.13 |#########################################| ^ (1.6%, 3.27, 1.8, 1%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
2 >> 11/20 <<   Training | 0.8612 (39, 38,  6, 16) | 1.024 | -20.0 | 84.90 | 66.45 |--------------------------------------------|
2             Validation | 0.8625 (39, 38,  6, 16) | 1.018 | -20.0 | 84.85 | 66.19 |#########################################| ^ (1.6%, 3.31, 1.9, 0%) 
2 >> 12/20 <<   Training | 0.8611 (39, 38,  7, 16) | 1.012 | -20.0 | 84.91 | 66.50 |--------------------------------------------|
2             Validation | 0.8624 (39, 38,  6, 16) | 1.005 | -20.0 | 84.86 | 66.24 |##########################################| ^ (1.6%, 3.34, 1.5, 5%) 
2 >> 13/20 <<   Training | 0.8610 (39, 39,  7, 16) | 0.993 | -20.0 | 84.92 | 66.46 |--------------------------------------------|
2             Validation | 0.8624 (38, 39,  7, 16) | 0.987 | -20.0 | 84.87 | 66.20 |#########################################| ^ (1.6%, 3.34, 1.3, 11%) 
2 >> 14/20 <<   Training | 0.8611 (38, 39,  7, 16) | 0.978 | -18.3 | 84.92 | 66.44 |--------------------------------------------|
2             Validation | 0.8624 (38, 39,  7, 16) | 0.972 | -20.0 | 84.87 | 66.19 |#########################################| ^ (1.6%, 3.27, 1.6, 3%) 
2 >> 15/20 <<   Training | 0.8609 (39, 38,  7, 16) | 1.009 | -20.0 | 84.94 | 66.47 |--------------------------------------------|
2             Validation | 0.8623 (39, 38,  7, 16) | 1.003 | -20.0 | 84.88 | 66.20 |##########################################| ^ (1.6%, 3.40, 1.4, 9%) 
Decay learning rate: 0.000625 -> 0.000313
2 >> 16/20 <<   Training | 0.8608 (38, 39,  7, 16) | 0.986 | -20.0 | 84.94 | 66.49 |--------------------------------------------|
2             Validation | 0.8623 (38, 39,  6, 16) | 0.980 | -20.0 | 84.89 | 66.22 |##########################################| ^ (1.7%, 3.36, 1.8, 1%) 
Decay learning rate: 0.000313 -> 0.000156
2 >> 17/20 <<   Training | 0.8608 (39, 38,  7, 16) | 1.005 | -20.0 | 84.95 | 66.47 |--------------------------------------------|
2             Validation | 0.8622 (39, 39,  7, 16) | 0.998 | -20.0 | 84.89 | 66.20 |##########################################| ^ (1.7%, 3.40, 1.4, 7%) 
Decay learning rate: 0.000156 -> 0.000078
2 >> 18/20 <<   Training | 0.8607 (39, 39,  7, 16) | 0.994 | -20.0 | 84.95 | 66.49 |--------------------------------------------|
2             Validation | 0.8622 (38, 39,  6, 16) | 0.987 | -20.0 | 84.90 | 66.21 |##########################################| ^ (1.7%, 3.41, 1.4, 7%) 
Decay learning rate: 0.000078 -> 0.000039
2 >> 19/20 <<   Training | 0.8607 (39, 39,  6, 16) | 1.002 | -20.0 | 84.95 | 66.48 |--------------------------------------------|
2             Validation | 0.8622 (39, 39,  6, 16) | 0.996 | -20.0 | 84.90 | 66.20 |##########################################| ^ (1.7%, 3.42, 1.5, 5%) 
Decay learning rate: 0.000039 -> 0.000020
2 >> 20/20 <<   Training | 0.8607 (39, 39,  6, 16) | 0.998 | -20.0 | 84.95 | 66.48 |--------------------------------------------|
2             Validation | 0.8622 (39, 39,  6, 16) | 0.992 | -20.0 | 84.90 | 66.21 |##########################################| ^ (1.7%, 3.41, 1.5, 5%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8936 (45, 31,  6, 17) | 1.352 | 0.010 | 82.93 | 64.62 |--------------------------|
2             Validation | 0.8933 (45, 31,  6, 17) | 1.343 | 0.010 | 82.96 | 64.55 |#########################| ^ (1.1%, 4.79, 2.2, 0%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
2 >>  2/20 <<   Training | 0.8687 (40, 37,  6, 16) | 1.046 | 0.002 | 84.38 | 65.98 |---------------------------------------|
2             Validation | 0.8690 (40, 37,  6, 16) | 1.040 | 0.001 | 84.39 | 65.94 |#######################################| ^ (0.6%, 3.17, 1.8, 1%) 
2 >>  3/20 <<   Training | 0.8660 (40, 37,  7, 17) | 1.015 | 0.001 | 84.55 | 66.15 |-----------------------------------------|
2             Validation | 0.8669 (39, 37,  7, 17) | 1.009 | 0.001 | 84.55 | 66.02 |########################################| ^ (0.9%, 3.61, 1.7, 2%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
2 >>  4/20 <<   Training | 0.8619 (39, 39,  6, 16) | 1.005 | 0.002 | 85.00 | 66.20 |------------------------------------------|
2             Validation | 0.8629 (39, 39,  6, 16) | 0.998 | 0.001 | 84.98 | 66.06 |########################################| ^ (0.9%, 3.64, 1.3, 14%) 
2 >>  5/20 <<   Training | 0.8625 (39, 39,  6, 15) | 0.989 | 0.001 | 85.03 | 65.96 |---------------------------------------|
2             Validation | 0.8635 (39, 39,  6, 15) | 0.982 | 0.001 | 85.01 | 65.81 |######################################| ^ (1.0%, 3.26, 1.4, 8%) 
2 >>  6/20 <<   Training | 0.8608 (38, 39,  6, 16) | 0.989 | 0.001 | 85.12 | 66.35 |-------------------------------------------|
2             Validation | 0.8620 (38, 39,  6, 17) | 0.982 | 0.001 | 85.08 | 66.20 |#########################################| ^ (1.0%, 3.45, 1.2, 21%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
2 >>  7/20 <<   Training | 0.8601 (38, 39,  7, 16) | 0.978 | 0.001 | 85.19 | 66.30 |------------------------------------------|
2             Validation | 0.8614 (38, 39,  7, 17) | 0.972 | 0.001 | 85.16 | 66.12 |#########################################| ^ (1.1%, 3.22, 1.7, 1%) 
2 >>  8/20 <<   Training | 0.8601 (38, 38,  7, 17) | 0.984 | 0.001 | 85.21 | 66.51 |---------------------------------------------|
2             Validation | 0.8615 (38, 38,  6, 17) | 0.978 | 0.001 | 85.17 | 66.30 |###########################################| ^ (1.3%, 3.10, 1.2, 18%) 
2 >>  9/20 <<   Training | 0.8598 (40, 38,  6, 16) | 1.044 | 0.001 | 85.24 | 66.40 |-------------------------------------------|
2             Validation | 0.8611 (40, 38,  6, 16) | 1.037 | 0.001 | 85.19 | 66.19 |#########################################| ^ (1.3%, 3.28, 1.3, 11%) 
2 >> 10/20 <<   Training | 0.8592 (39, 39,  6, 16) | 1.003 | 0.001 | 85.26 | 66.45 |--------------------------------------------|
2             Validation | 0.8605 (39, 39,  6, 16) | 0.996 | 0.001 | 85.22 | 66.23 |##########################################| ^ (1.3%, 3.12, 1.4, 10%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
2 >> 11/20 <<   Training | 0.8589 (39, 39,  6, 16) | 1.001 | 0.001 | 85.28 | 66.45 |--------------------------------------------|
2             Validation | 0.8604 (39, 39,  6, 16) | 0.995 | 0.001 | 85.24 | 66.22 |##########################################| ^ (1.4%, 3.23, 1.5, 5%) 
2 >> 12/20 <<   Training | 0.8588 (39, 39,  6, 16) | 1.002 | 0.001 | 85.29 | 66.45 |--------------------------------------------|
2             Validation | 0.8604 (39, 39,  6, 16) | 0.995 | 0.001 | 85.24 | 66.21 |##########################################| ^ (1.5%, 3.23, 1.4, 7%) 
2 >> 13/20 <<   Training | 0.8588 (39, 38,  7, 16) | 1.002 | 0.001 | 85.29 | 66.46 |--------------------------------------------|
2             Validation | 0.8603 (39, 38,  6, 16) | 0.996 | 0.001 | 85.25 | 66.23 |##########################################| ^ (1.4%, 3.24, 1.2, 17%) 
2 >> 14/20 <<   Training | 0.8588 (39, 39,  6, 16) | 0.993 | 0.001 | 85.29 | 66.45 |--------------------------------------------|
2             Validation | 0.8603 (38, 39,  6, 16) | 0.987 | 0.001 | 85.25 | 66.22 |##########################################| ^ (1.4%, 3.18, 1.4, 8%) 
2 >> 15/20 <<   Training | 0.8587 (38, 39,  7, 16) | 0.985 | 0.001 | 85.30 | 66.49 |--------------------------------------------|
2             Validation | 0.8603 (38, 39,  6, 16) | 0.979 | 0.001 | 85.26 | 66.26 |##########################################| ^ (1.4%, 3.17, 1.4, 10%) 
Decay learning rate: 0.000625 -> 0.000313
2 >> 16/20 <<   Training | 0.8586 (38, 39,  6, 16) | 0.989 | 0.001 | 85.31 | 66.45 |--------------------------------------------|
2             Validation | 0.8602 (38, 39,  6, 16) | 0.983 | 0.001 | 85.26 | 66.22 |##########################################| ^ (1.5%, 3.25, 1.2, 18%) 
Decay learning rate: 0.000313 -> 0.000156
2 >> 17/20 <<   Training | 0.8586 (39, 39,  6, 16) | 0.992 | 0.001 | 85.31 | 66.48 |--------------------------------------------|
2             Validation | 0.8602 (38, 39,  6, 16) | 0.986 | 0.001 | 85.26 | 66.24 |##########################################| ^ (1.5%, 3.25, 1.4, 7%) 
Decay learning rate: 0.000156 -> 0.000078
2 >> 18/20 <<   Training | 0.8586 (39, 39,  6, 16) | 1.004 | 0.001 | 85.31 | 66.49 |--------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.997 | 0.001 | 85.27 | 66.26 |##########################################| ^ (1.5%, 3.24, 1.5, 6%) 
Decay learning rate: 0.000078 -> 0.000039
2 >> 19/20 <<   Training | 0.8586 (39, 39,  6, 16) | 1.001 | 0.001 | 85.31 | 66.48 |--------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.995 | 0.001 | 85.27 | 66.24 |##########################################| ^ (1.5%, 3.26, 1.3, 15%) 
Decay learning rate: 0.000039 -> 0.000020
2 >> 20/20 <<   Training | 0.8586 (39, 39,  6, 16) | 1.000 | 0.001 | 85.31 | 66.48 |--------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.994 | 0.001 | 85.27 | 66.24 |##########################################| ^ (1.5%, 3.26, 1.3, 15%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
2 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
2 >>  1/20 <<   Training | 0.8815 (39, 39,  6, 16) | 1.039 | 0.002 | 83.34 | 64.13 |---------------------|
2             Validation | 0.8814 (39, 39,  6, 16) | 1.033 | 0.002 | 83.35 | 64.05 |####################| ^ (1.0%, 3.82, 1.4, 8%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
2 >>  2/20 <<   Training | 0.8700 (39, 39,  6, 16) | 0.936 | 0.001 | 84.38 | 66.03 |----------------------------------------|
2             Validation | 0.8705 (38, 39,  6, 16) | 0.930 | 0.000 | 84.37 | 65.95 |#######################################| ^ (0.7%, 3.47, 1.7, 2%) 
2 >>  3/20 <<   Training | 0.8675 (38, 39,  6, 17) | 0.960 | 0.002 | 84.73 | 66.06 |----------------------------------------|
2             Validation | 0.8686 (38, 39,  6, 17) | 0.953 | 0.001 | 84.73 | 65.87 |######################################| ^ (1.3%, 5.24, 1.8, 1%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
2 >>  4/20 <<   Training | 0.8614 (38, 39,  7, 17) | 0.980 | 0.001 | 85.12 | 66.34 |-------------------------------------------|
2             Validation | 0.8625 (38, 39,  7, 17) | 0.973 | 0.001 | 85.09 | 66.14 |#########################################| ^ (1.3%, 5.16, 1.8, 1%) 
2 >>  5/20 <<   Training | 0.8611 (39, 39,  6, 16) | 1.008 | 0.001 | 85.18 | 66.05 |----------------------------------------|
2             Validation | 0.8622 (39, 39,  6, 16) | 1.002 | 0.001 | 85.15 | 65.84 |######################################| ^ (1.4%, 4.39, 1.6, 3%) 
2 >>  6/20 <<   Training | 0.8606 (37, 40,  6, 16) | 0.923 | 0.001 | 85.25 | 66.28 |------------------------------------------|
2             Validation | 0.8621 (37, 40,  6, 16) | 0.917 | 0.001 | 85.22 | 66.05 |########################################| ^ (1.5%, 4.76, 2.0, 0%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
2 >>  7/20 <<   Training | 0.8592 (39, 39,  6, 16) | 1.006 | 0.001 | 85.29 | 66.43 |--------------------------------------------|
2             Validation | 0.8609 (39, 39,  6, 16) | 0.999 | 0.001 | 85.25 | 66.13 |#########################################| ^ (1.9%, 4.37, 1.6, 3%) 
2 >>  8/20 <<   Training | 0.8591 (39, 39,  6, 16) | 0.993 | 0.001 | 85.30 | 66.42 |--------------------------------------------|
2             Validation | 0.8608 (39, 39,  6, 16) | 0.987 | 0.001 | 85.26 | 66.13 |#########################################| ^ (1.8%, 4.59, 1.3, 12%) 
2 >>  9/20 <<   Training | 0.8590 (39, 39,  6, 16) | 0.980 | 0.001 | 85.33 | 66.43 |--------------------------------------------|
2             Validation | 0.8607 (38, 39,  6, 16) | 0.973 | 0.001 | 85.29 | 66.13 |#########################################| ^ (1.8%, 4.34, 1.7, 2%) 
2 >> 10/20 <<   Training | 0.8585 (39, 39,  6, 16) | 0.998 | 0.001 | 85.36 | 66.51 |---------------------------------------------|
2             Validation | 0.8604 (39, 39,  6, 16) | 0.992 | 0.000 | 85.32 | 66.18 |#########################################| ^ (2.0%, 4.05, 1.7, 2%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
2 >> 11/20 <<   Training | 0.8588 (39, 38,  7, 17) | 1.019 | 0.001 | 85.37 | 66.56 |---------------------------------------------|
2             Validation | 0.8607 (39, 38,  7, 17) | 1.012 | 0.001 | 85.32 | 66.22 |##########################################| ^ (2.1%, 4.21, 1.7, 2%) 
2 >> 12/20 <<   Training | 0.8585 (39, 38,  7, 16) | 1.025 | 0.001 | 85.37 | 66.57 |---------------------------------------------|
2             Validation | 0.8604 (39, 38,  6, 16) | 1.018 | 0.001 | 85.33 | 66.23 |##########################################| ^ (2.1%, 4.26, 1.9, 1%) 
2 >> 13/20 <<   Training | 0.8583 (39, 39,  6, 16) | 1.010 | 0.001 | 85.38 | 66.53 |---------------------------------------------|
2             Validation | 0.8602 (39, 39,  6, 16) | 1.003 | 0.001 | 85.33 | 66.18 |#########################################| ^ (2.2%, 4.11, 1.5, 5%) 
2 >> 14/20 <<   Training | 0.8583 (38, 39,  6, 16) | 0.985 | 0.001 | 85.37 | 66.46 |--------------------------------------------|
2             Validation | 0.8603 (38, 39,  6, 16) | 0.979 | 0.000 | 85.33 | 66.13 |#########################################| ^ (2.1%, 4.18, 1.9, 1%) 
2 >> 15/20 <<   Training | 0.8582 (39, 38,  7, 16) | 1.022 | 0.001 | 85.38 | 66.49 |--------------------------------------------|
2             Validation | 0.8603 (39, 38,  6, 16) | 1.015 | 0.001 | 85.33 | 66.13 |#########################################| ^ (2.2%, 4.31, 2.2, 0%) 
Decay learning rate: 0.000625 -> 0.000313
2 >> 16/20 <<   Training | 0.8581 (38, 39,  6, 16) | 0.986 | 0.001 | 85.39 | 66.54 |---------------------------------------------|
2             Validation | 0.8602 (38, 39,  6, 16) | 0.980 | 0.001 | 85.34 | 66.18 |#########################################| ^ (2.2%, 4.35, 1.7, 2%) 
Decay learning rate: 0.000313 -> 0.000156
2 >> 17/20 <<   Training | 0.8580 (39, 39,  6, 16) | 1.006 | 0.001 | 85.39 | 66.52 |---------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.999 | 0.001 | 85.35 | 66.16 |#########################################| ^ (2.2%, 4.34, 1.8, 1%) 
Decay learning rate: 0.000156 -> 0.000078
2 >> 18/20 <<   Training | 0.8580 (39, 39,  6, 16) | 0.993 | 0.001 | 85.39 | 66.54 |---------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.986 | 0.001 | 85.35 | 66.18 |#########################################| ^ (2.2%, 4.37, 1.9, 1%) 
Decay learning rate: 0.000078 -> 0.000039
2 >> 19/20 <<   Training | 0.8580 (39, 39,  6, 16) | 1.004 | 0.001 | 85.39 | 66.53 |---------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.997 | 0.001 | 85.35 | 66.18 |#########################################| ^ (2.2%, 4.37, 2.0, 0%) 
Decay learning rate: 0.000039 -> 0.000020
2 >> 20/20 <<   Training | 0.8580 (39, 39,  6, 16) | 0.997 | 0.001 | 85.39 | 66.54 |---------------------------------------------|
2             Validation | 0.8601 (39, 39,  6, 16) | 0.990 | 0.001 | 85.35 | 66.18 |#########################################| ^ (2.2%, 4.37, 2.0, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset2_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
