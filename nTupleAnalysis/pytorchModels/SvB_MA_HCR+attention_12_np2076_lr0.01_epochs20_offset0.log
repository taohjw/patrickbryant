0 >> Epoch <<   Data Set |  Loss %(zz, zh, tt, mj) | σ     | σ(SR) | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
0 >>  1/20 <<   Training | 0.6670 (29, 34,  8, 30) | 0.666 | 0.784 | 88.28 | 86.41 |------------------------|
0             Validation | 0.6728 (29, 33,  8, 30) | 0.666 | 0.786 | 88.04 | 86.11 |#####################| ^ (0.8%, 10.50, 2.6, 0%) 
setGhostBatches(16)
Change training batch size: 1024 -> 2048 (996 batches)
0 >>  2/20 <<   Training | 0.6255 (28, 31, 10, 33) | 0.776 | 0.903 | 89.81 | 87.78 |-------------------------------------|
0             Validation | 0.6327 (29, 30, 10, 32) | 0.764 | 0.887 | 89.28 | 87.58 |###################################| ^ (0.5%, 12.40, 1.7, 2%) 
0 >>  3/20 <<   Training | 0.6281 (27, 36, 10, 28) | 0.826 | 0.956 | 89.46 | 87.98 |---------------------------------------|
0             Validation | 0.6372 (28, 35, 10, 28) | 0.818 | 0.943 | 88.93 | 87.63 |####################################| ^ (0.9%, 13.11, 1.9, 0%) 
setGhostBatches(4)
Change training batch size: 2048 -> 4096 (498 batches)
0 >>  4/20 <<   Training | 0.6093 (28, 28,  9, 35) | 0.841 | 0.972 | 90.43 | 88.38 |-------------------------------------------|
0             Validation | 0.6187 (30, 28,  9, 34) | 0.836 | 0.969 | 89.90 | 88.05 |########################################| ^ (0.8%, 10.26, 2.4, 0%) 
0 >>  5/20 <<   Training | 0.6061 (26, 28,  9, 37) | 0.864 | 0.999 | 90.71 | 88.62 |----------------------------------------------|
0             Validation | 0.6150 (27, 28,  9, 36) | 0.856 | 0.991 | 90.09 | 88.33 |###########################################| ^ (0.7%, 11.27, 1.8, 1%) 
0 >>  6/20 <<   Training | 0.6048 (26, 30, 10, 36) | 0.857 | 0.989 | 90.73 | 88.61 |----------------------------------------------|
0             Validation | 0.6142 (28, 29,  9, 35) | 0.851 | 0.988 | 90.04 | 88.34 |###########################################| ^ (0.7%, 11.35, 2.5, 0%) 
setGhostBatches(1)
Change training batch size: 4096 -> 8192 (249 batches)
0 >>  7/20 <<   Training | 0.6020 (28, 30,  9, 34) | 0.854 | 0.988 | 90.59 | 88.75 |-----------------------------------------------|
0             Validation | 0.6118 (29, 30,  9, 33) | 0.850 | 0.986 | 89.91 | 88.49 |############################################| ^ (0.7%, 11.36, 2.4, 0%) 
0 >>  8/20 <<   Training | 0.6016 (27, 34, 10, 30) | 0.878 | 1.016 | 90.89 | 88.81 |------------------------------------------------|
0             Validation | 0.6111 (28, 34, 10, 29) | 0.863 | 1.000 | 90.24 | 88.52 |#############################################| ^ (0.8%, 10.93, 1.6, 3%) 
0 >>  9/20 <<   Training | 0.6039 (25, 31, 10, 36) | 0.837 | 0.969 | 90.73 | 88.61 |----------------------------------------------|
0             Validation | 0.6132 (26, 31,  9, 35) | 0.820 | 0.953 | 90.02 | 88.39 |###########################################| ^ (0.6%, 10.83, 1.9, 1%) 
0 >> 10/20 <<   Training | 0.6013 (27, 30, 10, 34) | 0.856 | 0.992 | 90.73 | 88.70 |-----------------------------------------------|
0             Validation | 0.6118 (29, 29, 10, 33) | 0.845 | 0.985 | 90.01 | 88.44 |############################################| ^ (0.7%, 11.51, 1.7, 2%) 
setGhostBatches(0)
Change training batch size: 8192 -> 16384 (124 batches)
0 >> 11/20 <<   Training | 0.5963 (27, 32, 10, 32) | 0.887 | 1.025 | 91.00 | 88.89 |------------------------------------------------|
0             Validation | 0.6081 (29, 31,  9, 32) | 0.864 | 1.002 | 90.25 | 88.55 |#############################################| ^ (0.9%, 11.88, 2.2, 0%) 
0 >> 12/20 <<   Training | 0.6015 (28, 30, 11, 32) | 0.875 | 1.010 | 90.94 | 88.62 |----------------------------------------------|
0             Validation | 0.6134 (29, 30, 11, 31) | 0.865 | 1.004 | 90.27 | 88.22 |##########################################| ^ (1.0%, 12.22, 2.8, 0%) 
0 >> 13/20 <<   Training | 0.5961 (28, 30, 10, 33) | 0.880 | 1.018 | 90.98 | 88.90 |------------------------------------------------|
0             Validation | 0.6070 (29, 30, 10, 32) | 0.860 | 1.000 | 90.30 | 88.60 |#############################################| ^ (0.8%, 11.74, 2.0, 0%) 
0 >> 14/20 <<   Training | 0.5967 (25, 30, 10, 37) | 0.881 | 1.017 | 91.19 | 88.91 |-------------------------------------------------|
0             Validation | 0.6071 (26, 29, 10, 36) | 0.854 | 0.993 | 90.45 | 88.60 |#############################################| ^ (0.8%, 11.79, 2.0, 0%) 
0 >> 15/20 <<   Training | 0.5980 (28, 29,  9, 36) | 0.883 | 1.020 | 90.91 | 88.84 |------------------------------------------------|
0             Validation | 0.6096 (29, 29,  9, 35) | 0.869 | 1.006 | 90.24 | 88.52 |#############################################| ^ (0.8%, 12.44, 2.8, 0%) 
Decay learning rate: 0.010000 -> 0.002500
0 >> 16/20 <<   Training | 0.5926 (27, 31, 10, 34) | 0.895 | 1.033 | 91.13 | 89.00 |--------------------------------------------------|
0             Validation | 0.6041 (28, 30, 10, 33) | 0.877 | 1.014 | 90.39 | 88.67 |##############################################| ^ (0.9%, 11.57, 2.1, 0%) 
Decay learning rate: 0.002500 -> 0.000625
0 >> 17/20 <<   Training | 0.5918 (27, 31, 10, 33) | 0.894 | 1.031 | 91.15 | 89.03 |--------------------------------------------------|
0             Validation | 0.6033 (28, 31, 10, 33) | 0.880 | 1.021 | 90.43 | 88.70 |###############################################| ^ (0.8%, 11.73, 2.6, 0%) 
Decay learning rate: 0.000625 -> 0.000156
0 >> 18/20 <<   Training | 0.5917 (27, 31, 10, 34) | 0.894 | 1.032 | 91.17 | 89.03 |--------------------------------------------------|
0             Validation | 0.6032 (28, 30, 10, 33) | 0.882 | 1.023 | 90.44 | 88.70 |###############################################| ^ (0.8%, 11.71, 2.4, 0%) 
Decay learning rate: 0.000156 -> 0.000039
0 >> 19/20 <<   Training | 0.5917 (27, 31, 10, 34) | 0.894 | 1.031 | 91.17 | 89.03 |--------------------------------------------------|
0             Validation | 0.6032 (28, 30, 10, 33) | 0.883 | 1.024 | 90.44 | 88.71 |###############################################| ^ (0.9%, 11.72, 2.5, 0%) 
Decay learning rate: 0.000039 -> 0.000010
0 >> 20/20 <<   Training | 0.5917 (27, 31, 10, 34) | 0.894 | 1.031 | 91.17 | 89.03 |--------------------------------------------------|
0             Validation | 0.6032 (28, 30, 10, 33) | 0.884 | 1.024 | 90.44 | 88.71 |###############################################| ^ (0.9%, 11.71, 2.4, 0%) * ZZ4b/nTupleAnalysis/pytorchModels/SvB_MA_HCR+attention_12_np2076_lr0.01_epochs20_offset0_epoch20.pkl
Decay learning rate: 0.000010 -> 0.000002
