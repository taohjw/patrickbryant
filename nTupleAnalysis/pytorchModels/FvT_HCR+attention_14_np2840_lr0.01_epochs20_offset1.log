1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8986 (35, 37,  8, 20) | 0.943 |   7.1 | 81.65 | 64.67 |--------------------------|
1             Validation | 0.8986 (35, 37,  8, 20) | 0.942 |   7.4 | 81.63 | 64.62 |##########################| ^ (0.9%, 2.12, 1.1, 30%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
1 >>  2/20 <<   Training | 0.9191 (31, 37,  9, 23) | 0.838 |   8.8 | 81.76 | 65.78 |-------------------------------------|
1             Validation | 0.9194 (31, 37,  9, 23) | 0.837 |   9.0 | 81.69 | 65.73 |#####################################| ^ (0.9%, 2.75, 1.1, 27%) 
1 >>  3/20 <<   Training | 0.8818 (35, 39,  8, 19) | 0.893 |   7.3 | 83.16 | 65.70 |-------------------------------------|
1             Validation | 0.8822 (35, 39,  8, 19) | 0.892 |   7.1 | 83.12 | 65.61 |####################################| ^ (1.0%, 2.89, 1.5, 5%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
1 >>  4/20 <<   Training | 0.8780 (35, 38,  8, 19) | 0.905 |   8.6 | 83.51 | 66.22 |------------------------------------------|
1             Validation | 0.8788 (35, 38,  8, 19) | 0.905 |   8.4 | 83.43 | 66.09 |########################################| ^ (1.0%, 2.90, 1.4, 10%) 
1 >>  5/20 <<   Training | 0.8779 (36, 38,  8, 19) | 0.935 |   8.2 | 83.38 | 66.16 |-----------------------------------------|
1             Validation | 0.8788 (36, 38,  8, 19) | 0.935 |   7.5 | 83.30 | 66.04 |########################################| ^ (1.1%, 2.79, 0.9, 49%) 
1 >>  6/20 <<   Training | 0.8729 (38, 37,  7, 18) | 0.969 |  11.5 | 83.70 | 66.24 |------------------------------------------|
1             Validation | 0.8740 (38, 37,  7, 18) | 0.969 |  11.1 | 83.64 | 66.08 |########################################| ^ (1.2%, 2.95, 1.2, 20%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
1 >>  7/20 <<   Training | 0.8615 (39, 39,  6, 16) | 1.012 | -13.9 | 84.93 | 66.31 |-------------------------------------------|
1             Validation | 0.8622 (39, 39,  6, 16) | 1.011 | -12.6 | 84.92 | 66.17 |#########################################| ^ (1.2%, 3.22, 2.5, 0%) 
1 >>  8/20 <<   Training | 0.8611 (39, 38,  7, 16) | 1.011 | -12.9 | 84.98 | 66.32 |-------------------------------------------|
1             Validation | 0.8621 (39, 38,  7, 16) | 1.010 | -12.8 | 84.96 | 66.14 |#########################################| ^ (1.4%, 3.37, 1.9, 0%) 
1 >>  9/20 <<   Training | 0.8606 (39, 39,  6, 16) | 0.994 | -12.1 | 85.03 | 66.42 |--------------------------------------------|
1             Validation | 0.8614 (39, 39,  7, 16) | 0.993 | -12.3 | 85.02 | 66.24 |##########################################| ^ (1.4%, 3.09, 1.4, 8%) 
1 >> 10/20 <<   Training | 0.8607 (37, 40,  6, 16) | 0.951 | -13.6 | 85.06 | 66.38 |-------------------------------------------|
1             Validation | 0.8614 (37, 40,  6, 16) | 0.950 | -13.5 | 85.06 | 66.22 |##########################################| ^ (1.2%, 3.09, 1.7, 2%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
1 >> 11/20 <<   Training | 0.8600 (39, 38,  6, 16) | 1.012 | -14.6 | 85.11 | 66.51 |---------------------------------------------|
1             Validation | 0.8608 (39, 38,  6, 16) | 1.012 | -14.6 | 85.10 | 66.33 |###########################################| ^ (1.3%, 3.22, 1.2, 20%) 
1 >> 12/20 <<   Training | 0.8599 (39, 39,  6, 16) | 0.993 | -15.8 | 85.12 | 66.49 |--------------------------------------------|
1             Validation | 0.8608 (39, 39,  6, 16) | 0.992 | -15.5 | 85.11 | 66.31 |###########################################| ^ (1.3%, 3.25, 2.2, 0%) 
1 >> 13/20 <<   Training | 0.8598 (39, 39,  6, 16) | 1.013 | -15.4 | 85.12 | 66.50 |---------------------------------------------|
1             Validation | 0.8607 (39, 38,  6, 16) | 1.012 | -15.4 | 85.11 | 66.32 |###########################################| ^ (1.4%, 3.15, 1.7, 2%) 
1 >> 14/20 <<   Training | 0.8597 (39, 39,  6, 16) | 0.997 | -14.4 | 85.13 | 66.47 |--------------------------------------------|
1             Validation | 0.8606 (39, 39,  6, 16) | 0.997 | -15.0 | 85.12 | 66.29 |##########################################| ^ (1.4%, 3.19, 1.8, 1%) 
1 >> 15/20 <<   Training | 0.8597 (38, 39,  6, 16) | 0.987 | -17.3 | 85.14 | 66.54 |---------------------------------------------|
1             Validation | 0.8606 (38, 39,  6, 16) | 0.986 | -18.3 | 85.13 | 66.35 |###########################################| ^ (1.3%, 3.18, 2.2, 0%) 
Decay learning rate: 0.000625 -> 0.000313
1 >> 16/20 <<   Training | 0.8596 (39, 39,  6, 16) | 0.990 | -16.5 | 85.14 | 66.52 |---------------------------------------------|
1             Validation | 0.8605 (39, 39,  6, 16) | 0.990 | -16.3 | 85.14 | 66.33 |###########################################| ^ (1.4%, 3.19, 2.3, 0%) 
Decay learning rate: 0.000313 -> 0.000156
1 >> 17/20 <<   Training | 0.8595 (39, 39,  6, 16) | 1.000 | -16.2 | 85.15 | 66.51 |---------------------------------------------|
1             Validation | 0.8605 (39, 39,  6, 16) | 0.999 | -16.5 | 85.14 | 66.31 |###########################################| ^ (1.4%, 3.19, 2.2, 0%) 
Decay learning rate: 0.000156 -> 0.000078
1 >> 18/20 <<   Training | 0.8595 (39, 39,  6, 16) | 0.993 | -15.6 | 85.15 | 66.51 |---------------------------------------------|
1             Validation | 0.8604 (39, 39,  6, 16) | 0.992 | -15.9 | 85.14 | 66.32 |###########################################| ^ (1.4%, 3.18, 1.8, 1%) 
Decay learning rate: 0.000078 -> 0.000039
1 >> 19/20 <<   Training | 0.8595 (39, 39,  6, 16) | 0.997 | -15.4 | 85.15 | 66.51 |---------------------------------------------|
1             Validation | 0.8604 (39, 39,  6, 16) | 0.996 | -15.8 | 85.14 | 66.32 |###########################################| ^ (1.4%, 3.19, 1.7, 2%) 
Decay learning rate: 0.000039 -> 0.000020
1 >> 20/20 <<   Training | 0.8595 (39, 39,  6, 16) | 1.001 | -15.6 | 85.15 | 66.51 |---------------------------------------------|
1             Validation | 0.8604 (39, 39,  6, 16) | 1.000 | -16.1 | 85.14 | 66.31 |###########################################| ^ (1.4%, 3.20, 1.8, 1%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r_max | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.8954 (34, 38,  8, 20) | 0.863 |   9.1 | 82.53 | 64.88 |----------------------------|
1             Validation | 0.8954 (34, 38,  8, 20) | 0.863 |  10.3 | 82.55 | 64.85 |############################| ^ (1.2%, 2.15, 1.7, 2%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
1 >>  2/20 <<   Training | 0.8832 (35, 38,  8, 19) | 0.896 | -15.5 | 83.28 | 65.62 |------------------------------------|
1             Validation | 0.8837 (35, 38,  8, 19) | 0.895 |   8.2 | 83.29 | 65.52 |###################################| ^ (0.9%, 2.97, 0.9, 51%) 
1 >>  3/20 <<   Training | 0.8734 (38, 37,  7, 18) | 0.982 | -20.0 | 83.94 | 65.98 |---------------------------------------|
1             Validation | 0.8749 (38, 37,  7, 18) | 0.980 | -20.0 | 83.86 | 65.74 |#####################################| ^ (1.7%, 3.48, 1.1, 24%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
1 >>  4/20 <<   Training | 0.8727 (38, 37,  7, 19) | 0.985 | -20.0 | 83.93 | 66.19 |-----------------------------------------|
1             Validation | 0.8744 (38, 37,  7, 19) | 0.984 | -20.0 | 83.84 | 65.91 |#######################################| ^ (1.8%, 4.31, 0.8, 61%) 
1 >>  5/20 <<   Training | 0.8704 (37, 37,  7, 18) | 0.983 | -20.0 | 84.12 | 66.11 |-----------------------------------------|
1             Validation | 0.8720 (37, 37,  7, 18) | 0.981 | -20.0 | 84.03 | 65.84 |######################################| ^ (1.8%, 4.53, 0.7, 82%) 
1 >>  6/20 <<   Training | 0.8695 (37, 39,  7, 18) | 0.947 | -20.0 | 83.91 | 66.14 |-----------------------------------------|
1             Validation | 0.8714 (37, 39,  7, 18) | 0.946 | -20.0 | 83.80 | 65.83 |######################################| ^ (2.0%, 4.94, 0.7, 73%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
1 >>  7/20 <<   Training | 0.8614 (40, 38,  6, 16) | 1.040 | -20.0 | 85.06 | 66.27 |------------------------------------------|
1             Validation | 0.8627 (40, 38,  6, 16) | 1.039 | -20.0 | 85.01 | 66.04 |########################################| ^ (1.6%, 4.24, 0.8, 65%) 
1 >>  8/20 <<   Training | 0.8609 (37, 40,  6, 16) | 0.937 | -20.0 | 85.10 | 66.46 |--------------------------------------------|
1             Validation | 0.8623 (37, 40,  7, 16) | 0.936 | -20.0 | 85.04 | 66.23 |##########################################| ^ (1.6%, 4.07, 0.9, 54%) 
1 >>  9/20 <<   Training | 0.8604 (39, 39,  6, 15) | 0.986 | -20.0 | 85.16 | 66.29 |------------------------------------------|
1             Validation | 0.8620 (39, 39,  6, 15) | 0.985 | -20.0 | 85.09 | 66.04 |########################################| ^ (1.7%, 4.28, 0.8, 59%) 
1 >> 10/20 <<   Training | 0.8602 (39, 39,  6, 15) | 1.016 | -20.0 | 85.17 | 66.32 |-------------------------------------------|
1             Validation | 0.8617 (39, 39,  6, 15) | 1.015 | -20.0 | 85.11 | 66.08 |########################################| ^ (1.6%, 4.44, 0.8, 66%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
1 >> 11/20 <<   Training | 0.8595 (39, 39,  6, 16) | 1.005 | -20.0 | 85.20 | 66.45 |--------------------------------------------|
1             Validation | 0.8611 (39, 39,  6, 16) | 1.004 | -20.0 | 85.14 | 66.19 |#########################################| ^ (1.7%, 4.46, 1.0, 42%) 
1 >> 12/20 <<   Training | 0.8594 (38, 39,  6, 16) | 0.995 | -20.0 | 85.21 | 66.53 |---------------------------------------------|
1             Validation | 0.8611 (39, 39,  6, 16) | 0.994 | -20.0 | 85.14 | 66.27 |##########################################| ^ (1.7%, 4.57, 0.9, 46%) 
1 >> 13/20 <<   Training | 0.8593 (39, 38,  6, 16) | 1.019 | -20.0 | 85.21 | 66.51 |---------------------------------------------|
1             Validation | 0.8610 (39, 38,  7, 16) | 1.018 | -20.0 | 85.15 | 66.24 |##########################################| ^ (1.8%, 4.41, 1.0, 44%) 
1 >> 14/20 <<   Training | 0.8596 (38, 39,  7, 17) | 0.977 | -20.0 | 85.21 | 66.55 |---------------------------------------------|
1             Validation | 0.8613 (38, 39,  7, 17) | 0.976 | -20.0 | 85.15 | 66.29 |##########################################| ^ (1.7%, 4.59, 1.0, 40%) 
1 >> 15/20 <<   Training | 0.8592 (39, 38,  6, 16) | 1.020 | -20.0 | 85.23 | 66.54 |---------------------------------------------|
1             Validation | 0.8610 (39, 38,  7, 16) | 1.019 | -20.0 | 85.16 | 66.27 |##########################################| ^ (1.7%, 4.60, 1.0, 43%) 
Decay learning rate: 0.000625 -> 0.000313
1 >> 16/20 <<   Training | 0.8592 (40, 38,  6, 16) | 1.032 | -20.0 | 85.23 | 66.52 |---------------------------------------------|
1             Validation | 0.8610 (40, 38,  6, 16) | 1.031 | -20.0 | 85.16 | 66.25 |##########################################| ^ (1.7%, 4.65, 0.9, 55%) 
Decay learning rate: 0.000313 -> 0.000156
1 >> 17/20 <<   Training | 0.8591 (39, 39,  6, 16) | 0.998 | -20.0 | 85.23 | 66.53 |---------------------------------------------|
1             Validation | 0.8608 (39, 39,  7, 16) | 0.997 | -20.0 | 85.17 | 66.25 |##########################################| ^ (1.7%, 4.61, 1.0, 41%) 
Decay learning rate: 0.000156 -> 0.000078
1 >> 18/20 <<   Training | 0.8590 (39, 39,  6, 16) | 0.996 | -20.0 | 85.24 | 66.50 |---------------------------------------------|
1             Validation | 0.8608 (39, 39,  7, 16) | 0.995 | -20.0 | 85.17 | 66.23 |##########################################| ^ (1.7%, 4.66, 1.0, 39%) 
Decay learning rate: 0.000078 -> 0.000039
1 >> 19/20 <<   Training | 0.8590 (39, 39,  6, 16) | 0.995 | -20.0 | 85.24 | 66.51 |---------------------------------------------|
1             Validation | 0.8608 (39, 39,  7, 16) | 0.994 | -20.0 | 85.17 | 66.24 |##########################################| ^ (1.8%, 4.63, 0.9, 48%) 
Decay learning rate: 0.000039 -> 0.000020
1 >> 20/20 <<   Training | 0.8590 (39, 39,  6, 16) | 0.998 | -20.0 | 85.24 | 66.50 |---------------------------------------------|
1             Validation | 0.8608 (39, 39,  6, 16) | 0.997 | -20.0 | 85.17 | 66.23 |##########################################| ^ (1.7%, 4.65, 1.0, 41%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9096 (44, 39,  5, 12) | 1.226 | 0.008 | 82.58 | 63.89 |------------------|
1             Validation | 0.9082 (44, 39,  5, 12) | 1.224 | 0.008 | 82.70 | 63.98 |###################| ^ (1.3%, 4.10, 1.5, 6%) 
setGhostBatches(8)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
1 >>  2/20 <<   Training | 0.8697 (38, 39,  7, 17) | 0.976 | 0.000 | 84.13 | 65.86 |--------------------------------------|
1             Validation | 0.8697 (38, 39,  7, 17) | 0.975 | 0.000 | 84.14 | 65.81 |######################################| ^ (0.8%, 3.01, 1.5, 6%) 
1 >>  3/20 <<   Training | 0.8673 (39, 39,  6, 15) | 1.021 | 0.001 | 84.60 | 65.64 |------------------------------------|
1             Validation | 0.8673 (39, 39,  6, 15) | 1.020 | 0.001 | 84.61 | 65.55 |###################################| ^ (0.9%, 3.61, 1.2, 23%) 
setGhostBatches(1)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
1 >>  4/20 <<   Training | 0.8627 (39, 38,  6, 16) | 1.007 | 0.001 | 84.87 | 66.39 |-------------------------------------------|
1             Validation | 0.8634 (39, 38,  6, 16) | 1.006 | 0.001 | 84.86 | 66.21 |##########################################| ^ (1.3%, 3.51, 1.3, 11%) 
1 >>  5/20 <<   Training | 0.8617 (39, 38,  6, 16) | 1.028 | 0.001 | 84.98 | 66.31 |-------------------------------------------|
1             Validation | 0.8627 (39, 38,  6, 16) | 1.027 | 0.001 | 84.95 | 66.08 |########################################| ^ (1.6%, 3.69, 2.3, 0%) 
1 >>  6/20 <<   Training | 0.8620 (40, 37,  6, 16) | 1.051 | 0.001 | 85.09 | 66.48 |--------------------------------------------|
1             Validation | 0.8632 (40, 37,  6, 16) | 1.051 | 0.001 | 85.07 | 66.22 |##########################################| ^ (1.7%, 3.37, 1.7, 2%) 
setGhostBatches(0)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
1 >>  7/20 <<   Training | 0.8599 (39, 39,  6, 16) | 1.009 | 0.001 | 85.17 | 66.37 |-------------------------------------------|
1             Validation | 0.8611 (39, 39,  6, 16) | 1.008 | 0.001 | 85.15 | 66.10 |########################################| ^ (1.9%, 3.91, 0.9, 47%) 
1 >>  8/20 <<   Training | 0.8599 (40, 38,  6, 16) | 1.019 | 0.001 | 85.18 | 66.40 |-------------------------------------------|
1             Validation | 0.8612 (40, 38,  6, 16) | 1.018 | 0.001 | 85.15 | 66.12 |#########################################| ^ (2.0%, 4.05, 1.5, 5%) 
1 >>  9/20 <<   Training | 0.8601 (39, 39,  6, 15) | 1.001 | 0.002 | 85.19 | 66.44 |--------------------------------------------|
1             Validation | 0.8614 (39, 39,  6, 15) | 1.000 | 0.002 | 85.16 | 66.15 |#########################################| ^ (2.0%, 4.12, 1.4, 8%) 
1 >> 10/20 <<   Training | 0.8596 (38, 40,  6, 16) | 0.962 | 0.001 | 85.24 | 66.39 |-------------------------------------------|
1             Validation | 0.8610 (38, 40,  6, 16) | 0.961 | 0.001 | 85.20 | 66.10 |########################################| ^ (2.0%, 4.18, 1.3, 11%) 
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
1 >> 11/20 <<   Training | 0.8592 (39, 38,  7, 17) | 1.005 | 0.001 | 85.25 | 66.61 |----------------------------------------------|
1             Validation | 0.8606 (39, 38,  7, 17) | 1.004 | 0.001 | 85.23 | 66.31 |###########################################| ^ (2.0%, 4.17, 1.6, 4%) 
1 >> 12/20 <<   Training | 0.8589 (39, 39,  6, 16) | 0.992 | 0.001 | 85.26 | 66.57 |---------------------------------------------|
1             Validation | 0.8603 (39, 39,  6, 16) | 0.991 | 0.001 | 85.23 | 66.27 |##########################################| ^ (2.0%, 4.33, 1.5, 6%) 
1 >> 13/20 <<   Training | 0.8589 (39, 38,  7, 16) | 1.005 | 0.001 | 85.27 | 66.60 |----------------------------------------------|
1             Validation | 0.8604 (39, 38,  7, 16) | 1.005 | 0.001 | 85.23 | 66.29 |##########################################| ^ (2.1%, 4.43, 1.3, 13%) 
1 >> 14/20 <<   Training | 0.8588 (39, 39,  6, 16) | 1.002 | 0.001 | 85.27 | 66.52 |---------------------------------------------|
1             Validation | 0.8602 (39, 39,  6, 16) | 1.001 | 0.001 | 85.24 | 66.21 |##########################################| ^ (2.0%, 4.48, 1.3, 15%) 
1 >> 15/20 <<   Training | 0.8587 (38, 39,  6, 16) | 0.994 | 0.001 | 85.28 | 66.60 |----------------------------------------------|
1             Validation | 0.8602 (39, 39,  6, 16) | 0.993 | 0.001 | 85.24 | 66.29 |##########################################| ^ (2.1%, 4.53, 0.9, 50%) 
Decay learning rate: 0.000625 -> 0.000313
1 >> 16/20 <<   Training | 0.8587 (39, 39,  6, 16) | 1.002 | 0.001 | 85.28 | 66.57 |---------------------------------------------|
1             Validation | 0.8602 (39, 39,  6, 16) | 1.002 | 0.001 | 85.25 | 66.27 |##########################################| ^ (2.1%, 4.63, 1.6, 3%) 
Decay learning rate: 0.000313 -> 0.000156
1 >> 17/20 <<   Training | 0.8586 (39, 39,  6, 16) | 0.996 | 0.001 | 85.28 | 66.58 |---------------------------------------------|
1             Validation | 0.8601 (39, 39,  6, 16) | 0.996 | 0.001 | 85.25 | 66.26 |##########################################| ^ (2.1%, 4.66, 1.2, 17%) 
Decay learning rate: 0.000156 -> 0.000078
1 >> 18/20 <<   Training | 0.8586 (39, 39,  6, 16) | 0.994 | 0.001 | 85.29 | 66.57 |---------------------------------------------|
1             Validation | 0.8601 (39, 39,  6, 16) | 0.993 | 0.001 | 85.26 | 66.26 |##########################################| ^ (2.1%, 4.61, 1.1, 27%) 
Decay learning rate: 0.000078 -> 0.000039
1 >> 19/20 <<   Training | 0.8586 (39, 39,  6, 16) | 0.996 | 0.001 | 85.29 | 66.58 |---------------------------------------------|
1             Validation | 0.8601 (39, 39,  6, 16) | 0.995 | 0.001 | 85.26 | 66.26 |##########################################| ^ (2.1%, 4.62, 1.2, 20%) 
Decay learning rate: 0.000039 -> 0.000020
1 >> 20/20 <<   Training | 0.8586 (39, 39,  6, 16) | 0.999 | 0.001 | 85.29 | 66.58 |---------------------------------------------|
1             Validation | 0.8601 (39, 39,  6, 16) | 0.999 | 0.001 | 85.26 | 66.26 |##########################################| ^ (2.1%, 4.63, 1.2, 22%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
1 >> Epoch <<   Data Set |  Loss %(d4, d3, t4, t3) | Norm  | r neg | % AUC | % AUC | AUC Bar Graph ^ (ABC, Max Loss, chi2/bin, p-value) * Output Model
1 >>  1/20 <<   Training | 0.9096 (44, 39,  5, 12) | 1.226 | 0.008 | 82.58 | 63.89 |------------------|
1             Validation | 0.9082 (44, 39,  5, 12) | 1.224 | 0.008 | 82.70 | 63.98 |###################| ^ (1.3%, 4.10, 1.5, 6%) 
setGhostBatches(16)
Increase training batch size: 1024 -> 2048 (1210 batches)
Decay learning rate: 0.010000 -> 0.005000
1 >>  2/20 <<   Training | 0.8696 (38, 39,  7, 17) | 0.984 | 0.000 | 84.15 | 65.84 |--------------------------------------|
1             Validation | 0.8696 (38, 39,  7, 17) | 0.983 | 0.000 | 84.17 | 65.79 |#####################################| ^ (0.7%, 3.12, 1.2, 17%) 
1 >>  3/20 <<   Training | 0.8671 (40, 39,  6, 15) | 1.021 | 0.001 | 84.69 | 65.50 |-----------------------------------|
1             Validation | 0.8669 (40, 39,  6, 15) | 1.020 | 0.001 | 84.71 | 65.43 |##################################| ^ (0.8%, 3.53, 1.6, 3%) 
setGhostBatches(4)
Increase training batch size: 2048 -> 4096 (605 batches)
Decay learning rate: 0.005000 -> 0.002500
1 >>  4/20 <<   Training | 0.8627 (39, 38,  6, 16) | 1.003 | 0.001 | 84.91 | 66.35 |-------------------------------------------|
1             Validation | 0.8632 (39, 38,  6, 16) | 1.002 | 0.001 | 84.92 | 66.19 |#########################################| ^ (1.2%, 3.41, 1.0, 37%) 
1 >>  5/20 <<   Training | 0.8615 (39, 38,  6, 16) | 1.022 | 0.001 | 85.03 | 66.30 |------------------------------------------|
1             Validation | 0.8624 (39, 38,  6, 16) | 1.021 | 0.001 | 85.01 | 66.09 |########################################| ^ (1.4%, 3.61, 2.0, 0%) 
1 >>  6/20 <<   Training | 0.8619 (40, 37,  6, 16) | 1.051 | 0.001 | 85.11 | 66.47 |--------------------------------------------|
1             Validation | 0.8630 (40, 37,  6, 16) | 1.051 | 0.001 | 85.09 | 66.24 |##########################################| ^ (1.6%, 3.24, 1.4, 9%) 
setGhostBatches(1)
Increase training batch size: 4096 -> 8192 (302 batches)
Decay learning rate: 0.002500 -> 0.001250
1 >>  7/20 <<   Training | 0.8595 (38, 39,  6, 16) | 0.994 | 0.001 | 85.22 | 66.44 |--------------------------------------------|
1             Validation | 0.8607 (39, 39,  6, 16) | 0.993 | 0.001 | 85.20 | 66.18 |#########################################| ^ (1.8%, 3.77, 1.1, 30%) 
1 >>  8/20 <<   Training | 0.8594 (39, 39,  6, 16) | 0.999 | 0.001 | 85.23 | 66.43 |--------------------------------------------|
1             Validation | 0.8607 (39, 39,  6, 16) | 0.998 | 0.001 | 85.20 | 66.16 |#########################################| ^ (1.9%, 4.09, 1.6, 3%) 
1 >>  9/20 <<   Training | 0.8591 (39, 39,  6, 16) | 1.002 | 0.001 | 85.26 | 66.50 |--------------------------------------------|
1             Validation | 0.8604 (39, 39,  6, 16) | 1.002 | 0.001 | 85.24 | 66.21 |##########################################| ^ (2.0%, 4.61, 1.4, 9%) 
1 >> 10/20 <<   Training | 0.8588 (39, 39,  6, 16) | 1.006 | 0.001 | 85.29 | 66.49 |--------------------------------------------|
1             Validation | 0.8602 (39, 39,  6, 16) | 1.006 | 0.001 | 85.26 | 66.19 |#########################################| ^ (2.0%, 4.79, 1.2, 22%) 
setGhostBatches(0)
Increase training batch size: 8192 -> 16384 (151 batches)
Decay learning rate: 0.001250 -> 0.000625
1 >> 11/20 <<   Training | 0.8589 (40, 38,  6, 16) | 1.042 | 0.001 | 85.32 | 66.54 |---------------------------------------------|
1             Validation | 0.8604 (40, 38,  6, 15) | 1.041 | 0.001 | 85.29 | 66.23 |##########################################| ^ (2.0%, 4.97, 1.2, 20%) 
1 >> 12/20 <<   Training | 0.8586 (39, 39,  6, 16) | 1.011 | 0.001 | 85.32 | 66.58 |---------------------------------------------|
1             Validation | 0.8601 (39, 39,  6, 16) | 1.010 | 0.001 | 85.29 | 66.28 |##########################################| ^ (2.0%, 4.97, 1.8, 1%) 
1 >> 13/20 <<   Training | 0.8585 (39, 38,  6, 16) | 1.022 | 0.001 | 85.33 | 66.61 |----------------------------------------------|
1             Validation | 0.8601 (39, 38,  6, 16) | 1.021 | 0.001 | 85.29 | 66.29 |##########################################| ^ (2.1%, 5.08, 0.9, 53%) 
1 >> 14/20 <<   Training | 0.8585 (39, 39,  6, 16) | 1.015 | 0.001 | 85.33 | 66.52 |---------------------------------------------|
1             Validation | 0.8599 (39, 39,  6, 16) | 1.015 | 0.001 | 85.31 | 66.22 |##########################################| ^ (2.0%, 5.04, 1.3, 13%) 
1 >> 15/20 <<   Training | 0.8583 (39, 39,  6, 16) | 0.993 | 0.001 | 85.34 | 66.60 |----------------------------------------------|
1             Validation | 0.8598 (39, 39,  6, 16) | 0.993 | 0.001 | 85.31 | 66.28 |##########################################| ^ (2.1%, 5.12, 1.2, 23%) 
Decay learning rate: 0.000625 -> 0.000313
1 >> 16/20 <<   Training | 0.8583 (39, 39,  6, 16) | 1.002 | 0.001 | 85.34 | 66.57 |---------------------------------------------|
1             Validation | 0.8599 (39, 39,  6, 16) | 1.001 | 0.001 | 85.31 | 66.26 |##########################################| ^ (2.0%, 5.25, 1.6, 3%) 
Decay learning rate: 0.000313 -> 0.000156
1 >> 17/20 <<   Training | 0.8582 (39, 39,  6, 16) | 0.990 | 0.001 | 85.35 | 66.59 |---------------------------------------------|
1             Validation | 0.8598 (39, 39,  6, 16) | 0.990 | 0.001 | 85.31 | 66.27 |##########################################| ^ (2.1%, 5.23, 1.3, 15%) 
Decay learning rate: 0.000156 -> 0.000078
1 >> 18/20 <<   Training | 0.8582 (39, 39,  6, 16) | 0.993 | 0.001 | 85.35 | 66.58 |---------------------------------------------|
1             Validation | 0.8597 (39, 39,  6, 16) | 0.993 | 0.001 | 85.32 | 66.26 |##########################################| ^ (2.1%, 5.19, 1.5, 4%) 
Decay learning rate: 0.000078 -> 0.000039
1 >> 19/20 <<   Training | 0.8582 (39, 39,  6, 16) | 0.996 | 0.001 | 85.35 | 66.59 |---------------------------------------------|
1             Validation | 0.8597 (39, 39,  6, 16) | 0.995 | 0.001 | 85.32 | 66.27 |##########################################| ^ (2.1%, 5.20, 1.4, 7%) 
Decay learning rate: 0.000039 -> 0.000020
1 >> 20/20 <<   Training | 0.8582 (39, 39,  6, 16) | 0.999 | 0.001 | 85.35 | 66.59 |---------------------------------------------|
1             Validation | 0.8597 (39, 39,  6, 16) | 0.999 | 0.001 | 85.32 | 66.27 |##########################################| ^ (2.1%, 5.20, 1.5, 6%) * ZZ4b/nTupleAnalysis/pytorchModels/FvT_HCR+attention_14_np2840_lr0.01_epochs20_offset1_epoch20.pkl
Decay learning rate: 0.000020 -> 0.000010
